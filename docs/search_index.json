[["index.html", "Case Studies Preface", " Case Studies 2021-05-18 Preface This book is the result of a student projects for Case Studies course at the Warsaw University of Technology. Each team prepared an article on one of the topics selected from reproducibility, imputation, and interpretability. This project is inspired by a book Limitations of Interpretable Machine Learning Methods created at the Department of Statistics, LMU Munich XAI Stories. Case studies for eXplainable Artificial Intelligence done at the Warsaw University of Technology and at the University of Warsaw and ML Case Studies during a case study a year ago. We used the LIML project as the cornerstone for this repository. The cover created by Anna Kozak. Creative Commons License This book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["technical-setup.html", "Technical Setup", " Technical Setup The book chapters are written in the Markdown language. The simulations, data examples and visualizations were created with R (R Core Team 2018) and Python. The book was compiled with the bookdown package. We collaborated using git and github. For details, head over to the book’s repository. References "],["explainable-artificial-intelligence.html", "Chapter 1 Explainable Artificial Intelligence", " Chapter 1 Explainable Artificial Intelligence Author: Anna Kozak Machine Learning is used more and more in virtually any aspect of our life. We train models to predict the future in banking, telecommunication, insurance, industry, and many other areas. The models give us predictions, however, very often we do not know how they are calculated. Can we trust these predictions? Why should we use the results of models which we do not fully understand? This results in a lack of understanding of the results obtained, so there is now a strong need to explain the decisions made by the non-interpretable models called black boxes. There are several tools for exploring and explaining the predictive models, which allow to understanding how they are works. During the class, we explored methods of explaining global as well as local, which you can read more about in the Explanatory Model Analysis (Biecek and Burzykowski 2021) book. Teams work on data from a Kaggle that described problems in the world around us. Each team was responsible for analyzing, modeling, and building explanations for complex models. Each chapter includes a story about how to use explainable AI to understand the model. References "],["xai1-explainable-cards.html", "1.1 Explaining Credit Card Customers churns", " 1.1 Explaining Credit Card Customers churns Authors: Katarzyna Solawa, Przemysław Chojecki, Bartosz Sawicki (Warsaw University of Techcnology) "],["ml-in-predition-of-real-estate-prices.html", "1.2 ML in predition of real estate prices", " 1.2 ML in predition of real estate prices Authors: Sebastian Deręgowski, Maciej Gryszkiewicz, Paweł Morgen (Warsaw University of Technology) 1.2.1 Abstract Lorem ipsum 1.2.2 Introduction Lorem ipsum 1.2.3 Related Work Lorem ipsum 1.2.4 Methodology Lorem ipsum 1.2.5 Results Lorem ipsum 1.2.6 Summary and conclusions Lorem ipsum "],["xai-heart-disease.html", "1.3 How not to have broken heart &lt;3", " 1.3 How not to have broken heart &lt;3 Authors: Przybyłek Paulina, Rólkiewicz Renata, Słowakiewicz Patryk 1.3.1 Introduction "],["xai1-explainable-wine.html", "1.4 Wines", " 1.4 Wines Authors: Jakub Kosterna, Bartosz Siński, Jan Smoleń "],["xai1-explainable-hotels.html", "1.5 eXplaining predictions of booking cancelations", " 1.5 eXplaining predictions of booking cancelations Authors: Mateusz Krzyziński, Anna Urbala, Artur Żółkowski (Warsaw University of Technology) 1.5.1 Introduction One of the biggest problems and challenges facing the hospitality industry is the significant number of canceled reservations. Common reasons for cancellations include sudden deterioration in health, accidents, bad weather conditions, schedule conflicts or unexpected responsibilities (Falk and Vieru 2018). Interestingly, a noticeable group consists of customers who, after making a reservation, are still looking for new, better offers, and even make many reservations at the same time to be able to choose the most advantageous one (Antonio, Almeida, and Nunes 2017). The hospitality industry’s response to the above problem are hotel cancellation policies. They play a crucial role in determining various aspects of the hotel business, including the ultimate goal of revenues and profits optimisation. In recent years (before the pandemic), there has been a clear tightening of these policies. Hotels do this, for example by shortening the free cancellation windows or increasing cancellation penalties (Riasi, Schwartz, and Chen 2019; Smith et al. 2015). The use of machine learning to forecast and identify potential cancellations is also playing an increasing role. There are many systems to support hotel management that use booking data. Various machine learning algorithms are used for this purpose, ranging from support vector machines, through artificial neural networks, to the most common tree-based models (Sánchez-Medina and C-Sánchez 2020; Andriawan et al. 2020). Most of the solutions and projects are only theoretical, while some have been tested in practice, enabling cancellations to be reduced by up to 37 percentage points (Antonio, Almeida, and Nunes 2019). Unfortunately, most papers do not tackle the issue of the importance of the used explanatory variables and do not try to explain the model’s predictions. However, it is the exploration of trained models that should be treated as one of the key factors in the design of hotel management support systems. Business validation and ethical verification of solutions is necessary. Bearing in mind that a strict cancellation policy or overbooking strategy can have negative effects on both reputation and revenue, systems designers should be wary of unfair biased behaviour. At the same time, the use of explanatory artificial intelligence methods is helpful in creating models with better performance scores. In the following chapter, we present an analysis of predictive models for hotel bookings cancellations. We answer questions about the reasons for the model prediction both in general view and in relation to individual reservations. 1.5.2 Dataset and models 1.5.3 Local explanations In order to explain model output for a particular guest and their booking, we used instance-level exploration methods, such as Break-down, SHAP (SHapley Additive exPlanations), LIME (Local Interpretable Model-agnostic Explanations), CP (Ceteris Paribus). We decided to investigate noteworthy predictions, i.e. false positive and false negative (respectively canceled bookings predicted as not canceled and vice versa), the most valid (the predictions the model was most sure of), and the closest to decision boundary. 1.5.3.1 False positive and false negative predictions We might discover that the model is providing incorrect predictions. The key is to find the reasons for this, that is, to answer the question what has driven the wrong prediction. We used local explanations methods for the observations in both groups with the worst predictions, i.e. the lowest probability of proper classification. FIGURE 1.1: A plot of Shapley values for random forest model and misclassified observation (false positive) with the highest probability of cancellation. The green and red bars correspond to the contribution of the variable to the prediction. The green ones increase the probability of cancellation, while the red ones decrease it (increase the probability of no cancellation). On the x-axis, there is the model prediction value, while on the y-axis there are variables and their values for the observation. Informations with the biggest contribution to the final prediction are guest’s country of origin (Portugal), a total number of special requests equals zero, and the fact that customer type related to the given booking is Transient-Party. This is an indication that the model may be slightly biased due to the country of origin. It is the property of the customer and not of the booking itself. Thus, depending on the application, it is worth considering whether this response is satisfactory and meets ethical standards. With every value contributes to the misprediction, the only feature with the correct contribution is customer type (Transient-Party guests are the most popular type of customers, accounting for as much as 75% of bookers). FIGURE 1.2: A plot of Shapley values for random forest model and misclassified observation (false negative) with the lowest probability of cancellation. The elements of the plot have the same meaning as in the previous case. It can be seen that the largest contributions are related to the country of origin of the booker and the type of guest assigned to them. It is worth noting that the values of these variables are the same as in the case of the observation analyzed above. Again, they contribute to the same side of the prediction, but the contribution values are different. In this case, the type of client turns out to be the most important. Most of the values also affect the prediction of no cancellation. It is interesting that a slightly later booking in relation to the date of stay (lead time) has the opposite effect than in the previous example. The reason is the dependencies between the variables. FIGURE 1.3: A plot of LIME model values for the random forest model and the most misslassified observations. The similarity between the observations is also noticeable in the lime method. The first five variables are identical and have almost the same coefficients. Therefore, it is these less significant variables that influence the final prediction. In the case of these two observations with similar characteristics, but completely different predictions, the use of the SHAP method (generalizing the breakdown method) gives a better picture. The Glass-box model selected in LIME method to approximate the black-box model, and not the data themselves, is not able to capture dependencies between variables. 1.5.3.2 The most valid predictions The considered model returns an appropriate prediction in over 89% of cases. However, the level of certainty of the model with respect to the prediction (i.e. the probability that an observation is assigned to a class) may be different. Thus, it is worth considering why the model is almost sure of some outputs and how would the model’s predictions change if the values of some of the explanatory variables changed. We used local explanations methods for the observations in both groups with the best predictions, i.e. the highest probability of proper classification (equal to 1.0). FIGURE 1.4: A plot of Shapley values for random forest model and observation with sure negative prediction. The elements of the plot were described above. Like in the previous examples - the largest contribution has the country, in that case: France. Again, this is a Transient-Party customer and that also affected the prediction. Also, no special requests affect negatively to prediction (more than eg. no previous cancellations). Only one of the top variables affected positively: it was no required car parking spaces, but this impact was unnoticeable in the final prediction. FIGURE 1.5: A plot of Shapley values for random forest model and observation with sure positive prediction. The elements of the plot were described above. Again, Portugal as a country of origin affected positively the probability of cancelation (keep in mind that the hotel is in Portugal, so we can assume that compatriots cancel their reservations more often). Also, no special requests affected positively on prediction (although in the previous case it had a negative effect). We can notice that for positive prediction other factors have the biggest impact than for negative. Eg. a longer lead time moved up to third place and now has a positive impact. FIGURE 1.6: Ceteris-paribus profiles for the selected continuous explanatory variables and label encoded country variable for the random forest model and observations with the sure prediction. Dots indicate the values of the variables and the values of the predictions for observations. Green profiles are for sure positive prediction (a cancellation), while blue profiles are for sure negative prediction. Looking at the ceteris paribus profiles, it is intuitive to see that the prediction for the observation classified as the not canceled stay is more stable, i.e. less sensitive to changes in the values of explanatory variables. In the case of observation of canceled reservations, a change of the arrival date by a few weeks would cause a significant decrease in the certainty of the prediction. It is related to the seasonality of bookings (the decrease occurs at the beginning of July - the holiday period). However, the biggest changes in the prediction for this observation could be due to noting the fact of additional booking requirements (required car parking spaces and a total number of special requests). Changing these values to non-zero would change the prediction completely. Moreover, the huge changes depend on the country of origin of the booker, which in this case is Portugal. When considering the prediction for an observation classified as not canceled, we see that the only explanatory variable whose change would have a significant impact on the certainty of the prediction is the number of previously canceled reservations. A change to any non-zero value would change the prediction, but its certainty would be close to the decision boundary. 1.5.3.3 The closest to decision boundary predictions We analyzed the situations when the model is sure of the returned output. However, the observations for which the prediction was uncertain, close to the decision limit, are also worth considering. We might want to know the answer to the question of whether it is a matter of similar numbers of explanatory variables shifting the prediction in different directions, or maybe there are variables that do not fit the whole picture, and therefore the model is not certain. It is also worth checking how much such predictions fluctuate depending on the changes in the explanatory variables. FIGURE 1.7: A plot of Shapley values for random forest model and observation classified as negative with probability near 50%. The elements of the plot were described above. This is a very interesting example. One variable fixed prediction. Very short lead time (one day) opposed all other factors like Portugal as country of origin or no special requests and made the model predict correctly. It is amazing, that one factor can change everything. FIGURE 1.8: A plot of Shapley values for random forest model and observation classified as positive with probability near 50%. The elements of the plot were described above. This observation is not so exciting as the previous one, but it is the next evidence that the special requests decrease the probability that the client will cancel the reservation. Nevertheless, this observation was classified as positive. Agent was the most important positive variable although in the previous examples he did not have such a contribution. But… agent has that huge contribution only in Shapley. FIGURE 1.9: A plot of LIME model values for the random forest model and the same observation. Here agent has much less impact. This is a reminder for us that each method works differently and takes different variables into account. It is worth remembering this. In this method for that (and a lot of other) observation the most important factor is no previous cancelations, but it is not enough for the model to make a negative decision. 1.5.4 Global explanations The second group of methods of explainable artificial intelligence are those concerning not a single observation, but the entire set of them. We used these model level explanations to provide information about the quality of the model performance and infer how the model behaves in general. The methods we used for this purpose are Permutational Variable Importance, PDP (Partial Dependence Profile), ALE (Accumulated Local Effects). We applied them not only to the main random forest model, but also to other trained models to compare the obtained results. 1.5.4.1 Importance of explanatory variables First, we decided to check which variables are important for our main model and compare the obtained results with the intuitions we had after conducting the exploration at the prediction level. FIGURE 1.10: A plot of variable importance. The length of each bar represents the difference between the loss function (1-AUC) for the original data and the data with the permuted values of a particular variable. As we can see above, the most important variable for the model is the information about the guest’s country of origin. This confirms the intuitions obtained thanks to local explanations, for many observations this variable was the key aspect. In particular, the origin of Portugal (the country where the hotels are located) is the most significant for the prediction. The method indicated that the next principal variables for predictions are information on lead time and number of special booking requests. Lead time is the number of days that elapsed between the entering date of the booking into the system and the arrival date. It is a factor that may inform about whether the stay was planned long before or it is spontaneous. Meanwhile, the number of special requests is related to additional interest in the booking, which may indicate that it is an important stay for the client. The next variables with a significant average impact on the model are the group of those concerning the booking method (agent and market segment) and the type of booking (customer type and adr associated with the booking cost). It should also be noted that some of the variables are of marginal importance. These are (from the bottom of the plot) the type of hotel (recall that the data relate to two hotels of different specificity), the number of adults, the number of previously not canceled bookings (this is probably also related to a small number of observations with a non-zero value of this feature - 3.1% in the entire dataset), the type of room reserved. These are the variables that should be considered to be excluded in order to simplify the model. On the other hand, it is quite surprising that the number of previously canceled reservation, which was often indicated by LIME as one of the most important factors for the predictions under consideration, is so insignificant (0.007 drop-out loss) according to the algorithm of the permutational variable importance. 1.5.4.2 Comparison with other models Plots similar to that above in Figure 1.10 are useful for comparison of a variables’ importance in different models. It may be helpful in generating new knowledge - identification of recurring key variables may lead to the discovery of new factors involved in a particular mechanism (Biecek and Burzykowski 2021) (in our case, cancellation of reservations). Thus, we decided to compare the importance of the variables in the models we had trained, described in the Dataset and models section. The plots below show the results for our main Random Forest model and 4 other models with legend: green - Random Forest with label encoding (main model), red - Random Forest with one hot encoding, purple - Logistic Regression, blue - Decision Tree, orange - XGBoost. FIGURE 1.11: The comparison of the importance of explanatory variables for selected models. The elements of a single plot were described above. Note the different starting locations for the bars, due to differences in the AUC score value obtained for the training dataset for different models. It can be seen that the importance of the variables is related to the type of algorithm used in a given model. For example, in a decision tree, each variable has a clearly noticeable effect on the predictions. In all the tree-based models explained, the most important variable overlaps - it is the aforementioned country of origin. In logistic regression model, this feature is the third most important variable, but it is the model with the worst score overall. However, in general, the groups of the most important variables in each model are also similar. So our earlier conclusions regarding the key booking cancellation factors are confirmed. Likewise, in each of the tree-based models (even in a single decision tree model) the same variables were indicated as least important. An interesting fact is that in random forest with one hot encoding the agent variable is even more important than in random forest with label encoding. It is a categorical variable, so we can see the influence of encoding here - trees could extract more information from this variable thanks to not creating unnatural numerical relationships as with label encoding. The second noticeable difference between these models is the importance of the market segment. 1.5.4.3 The global impact of variables Then we decided to check the influence of the most influential variables on the predictions in the context of the whole set. For this, we used the ALE method. The plots below show the results for our Random Forest and 3 other tree-based models with legend: green - Random Forest with label encoding (main model), red - Random Forest with one hot encoding, blue - Decision Tree, orange - XGBoost. Note that we chose not to generate plots for the logistic regression model because it performed too poorly. FIGURE 1.12: ALE plot for the country. The ALE plots work like Ceteris Paribus - they show how the variable affects a prediction but not only for one observation - for the entire training dataset. We have a greater probability of resignation for one country - this is Portugal. This confirms our hypothesis built on the basis of local explanatory methods. Generally, compatriots more often resign from booking. FIGURE 1.13: ALE plot for lead time. The elements of a single plot were described above. And again we have confirmation that the longer the lead time, the greater the chance that the customer will resign. The ALE plots are very similar to Ceteris Paribus but that’s a great example that they’re not the same. Look at the Ceteris Paribus profile for lead time in the Local explanations section. The probability of resignation increased to about 170 days, then decreased and remained at a constant level. Here we can see that it was the “local behavior.” Globally, the probability only grows, then it stabilizes (after about 350 days - a year). Therefore, you may suspect that it is not worth allowing reservations so far in advance. FIGURE 1.14: ALE plot for total of special requests and required car parking spaces. The elements of a single plot were described above. Additional actions taken by the client regarding booking reduce the likelihood of cancellation. The very first special request significantly reduces the probability of resignation. The influence of the next ones is not that clear. This also confirms the thesis we made earlier that the lack of special requests increases the probability of resignation. Reserving a parking space has an even greater impact on the predictions. We may think that in these hotels it is payable in advance, or that car travelers are less dependent on public transport, so their arrival is more certain. FIGURE 1.15: ALE plot for previous cancellations and previous bookings not canceled. The elements of a single plot were described above. Information about a given customer’s prior bookings is also very valuable for prediction. The fact of earlier cancellation of a reservation strongly influences the prediction of the next one, which seems natural. A non-zero number of prior non-canceled bookings works the opposite way, but the prediction values don’t fluctuate that much. After analyzing these examples, an important conclusion can be drawn about the XAI methods. ALE plots can be a great tool for analyzing the influence of variables on the prediction - they can be used to verify the hypotheses put forward at the stage of local explanations and introduce new ones. When comparing the results for different models, note that the profiles for random forests with both versions of the categorical variable encoding are almost identical (green and red lines in the graphs). Looking more broadly, the profiles are comparable for all models. The most significant differences can be seen in the variables relating to the previous reservations of a given customer - the number of canceled and non-canceled reservations. The XGBoost model favors non-zero values more - the prediction changes are bigger. In general, this model is the most sensitive to all variables, as can be seen from the shape of the profile curves. Moreover, we used the comparison of the PDP and ALE plots for our main model (see Figure 1.16). In the case of some variables, the profiles generated using both methods almost coincide. However, there are also variables where you can see differences in prediction values, but profiles are parallel to each other. This parallelism suggests and allows us to conclude that the used model is additive due for these explanatory variables (Biecek and Burzykowski 2021). FIGURE 1.16: Partial-dependence and accumulated-local profiles for the main random forest model and selected variables 1.5.5 Summary and conclusions References "],["explainable-artificial-inteligence-r.html", "Chapter 2 Explainable artificial inteligence (R)", " Chapter 2 Explainable artificial inteligence (R) "],["deep-learning-1.html", "Chapter 3 Deep Learning 1", " Chapter 3 Deep Learning 1 "],["deep-learning-2.html", "Chapter 4 Deep Learning 2", " Chapter 4 Deep Learning 2 "],["axondeepseg-automatic-axon-and-myelin-segmentation-from-microscopy-data-using-convolutional-neural-networks.html", "4.1 AxonDeepSeg: automatic axon and myelin segmentation from microscopy data using convolutional neural networks", " 4.1 AxonDeepSeg: automatic axon and myelin segmentation from microscopy data using convolutional neural networks Authors: Jakubowski Mikołaj, Tomaszewski Patryk, Ziemła Mateusz 4.1.1 Introduction "],["ara-cnn-a-bayesian-deep-learning-model-intended-for-histopathological-image-classification-.html", "4.2 ARA-CNN - a Bayesian deep learning model intended for histopathological image classification.", " 4.2 ARA-CNN - a Bayesian deep learning model intended for histopathological image classification. *Authors: Wojciech Szczypek, Jakub Lis, Jan Gąska (Warsaw University of Techcnology) "],["rethinking-the-u-net-architecture-for-multimodal-biomedical-image-segmentation.html", "4.3 Rethinking the U-Net architecture for multimodal biomedical image segmentation", " 4.3 Rethinking the U-Net architecture for multimodal biomedical image segmentation Grudzień Adrianna, Łukaszyk Marcin, Piasecki Michał 4.3.1 Introduction Tutaj będzie tekst kiedyś "],["dl2-rmdl-unet.html", "4.4 Analyzing Reproducibility churns", " 4.4 Analyzing Reproducibility churns Authors: Marceli Korbin, Szymon Szmajdziński, Paweł Wojciechowski (Warsaw University of Techcnology) "],["title.html", "4.5 Title", " 4.5 Title Authors: Filip Chrzuszcz, Szymon Rećko, Mateusz Sperkowski (Warsaw University of Technology) 4.5.1 Title, Authors, Abstract, Keywords 4.5.2 Introduction 4.5.3 Related Literature 4.5.4 Methods 4.5.5 Result Skrot do pierwszego projektu: Despite lower results than in the first paper, in most datasets we still achieved better results than the baselines paper attempted to beat. The ones that we weren’t able to reproduce where either limits of processing power, or could be assigned to effect off randomness which is basis od this paper. The authors unfortunately didn’t include their randomness results, therefore their exact calculations aren’t reproducible. 4.5.6 Discussion 4.5.7 Conclusion 4.5.8 References 4.5.9 Random Multimodel Deep Learning for Classification Results .tg {border-collapse:collapse;border-spacing:0;} .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; overflow:hidden;padding:10px 5px;word-break:normal;} .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;} .tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} Dataset WOS-5736 WOS-11967 WOS-46985 Reuters-21578 Score Source Paper Repr. Paper Repr. Paper Repr. Paper Repr. RMDL 3 RDLs 90.86 89.37 87.39 84.25 78.39 — 89.10 87.64 9 RDLs 92.60 89.28 90.65 — 81.92 — 90.36 89.83 15 RDLs 92.66 — 91.01 — 81.86 — 89.91 — 30 RDLs 93.57 — 91.59 — 82.42 — 90.69 — Table 1 4.5.9.1 Reuters-21578 Paper’s Plots Our Reproduction Figure 1 4.5.9.2 WOS-5736 Paper’s Plots Our Reproduction Figure 2 .tg {border-collapse:collapse;border-spacing:0;} .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; overflow:hidden;padding:10px 5px;word-break:normal;} .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;} .tg .tg-9wq8{border-color:inherit;text-align:center;vertical-align:middle} .tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} Dataset IMDB 20NewsGroup Score Source Paper Repr. Paper Repr. RMDL 3 RDLs 89.91 88.49 86.73 — 9 RDLs 90.13 — 87.62 — 15 RDLs 90.79 — 87.91 — Table 2 ERROR RATE 1-Accuracy .tg {border-collapse:collapse;border-spacing:0;} .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; overflow:hidden;padding:10px 5px;word-break:normal;} .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;} .tg .tg-9wq8{border-color:inherit;text-align:center;vertical-align:middle} .tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} Dataset MNIST CIFAR-10 Score Source Paper Repr. Paper Repr. RMDL 3 RDLs 0.51 0.55 9.89 38.23 9 RDLs 0.41 0.65 9.1 36.91 15 RDLs 0.21 — 8.74 — 30 RDLs 0.18 — 8.79 — Table 3 4.5.9.3 CIFAR 10 Paper’s Plots Our Reproduction Figure 3 4.5.9.4 MNIST Paper’s Plots Our Reproduction Figure 4 4.5.10 Adversarial Attacks Against Medical Deep Learning Systems "],["machine-learning.html", "Chapter 5 Machine Learning", " Chapter 5 Machine Learning Author: Hubert Baniecki An ever-growing domain of machine learning decision systems in medicine has crossed ways with the COVID-19 pandemic. Precariously, a vast majority of the proposed predictive models focus on achieving high performance; while overlooking comprehensive validation. Nowadays, providing representative data, model explainability, even bias detection become mandatory for responsible prediction making in high-stakes medical applications. The following chapters introduce new views into the already published work on the topic of patients’ COVID-19 mortality prognosis using supervised machine learning: 1. TBA 2. TBA 3. TBA 4. TBA "],["validation-and-comparison-of-covid-19-mortatility-prediction-models-on-multi-source-data.html", "5.1 Validation and comparison of COVID-19 mortatility prediction models on multi-source data", " 5.1 Validation and comparison of COVID-19 mortatility prediction models on multi-source data Authors: Komorowski Michał, Olender Przemysław, Sieńko Piotr, Welkier Konrad 5.1.1 Introduction "],["one-model-to-fit-them-all-covid-19-mortality-prediction-using-multinational-data.html", "5.2 One model to fit them all: COVID-19 mortality prediction using multinational data", " 5.2 One model to fit them all: COVID-19 mortality prediction using multinational data Authors: Kurek Marcelina, Stączek Mateusz, Wiśniewski Jakub, Zdulska Hanna 5.2.1 Abstract During the outbreak of SARS-CoV-2 many scientists tried to build a model that was able to predict survival or death of patients based on available medical data. Yan et al.(Yan, Zhang, Goncalves, Xiao, Wang, Guo, Sun, Tang, Jing, Zhang, Huang, et al. 2020) were among first researchers to publish their model based on blood data (lactic dehydrogenase (LDH), lymphocyte percentage and high-sensitivity C-reactive protein (hs-CRP)) with 90% accuracy, however recreations of this model trained on other countries’ data - US, Netherlands and France were not so successful. In this article we explore the possibility of building an international model for predicting COVID survival. Our research concludes that discarding place of origin resulted in unsatisfying performance around 0.70 ROC AUC score, while taking into account mentioned place of origin scored nearly 0.75 ROC AUC score. 5.2.2 Introduction 5.2.3 Data sources 5.2.4 Model building 5.2.5 Results First, a model performing very well on data can perform poorly on data coming from a different location. In our case, we found that the model presented by Yan et al. is not portable and does perform poorly on data from NY and NETHERLANDS. Combining data from different sources results in much bigger dataset to train a model on. In our dataset we combined data from CHINA, NY and NETHERLANDS and got 5 columns: first 3 columns contained data about blood samples (LDH, CRP and Lymphocytes), next column was the goal and the last column contained the source of data. However, models trained on such a dataset tend to include the source of data as an important feature [LINK TO TREE PICTURE]. That means the model creates sub-models and is biased for some or every source of data to give better predictions. Unfortunately, such models have low scores in different performance metrics and are worse than a model created specifically for a given source of data. Below, the Table 1 presents those scores of 3 models. Table 1: Scores from the DALEX explainer for 3 classifiers after grid search parameter tuning. Selected models were in the top 3 models tested by LazyPredict, sorted by “ROC AUC.” Tested model recall precision f1 accuracy auc AdaBoostClassifier 0.43 0.68 0.53 0.73 0.80 NearestCentroid 0.58 0.56 0.57 0.70 0.66 KNeighborsClassifier 0.72 0.65 0.68 0.77 0.84 When the source of data is excluded from the training dataset, the results look even less promising. This is expected as the origin of data proved to be a useful feature. Scores of the trained models are therefore a bit lower and there is not a single model that would be ready for use in the medical industry. 5.2.6 Discussion References "],["transparent-machine-learning-to-support-predicting-covid-19-infection-risk-based-on-chronic-diseases.html", "5.3 Transparent Machine Learning to Support Predicting COVID-19 Infection Risk Based on Chronic Diseases", " 5.3 Transparent Machine Learning to Support Predicting COVID-19 Infection Risk Based on Chronic Diseases Authors: Hubert Ruczyński, Dawid Przybyliński, Kinga Ulasik 5.3.1 Introduction "],["comparison-of-deep-learning-and-tree-based-models-in-the-clinical-prediction-of-the-course-of-covid-19-illness.html", "5.4 Comparison of deep learning and tree-based models in the clinical prediction of the course of COVID-19 illness", " 5.4 Comparison of deep learning and tree-based models in the clinical prediction of the course of COVID-19 illness Authors: Jakub Fołtyn, Kacper Grzymkowski, Konrad Komisarczyk 5.4.1 Abstract The COVID-19 pandemic overwhelmed medical staff around the world, showing that effective and explainable models are needed to help allocate limited resources to those in need. Many published models for predicting COVID-19 related ICU admission and mortality were tree-based models (Yan, Zhang, Goncalves, Xiao, Wang, Guo, Sun, Tang, Jing, Zhang, and al. 2020) or neural network models (Li et al. 2020). We compared the two architectures in effectiveness, explainability and reproducibility. The two architectures appear to be similar with regards to their effectiveness, but the DNN model had significant reproducibility issues and worse explainability. 5.4.2 Introduction 5.4.3 Methods 5.4.4 Results 5.4.5 Discussion References "],["rashomonml.html", "Chapter 6 RashomonML", " Chapter 6 RashomonML "],["topic.html", "6.1 Topic", " 6.1 Topic Authors: Adrian Stańdo, Maciej Pawlikowski, Mariusz Słapek (Warsaw University of Technology) 6.1.1 Abstract Technological advances were early adopted by healthcare with great benefits and developments. In many health-related realms machine learning is crucial such as: development of new medical procedures, the treatment of chronic diseases, the management of patient records and data. Explainable AI (XAI) gives invaluable tools in healthcare for understanding the models by humans. The aim of this article is to compare PDP profiles for a Rashomon set with a given metric. Following we use five different metrics of distance function and based on that we compare PDP curves. Additionally, the library in the Python language has been created, which automates this research. We have observed that the created models, which have similar scores, have different variable importances. In this paper these differences were measured and assessed to understand better the problem of predicting hospital mortality using data from MIMIC-III. 6.1.2 Literature review Rashomon is a intriguing Japanese movie in which four people witness an incident from different vantage points. When they come to testify in court, they all report the same facts, but their stories of what happened are very different. In machine learning Rashomon set is used to characterise problem in which many different models offer accurate results describing the same data. However, not every accurate model gives a right conclusion as described in (Breiman and others 2001): “If the model is a poor emulation of nature, the conclusion might be worng”. Herein authors also explain basics of Rashomon sets on example. Much more in depth and mathematical description is provided in (Semenova, Rudin, and Parr 2019). Another important topic related to Rashomon sets is analysing the feature importance of the model. It was described in this article (Fisher, Rudin, and Dominici 2019), where authors suggested to study the maximum and minimum of variable importance across all models included in the Rashomon set. This technique was called MCR (Model Class Reliance). Furthermore, (Dong and Rudin 2020) presented technique to visualise the “cloud” of variable importance for models in the set, which could help us understand the Rashomon set and choose the one which give the best interpretation. The last question stated in the article (Rudin et al. 2021) was about choosing model from the Rashomon set. It might be a difficult task, especially when we lack good exploration tools. (Das et al. 2019) created a system called BEAMS that allows to choose the most important features. Next, the program searches the hypothesis space in order to find model which fits best to given constraints. Since this system works only with linear regression classifiers, (Rudin et al. 2021) stated a question if it is possible to design a simmilar system which will search only models within the Rashomon set. References "],["title-1.html", "6.2 Title", " 6.2 Title Authors: Jan Borowski, Konstanty Kraszewski, Krzysztof Wolny 6.2.1 Literature review In 1950 Japanese director, Akira Kurosawa, presented film Rashomon. Movie resolves around four witnesses, that describes the same crime in four different ways. This situation was called Rashomon effect after the name of the movie. In other words Rashomon effect is a situation when we have multiple different descriptions to the same event. This term is commonly used in multiple sciences like sociology, psychology or history. At the begging of the 21st century Rashomon effect was introduced to predictive modelling by Leo Breiman and his work ‘Statistical modeling: The Two Cultures’(Breiman and others 2001). In this article he named Rashomon effect situation, where there are many approximately-equally accurate models. Although these models have similar results, they can differ, when it comes to the way they managed to achieve it. Breiman called for closer examination of the Rashomon effect and conclusions that can be drawn from it. Recently, we can observe growing interest in Rashomon effect, although there is still a lot to be discovered. One of the articles, that bring closer the problem is ‘A study in Rashomon curves and volumes: A new perspective on generalization and model simplicity in machine learning’[6-0-rashomon-intro]. It provides several approaches for estimating the size of the Rashomon effect as well as the usefulness of the Rashomon curve in model selection. References "],["roshomon-sets-of-in-hospital-mortality-prediction-random-forest-models.html", "6.3 Roshomon sets of in-hospital mortality prediction random forest models", " 6.3 Roshomon sets of in-hospital mortality prediction random forest models Authors: Jeugeniusz Winiczenko, Mikolaj Malec, Patryk Wrona (Warsaw University of Techcnology) 6.3.1 Abstract The concept of Rashomon set is gaining more and more popularity in machine learning world. However, most efficient ways of building and analysing such sets are yet to be discovered. The main aim of this study was to develop several approaches to creating Rashomon sets, examining their characteristics and using them for further predictions. Performance of models was estimated using the area under the receiver operating characteristic (AUC) curve. For models from Rashomon sets analysis of features’ importance and PDP curves was also conducted. In this study, physiological time-series and medical histories from the Medical Information Mart for Intensive Care (MIMIC-III) database were used. Random forest models were trained for mortality prediction task on 2 datasets; the first containing only physiological time-series and the second containing both physiological time-series and medical histories. For 2 sets of trained models, corresponding to 2 datasets, several Rashomon sets were created using different thresholds. 6.3.2 Related work Rashomon sets are sets of models performing extraordinarily well on a given task. In machine learning, this term was used for the very first time by Leo Breiman in his paper issued in 2001 (Breiman and others 2001). Just as the task could be any, like in our case predicting patient’s mortality, the use of given features in order to explain vary among many highly accurate models. Moreover, Leo Breiman also described this situation as the Rashomon Effect and explained details using exemplary models. Until recently, the Rashomon sets have been rarely a subject of scientific research. In 2019 (Semenova, Rudin, and Parr 2019) approached the issue creating mathematical and statistical definitions and notations regarding such sets of models. They described Rashomon sets as subspaces of the hypothesis space, that is subsets of models having comparable performance as the best model with respect to a loss function. In order to define well the problem, they introduced Rashomon ratio (fraction of models in rashomon set and all models from hypothesis space) and shattering coefficient - the maximum number of ways any n data points can be classified using functions from the hypothesis space. Another outstanding remark concerning Rashomon set was made when in 2019 (Fisher, Rudin, and Dominici 2019) emphasized the analysis of features’ importance within Rashomon sets. The authors suggested Model Class Reliance - a new variable importance (VI) tool to study the the range of VI values across all highly accurate models - models included in rashomon sets. Later, (Rudin et al. 2021) provided basic rules for interpretable machine learning and identified 10 technical challenge areas in interpretable machine learning. They emphasized the troubleshooting and easiness of using glass-box models today as well as their advantage over black-box models due to their inscrutable nature. In this article, Challenge number 9 involves understanding, exploring, and measuring the Rashomon set. The authors address questions about how to characterize and visualize rashomon sets, and finally, how to pick the best model out of rashomon set. The Variable Importance Clouds, introduced in (Dong and Rudin 2020), are an excellent tool that one can use to address the above problems. Sush cloud maps every variable to its importance for every well-performing model. In our work, we choose and visualize the Rashomon sets built on a set of features as well as their subset. We address the problem of searching the most crucial predictive variables among those Rashomon sets and investigate the impact of choosing subsets of input features on the whole process of determining Rashomon sets and their characteristics. References "],["rashomon-on-mimic-draft.html", "6.4 Rashomon on MIMIC - draft", " 6.4 Rashomon on MIMIC - draft Authors: Degórski Karol, Fic Piotr, Kamiński Adrian (Warsaw University of Techcnology) 6.4.1 Review of the literature The term Rashomon effect was created to describe a situation when there are many different models with quite similar predictions. Very often there are many different descriptions giving about the same minimum error rate, so that we cannot point one model as the best Breiman and others (2001). As an example of this effect in reality they gave Linear Regression model and finding 5 from 30 best describing variables of a given problem. In this case there are approximately 140,000 such subsets. The authors explained that usually we choose the model which has best results on a test set, although there may be also different subsets of 5 variables that give very similar results. They also noticed that this effect occurs in different models, such as decision trees or neural networks. Furthermore, Semenova, Rudin, and Parr (2019) contributed to expand the study about Rashomon effect. They defined Rashomon set as a subset of models that have similar performance to the best model in terms of loss function. Moreover they introduced Rashomon ratio, that represents the fraction of models that fit our data equally well. Also they explained that Rashomon curve is a function of empirical risk versus the Rashomon ratio. They saw that there is a good generalization of the Rashomon curve’s elbow model when choosing between performance and simplicity of the model. They found out that interpretability of model is connected with Rashomon sets. Accordingly, when the Rashomon set is large there may exist simpler and higher performing model. Analysis of the Rashomon effect is still a new and open for developement field of the interpretable machine learning. Because of that, there are remaining challanges and problems, which are missing a state-of-the-art approach. Some of them are a proper measure of the Rashomon set, the best techniques of its visualization and optimal choice of the model from the Rashomon set Rudin et al. (2021). One of useful tools for the mentioned tasks is a framework called Variable Importance Clouds Dong and Rudin (2020), which can be used for studying the variable importance among Rashomon set. References "],["roshomon-sets-on-death-prediction-xgb-models-using-mimic-iii-database.html", "6.5 Roshomon sets on death prediction XGB models using MIMIC-III database", " 6.5 Roshomon sets on death prediction XGB models using MIMIC-III database Authors: Ada Gąssowska, Elżbieta Jowik (Warsaw University of Techcnology) 6.5.1 An initial literature review The Rashomon Effect is not a common concept, therefore all references to it in the literature are limited to some extent. In general it is said to occur when the same phenomenon can be explained in many different ways. The Rashomon set in turn is introduced as the set of almost-equally-accurate models for a given problem. With regard to Machine Learning the above term was first used by Leo Breiman in his 2001 paper (Breiman and others 2001) to describe a class of problems where many differing, accurate models exist to describe the same data i.e. to describe the case where we have many models that are different but are approximately-equally accurate. The name concept came from the title of the movie by Kurosawa from 1950 in which different characters have different perspectives on the same crimes. Breiman emphasized that the observation of many different accurate models on specific datasets is a common phenomenon. Since 2001 the topic was rarely discussed. While doing research on different machine learning models, data was quite often not taken into consideration at all. As stated in the recent article “A study in Rashomon curves and volumes: A new perspective on generalization and model simplicity in machine learning” (Semenova, Rudin, and Parr 2019) Rashomon effect is directly linked to the topic of Interpretable Machine Learning. The authors mention that often when the Rashomon set is large it implies that there may be an explainable model that performs well on the dataset. In the paper they focus on analysing Rashomon effect on different datasets and trying to generalize the statements about what the size of Rashomons set tells us about the machine learning problem. The other important topic connected to Rashomon effect is analysing the variable importance of the model. The question why models with similar performance on the dataset take different variables into considaration for prediction needs to be adressed. This area of research is described in an article called “All Models are Wrong, but Many are Useful: Learning a Variables’ Importance by Studying an Entire Class of Prediction Models Simultaneously.” (Fisher, Rudin, and Dominici 2019) It is emphasised there that there are fields, for example criminal recidivism prediction, where Explainable Machine Learning (including Rashomon effect) is specially important, as the non-explainable models may rely on unacceptable data (sex, race etc). In another article (Dong and Rudin 2020) the authors emphasize the fact that only by comparing many models of similar performance the importance of a variable compared to other variables can be throughly understood. They present the concept of variable importance cloud and draw connection of it to other areas. They research showed that the variable importance may dramatically differ in approximately equally good models. "]]
