[["index.html", "Case Studies Preface", " Case Studies Faculty of Mathematics and Information Science, Warsaw University of Technology 2021-06-04 Preface This book is the result of a student projects for Case Studies course at the Warsaw University of Technology. Each team prepared an article on one of the topics selected from reproducibility, imputation, and interpretability. This project is inspired by a book Limitations of Interpretable Machine Learning Methods created at the Department of Statistics, LMU Munich XAI Stories. Case studies for eXplainable Artificial Intelligence done at the Warsaw University of Technology and at the University of Warsaw, and ML Case Studies during the last year’s course. We used the LIML project as the cornerstone for this repository. The cover created by Anna Kozak. Creative Commons License This book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["technical-setup.html", "Technical Setup", " Technical Setup The book chapters are written in the Markdown language. The simulations, data examples and visualizations were created with R (R Core Team 2018) and Python. The book was compiled with the bookdown package. We collaborated using git and github. For details, head over to the book’s repository. References R Core Team. (2018). R: A language and environment for statistical computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/ "],["explainable-artificial-intelligence-1.html", "Chapter 1 Explainable Artificial Intelligence 1", " Chapter 1 Explainable Artificial Intelligence 1 Author: Anna Kozak Machine Learning is used more and more in virtually any aspect of our life. We train models to predict the future in banking, telecommunication, insurance, industry, and many other areas. The models give us predictions, however, very often we do not know how they are calculated. Can we trust these predictions? Why should we use the results of models which we do not fully understand? This results in a lack of understanding of the results obtained, so there is now a strong need to explain the decisions made by the non-interpretable models called black boxes. There are several tools for exploring and explaining the predictive models, which allow to understanding how they are works. During the class, we explored methods of explaining global as well as local, which you can read more about in the Explanatory Model Analysis (Biecek and Burzykowski 2021) book. Teams work on data from a Kaggle that described problems in the world around us. Each team was responsible for analyzing, modeling, and building explanations for complex models. Each chapter includes a story about how to use explainable AI to understand the model. References Biecek, P., &amp; Burzykowski, T. (2021). Explanatory Model Analysis. Chapman; Hall/CRC, New York. https://pbiecek.github.io/ema/ "],["xai1-explainable-cards.html", "1.1 Explaining Credit Card Customers churns", " 1.1 Explaining Credit Card Customers churns Authors: Katarzyna Solawa, Przemysław Chojecki, Bartosz Sawicki (Warsaw University of Technology) 1.1.1 Global explanations In this section we describe our discoveries made by using explenatory methods globally. 1.1.1.1 Permutational Feature Importance We calculated permutational feature importance for XGBoost model, Random Forest model and a group of logistic regression models. The regression models were created with L1 regularization and different C coefficient was applied among the group. This coefficient is an inverse of a penalty term in L1 regularization, which means the smaller it is, the more weights shrinkage we expect. We examined if such shrinkage is noticeable in Permutational Feature Importance method. Then, we compared PFI obtained from different models. 1.1.1.1.1 Logistic Regression Models Figure 1.1: Permutational Feature Impotance of Total_Ct_Chng_Q4_Q1 for the group of logistic regression models The variable with the highest (among the group of logistic regressions) drop-out loss is shown in 1.1. The drop-out increases with the increase of C coefficient. The feature is more important for models with low regularization parameter, therefore it was shrinked by the Lasso. Figure 1.2: Permutational Feature Impotance of Total_Revolving_Bal for the group of logistic regression models 1.2 presents a variable importance plot of Total_Revolving_Bal feature, which has the second highest drop-out loss. It was not regularized, because drop-out loss decreases with the increase of C. It is the only column, which has this property. Figure 1.3: Permutational Feature Impotance of Gender for the group of logistic regression models Figure 1.4: Permutational Feature Impotance of Avg_Utilization_Ratio for the group of logistic regression models On the other plots, such as 1.3 and 1.4, the shrinkage made by L1 regularization is clearly visable. Models with high regularization parameter, and accordingly low C parameter, have smaller drop-out losses, which indicates lower importance of features. Drop-out loss increases proportionally to C parameter in nearly all of 21 columns. This shows that the effects of Lasso regularization can be observed in variable importance plots of logistic regression models. 1.1.1.1.2 XGBoost and Random Forest models In 1.5 we can observe that the most important column for both models is Total_Trans_Amt. This outcome can be logically explained: customers who do not use their credit card to execute many valuable transactions probably do not need that service, consequently resign. However, the drop-out loss for that column for the XGBoost is over 2 times higher than for the Random Forest, which means that the prior model bases its prediction on this column more than the latter model. Furthermore, more features are important for the XGBoost than for the Random Forest. We suppose this is a result of the models different training processes. New iterations (trees) in XGB are based on observations that were previously predicted incorrectly, thus new columns are taken into consideration to represent the differences between the observations. On the other hand, the Random Forest model selects the subset of the features randomly in each tree. Figure 1.5: Top 9 most important features in XGBoost and RandomForest feature importance comparison 1.1.1.1.3 Models comparison We compared the permutational feature importance of the group of logistic regression models, XGBoost model and Random Forest model. We can see in 1.6 the drop-out loss in XGBoost is similar to drop-out in logistic regression models. Figure 1.6: Permutational Feature Impotance of Total_Trans_Amt for all models If we compare the importance of Total_Revolving_Bal in 1.7, we see a huge difference between tree based models and regression models. The drop-out loss for the first ones is around 20 times lower than for the latter. Figure 1.7: Permutational Feature Impotance of Total_Revolving_Bal for all models Figure 1.8: Permutational Feature Impotance of Gender for all models Figure 1.9: Permutational Feature Impotance of Avg_Utilization_Ratio for all models We can also examine some of the less important features such as Gender (see 1.8 ) and Avg_Utilization_Ratio (see 1.9). In comparison to regression models importance of these variables in XGBoost and Random forest is neglectable. Therefore, we conclude that although the effects of L1 regularization in logistic regression are observable, tree-based models such as XGBoost and Random Forest select the most important features more restrictively. 1.1.1.2 PDP profiles We created Partial Dependence Plots of all variables in the dataset for XGBoost, Random Forest and Logistic Regression with L1 models. Many of the plots turned out to be a horizontal line located on the level of the mean prediction of the models. An example of such a variable is shown in 1.10, predictions of models does not change with the change of Gender. However, features that have high importance do have more complex plots. One can observe prediction varying with the change of Total_Trans_Amt, Total_Revolving_Bal or Total_Ct_Chng_Q4_Q1. Figure 1.10: Partial Dependence Plots of chosen features What we find interesting in 1.10 is an unobserved earlier effect of the Contacts_Count_12_mon variable. The plot is steady for values 1-5 and raises rapidly when the feature takes the value of 6. We examined this case and figured out, that only approx. 0.58% of all observations have value 6 in Contacts_Count_12_mon column. What is more, all of them describe attrited customers. We concluded there are two possible solutions: The dataset is not balanced for this feature. Indeed, the 6th contact with the bank representative is a breakthrough in the relationship with the customer. 1.1.1.3 ALE profiles Figure 1.11: Accumulated-local Profiles Plots of chosen features Accumulated-local Profiles for XGBoost, Random Forest and Logistic Regression with L1 were calculated. The results for chosen variables are shown in 1.11. ALE plots seem to be very similar to PDP profiles. It may suggest there are no interactions between variables in the models. To examine that we plotted both PDP and ALE profiles in 1.12, 1.13. We skip these plots for Logistic Regression because, by definition, there are no variables interactions in this class of models. ALE and PDP plots are parallel thus models detected no interactions between features and they are additive. Figure 1.12: Accumulated-local Profiles and Partial Dependence Profiles Plots of chosen features for XGBoost Figure 1.13: Accumulated-local Profiles and Partial Dependence Profiles Plots of chosen features for Random Forest "],["xai-in-real-estate-pricing-a-case-study.html", "1.2 XAI in real estate pricing: A case study", " 1.2 XAI in real estate pricing: A case study Authors: Sebastian Deręgowski, Maciej Gryszkiewicz, Paweł Morgen (Warsaw University of Technology) 1.2.1 Abstract Lorem ipsum 1.2.2 Introduction Real estate plays a key role in past and present economy. In 2011 it was valued in total $25 trillion in the US only, $16 trillion of that figure comes from residential households. To compare, at the and of that year the capitalisation of US stock market was around $18 trillion. In its prices are interested both ordinary people, who treat real estate as basic good, and investors, who view the real estate as asset. The task of determining a relationship between dependent factors and final price is non-trivial and requires a vast, domain-specific knowledge. However, machine learning (ML) methods allow to surpass this requirement by determining the interaction in data themselves and leaving only the task of data selection and learning supervision to the human user. This comes with a cost, though: the most efficient ML models are of the so-called black box family and they are complex beyond human understastanding. If they are to be fair, trusted and fully usable, their decision process has to be brought closer to the human user. This article describes research performing a case study based on the actual US Census data from California from 1990. An ML model tasked with predicting real estate value is developed and then its decisions are made as clear as possible, using various XAI methods. 1.2.3 Related Work 1.2.3.1 Characteristic of real estate market The real estate market is not a typical one, as is pointed out in the introduction of (Ghysels et al. 2013). It is extremely heterogenous - goods offered vary between themselves in location, state and other physical attributes. It is also nonliquid, since transactions are costly and irreversible. This makes the task of forecasting a price of a given house particularly difficult. Instead, certain indicators of the price are predicted, the simplest one of them being the median house value. 1.2.3.2 Determinants of real estate value The factors influencing the real estate prices can be linked with four phenonena (Belke and Keil 2017): Future expected revenue, Accessibility, Hedonic factors, State of global and local economy. In an asset pricing approach expected revenue plays a key role. Here factors such as forecasted future rent, upkeep cost and taxes are taken into account. A second approach focuses on accessibility. It takes into account affordability and sustainability of house prices. It brings, among others, the montlhy income of potential owners into equation. Hedonic factors are object-specific and neighbourhood-specific characteristics, which contribute to the real estate value. Their examples are local population, housing density and infrastracture accessibility (such as public transportation, healthcare or education facilities). Finally, the state of global economy contributes to the price, and the state itself is measured by indicators like GDP, inflation rate, unemployment rate and construction activity. 1.2.3.3 Machine Learning uses 1.2.4 Methodology 1.2.4.1 Data description 1.2.4.2 Data validity Data accessible to us sets the boundaries of our research. It contains characteristics of households in California at one, specific point in time, so incorporating global economic indicators makes no sense. Many desirable predictors, such as number of bathrooms in a household or the exact state of local infrastracture are simply not available. In this paper we focus on accessibility and hedonic factors. The presence of exact location is vital to the credibility of this research. Factors connected with household location, which could have influence on final price, are numerous. Leaving some of them out could lead to selection bias present in the results of our work (Ghysels et al. 2013). Thankfully, any factors connected with location - such as the local unemployment or state of infrastracture - are tied with geographical coordinates and are present in our data, although implicitly. 1.2.4.3 Model 1.2.5 Results After successful training of the aformentioned model, we employed XAI methods of local and global explanation in order to get a firmer grasp over model’s decision process. 1.2.5.1 Local explanations At this stage of our work, we were particulary interested in observations that were somehow unusual or characteristic. In order to study them, we used prediction decomposition methods (LIME, Break Down, Shapley Values or Ceteris Paribus). 1.2.5.1.1 Most missed predictions The first record that caught our attention was the one with the most inaccurate prediction. Observation No. 18563, as it is in question, was worth $450,000. Our model priced it at just $121,105. A Shapley Values plot for this observation (1.14) threw some light at the matter: Figure 1.14: Shapley Values for observation No. 18563 population variable has the biggest impact on price in this case, it has increased the median price by nearly $70,000. This result was quite puzzling, since this variable was not considered as majorly significant impact on the predictions. Even more suprising was the fact that the variable median_income, which in most cases decided about the prices of an apartment almost by itself (with high efficiency), turned out to be grossly mistaken. In order to explore it in more detail, the area where this flat was located and visualised using Google Maps service. Figure 1.15: Street View for observation No. 18563 Although the property itself is located near a state road, there is nothing more that would let us convince that this flat is worth its high price, even when taking the 30 years gap between the data collection time and present time into account. In this case, a wrong prediction of the model was not surprising. The second most misguided prediction (No. 15804) also significantly lowers the actual price of the flat ($147,083 to $475,000). If we have a look at a plot below (1.16) we can note that it is similar to previous one. Figure 1.16: Shapley Values for observation No. 15804 Again, the most important variable is population, which correctly attempts to increase the price of the home. Again, the value of the flat is underestimated by tagging the property with the INLAND category. This time however the median_income variable has very little impact on the prediction. Checking the view of the area in Google Maps (1.15) once again clarifies the situation. Here we can see an estate of luxurious villas. If the area was similar 30 years ago, it could indeed be worth $475,000. However, it is quite unusual for such a rich estate to be constructed in that kind of location, so again the underestimation is not a surprise. Figure 1.17: Street View for observation No. 15804 Next in line, we looked at the observation for which our model overestimates the real value of the apartment the most. Observation No. 5168, as it is in question, turned out to be located in Los Angeles, 10 minutes far from the beach, in a rather rich-looking housing estate. Considering this, it is not surprising that our model valued this property at $302,868 while it was really worth only $55,000. Figure 1.18: Street View for observation No. 5168 Figure 1.19: Location of observation No. 5168 in Los Angeles If we take a closer look at the Shapley Values decomposition plot for this prediction (1.20), we can notice that this time the two most important variables turned out to be latitude and londitude. Figure 1.20: Street View for observation No. 5168 As we can see, our model has learned that apartments located close to or in Los Angeles itself are usually worth much more than those outside of the city. Only a low value of the median_income variable could suggest that the apartment is not as expensive as it might seem (in fact, it is not expensive at all), but still there were more hints that it is rather a luxurious property. 1.2.5.1.2 Same area, different price At the end of our work on local explanations, we looked at two examples of houses, which, despite their proximity, turn out to be significantly different in terms of their price. Our focus is in the Los Angeles area, and our observations are the ones numbered 4526 and 4537 (1.21). To the nearest hundredths of a degree they are in the same place, this is the Korean district of Los Angeles, near one of UCLA campuses (figure below). Still the price difference is as high as $400,000. Figure 1.21: Location of observation No. 4526 and 4537 in Los Angeles In order to explore these results, Break Down decompositions for these observations were created(1.22 and 1.23). The model did not fully detect the difference between the households. The cheaper was overestimated ($205,000 instead of $75,000), and the more expensive was underestimated ($320,000 instead of $475,000). Nevertheless, the impact of individual variables can be observed. The key differences are the households and rooms_per_household variables. Interestingly, the model behaved differently for the longitude variable - its contribution is negative for the cheaper one, and positive for the more expensive one. Figure 1.22: Break Down for observation No. 4537 Figure 1.23: Break Down for observation No. 4526 1.2.5.2 Global explanations At this stage, an the analysis of the impact of entire variables on a trained model was performed. Global explanation methods (permutation significance, utility and partial dependency profile (PDP)) were used to study them. 1.2.5.3 Importance of variables The permutational significance method was used to determine which variables actually contributed to the performance of the trained model (1.24). We have observed that the most significant variable is median_income. Figure 1.24: Variables importance Real estate is about location, location, location. One of our first interests was the influence of location on the value of an apartment. The PDP for the longitude and latitude variables is shown in the figure below (1.25). Figure 1.25: PDP for longitude and latitude variables Properties to the west (shorter longitude) and south (lower latitude) are preferred. The first result is quite intuitive - to the west it is closer to the ocean, there are large agglomerations located there. The second is less expected - perhaps related to a warmer climate or better economic indicators. Let’s take a closer look at the dependencies (see: 1.26). We see a large drop in the value around 122°20’W (San Francisco area), then similar values up to a slow decline from 118°20’W to 117°20’W (Los Angeles and San Diego). As for the width, the location of these cities is again very important - we see a decrease between 33°30’N and 34°30’N (San Diego, Los Angeles) and a huge decrease between 37°30’N and 38°N (San Fransisco). We can see that the places of declines in the charts can be translated into the location of large agglomerations, but this does not explain the trend. Local growth, and not global decline, would be expected. Figure 1.26: Map of California Next in line we were interested in the most important variable for the model - median_income. Its influence on the model is shown in the figure below (1.27). Figure 1.27: PDP for median_income variable It was no suprise that residents’ earnings have a considerable impact on the price of an apartment. It is worth noting to what extent does the median_income variable influence the final result of the prediction. For the lowest-income households, the model will predict a property value of around $133,000, while for the most affluent households, the prediction will be around $353,000. So it is more than 2.5 times increase in the expected price. Moreover, the relationship is not completely linear - from a certain point, the increase in earnings does not translate into an increase in the price of the apartment. In the next step, we checked how other, less significant variables influence the model response (1.28). Although the graphs look very varied, the ranges of values on the vertical axis are small, as is the overall importance of these variables. For instance, population variable has the largest range of values, from $177,000 to $226,000. This range is inferior to ones observed in previous variables. Nevertheless, some interesting dependencies on the basis of the plots can be learned. From the plot for housing_median_age, a general trend that older properties tend to be a bit more expensive can be observed. The total_bedrooms plot shows that houses in blocks with a very low combined total of bedrooms are noticeably cheaper. The population variable distinguishes houses in households with a very low total population. These houses are usually more expensive. In turn, the households plot shows that a significantly low number of houses in the block reduces the predicted house price. This is in apparent opposition to the conclusions of the population variable. However, it should be noted that those considerations apply only when the values of the remaining variables are the same. If the number of houses increases, the amount of space per capita decreases and the price goes down. On the other hand, when the number of households increases with a dedicated population, the amount of space per inhabitant increases and the price goes up. This apparent abberation is explored in next section further. rooms_per_household variable has a small impact on house prices, but a decline is noticeable for houses with a low average number of rooms. Figure 1.28: PDP for other continuous variables 1.2.5.4 Doubts about household and population variables During the EDA, we found a correlation between households and population variables. This could have a negative impact on the interpretability of the PDP plotss and the meaningfulness of the information they convey. To verify that, we performed the following procedure: The population variable was divided into 5 intervals so that a similar number of observations fell into each interval, A PDP curve was determined for each interval, Differences between the determined curves were observed. If there were no differences, interactions in the model between variables would be non-existent and their correlation would only account for results noted on PDPs. The results of the verification in the figure below (1.29). Figure 1.29: Importance of population variable wrt. household variable We observed a different appearance of profiles for large values of households than for small ones. Hence, we concluded that there are interactions between these variables in the model, so the conclusions presented in the previous section are valid. 1.2.6 Summary and conclusions Over the course of this case study, after basic EDA and training of an exemplary ML model, various XAI methods were used to make the process of real estate pricing clearer. Local explanation of odd outcomes gave detailed reasons for which the model gave incorrect results. When compared with intuition and with actual photos of the households, it was clear, that these observations were unusual and the model did as expected. Global explanations allowed us to get a sense of the relationship between individual variables and the median house value. We conclude, that the methods employed made the decisions made more transparent and believe, that these methods have high potential of use in other fields. References Belke, A., &amp; Keil, J. (2017). Fundamental determinants of real estate prices: A panel study of german regions, (731). Ruhr Economic Papers. https://doi.org/10.4419/86788851 Ghysels, E., Plazzi, A., Valkanov, R., &amp; Torous, W. (2013). Chapter 9 - forecasting real estate prices, 2, 509–580. https://doi.org/https://doi.org/10.1016/B978-0-444-53683-9.00009-8 "],["xai-heart-disease.html", "1.3 How not to have broken heart &lt;3", " 1.3 How not to have broken heart &lt;3 Authors: Przybyłek Paulina, Rólkiewicz Renata, Słowakiewicz Patryk 1.3.1 Introduction "],["xai1-explainable-wine.html", "1.4 Red wine mystery: using explainable AI to inspect factors behind wine quality", " 1.4 Red wine mystery: using explainable AI to inspect factors behind wine quality Authors: Jakub Kosterna, Bartosz Siński, Jan Smoleń (Warsaw University of Technology) 1.4.1 Abstract Wine is one of the most widespread and culturally significant drinks in the world. However, the factors determining its quality remain a mystery to the absolute majority of people. There are many variables contributing to the final effect and it seems unclear which ones are crucial in making some wines better than the others. In this paper, we have looked at this issue from a fresh perspective using Red Wine Quality from Kaggle community dataset. Much to our initial surprise, despite the fact that a dozen or so chemical factors were taken into account, there is one that stands out and seems to be the main predictor of the drink’s quality - it is alcohol. The study used four black box models interpreted through modern methods of explainable artificial intelligence to explore the subject. 1.4.2 Introduction and Motivation Term ‘glass box models’ refers to interpretable machine learning models - user can explicitly see how they work, and follow the steps from inputs to outputs. The situation is completely different in the case of the very advanced black box models. The goal of explainable machine learning is to allow a human user to inspect the factors behind results given by the model (Baniecki et al. 2020a). There are numerous projects on the Internet that look at the Red Wine Quality dataset from different perspectives. However, due to the nuanced nature of the problem, many of them don’t allow us to draw any constructive conclusions concerning the impact that various physicochemical properties have on the quality of wine. Our goal was to implement XAI solutions in terms of the analysis of the above-mentioned dataset and to confront results with previous research and literature on the subject. In this study, we will be taking a look at all the variables included in our dataset, while paying some special attention to one that seems to be standing out the most - the alcohol content. It is also the one that is the most recognizable to an average consumer and, in contrast to other physicochemical properties, is easily found on every wine label. While analysing the results, we have to keep in mind the obvious limitations associated with the subject. Not only is the perception of the quality of wine an inherently subjective property, but it is also affected by factors not included in the data, such as the color of the wine or the temperature in which the drink was served. 1.4.3 Methodology 1.4.3.1 Dataset The original collection contains 1 600 observations, each representing one Portuguese Vinho Verde of the red variety. It is a proper to analyze and respected set, as evidenced by its verification, multiple use, as well as a very high rating of “usability” of 8.8 on the website. It consists of eleven predictors: fixed acidity - most acids involved with wine or fixed or nonvolatile (do not evaporate readily) volatile acidity - the amount of acetic acid in wine, which at too high of levels can lead to an unpleasant, vinegar taste citric acid - found in small quantities, citric acid can add ‘freshness’ and flavor to wines residual sugar - the amount of sugar remaining after fermentation stops, it’s rare to find wines with less than 1 gram/liter and wines with greater than 45 grams/liter are considered sweet chlorides - the amount of salt in the wine free sulfur dioxide - the free form of SO2 exists in equilibrium between molecular SO2 (as a dissolved gas) and bisulfite ion total sulfur dioxide - amount of free and bound forms of S02 density - the density of water is close to that of water depending on the percent alcohol and sugar content pH - describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic) sulphates - a wine additive which can contribute to sulfur dioxide gas (S02) levels, wich acts as an antimicrobial and antioxidant alcohol - alcohol by volume percentage The decision variable was originally quality - the median rating of an assembly of minimum 3 experts, who made their classification on a scale from 0 to 10. Due to the capabilities of the analyzed XAI tools, our team decided to make it a binary classification problem, assigning the wines rated &lt;= 5 value 0, and others - value 1. It resulted in an intuitive “bad / good” wine classification, which gave us 855 “good” and 744 “bad” wines. Figure 1.30: Distribution of target before and after transformation 1.4.3.2 Machine learning algorithms used In order to look at the nature of this data, four well-known algorithms have been trained from data divided into: 1199 observations for the training set and 400 for the test set. XGBoost (gbm) - powerful modern method based on AdaBoost and gradient boosting, imported from xgboost package, with tuned hyperparameters using the randomized search method from sklearn package, with the best values obtained: min_child_weight - 1, max_depth - 12, learning_rate - 0.05, gamma - 0.2 and colsample_bytree - 0.7 Support Vector Machine (svm) - algorithm, in which we plot each data item as a point in n-dimensional space (where n is number of features) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiates the two classes very well; imported from sklearn package, with tuned hyperparameters using the grid search method from sklearn package, with the best values obteined: C - 10000, gamma - 0.0001 and kernel - rbf Random Forest (rfm) - method building multiple decision trees and merges them together to get a more accurate and stable prediction; imported from sklearn package, with tuned hyperparameters using the randomized search method from sklearn package4, with the best values obtained: n_estimators - 2000, min_samples_split - 2, min_samples_leaf - 2, max_features - auto, max_depth - 100 and bootstrap - True Gradient Boosting (xgm) - a type of machine learning boosting. It relies on the intuition that the best possible next model, when combined with previous models, minimizes the overall prediction error; imported from sklearn package, with tuned hyperparameters using the grid search method from sklearn package, with the best values obteined: learning_rate - 0.1, max_depth - 7 and n_estimators - 50 All methods have been implemented in Python with random states set to 42. The choice of these methods was made on the basis of their popularity, diversity and practicality, taking into account the essence of the problem under consideration. After checking the operation of the models on the test set, the following quality measures were obtained: algorithm accuracy precision recall ROC AUC XGBoost 0.8050 0.806306 0.836449 0.802633 Support Vector Machine 0.7475 0.798942 0.705607 0.750653 Random Forest 0.7850 0.801887 0.794393 0.784293 Gradient Boosting 0.8075 0.818605 0.822430 0.806376 1.4.4 Global explanations 1.4.4.1 Permutation-based variable importance Figure 1.31: Permutation-based variable importance plot for tuned XGBoost model. In this case, size of the bar indicates the positive impact that a feature has on accuracy of the model Our starting point was rendering a permutation-based variable importance plot from DALEX package using our XGBoost model to examine which variables play the biggest role in model’s decision (Baniecki et al. 2020a). The goal of this method is to inspect which variables have positive impact on the accuracy of the prediction. Surprisingly, all of them seemed to provide information that benefited the accuracy of the model. This, combined with a relatively small size of our dataset, led us to decision not to exclude any variables in further proceedings. However, the importance of the variables varies greatly - alcohol and sulphates together overcome all other factors combined. 1.4.4.2 Mean absolute SHAP value Figure 1.32: Mean absolute SHAP value for tuned XGBoost model. Size of the bar indicates how much a feature influences the prediction on average In order to gain a different perspective, we examined a plot of mean absolute SHAP values of our model (Scott M. Lundberg and Lee 2017). SHAP method is explained in chapter Break Down and Shapley Additive Explanations (SHAP). Mean absoulte SHAP value can give us insight into which variables influence the predictions the most. Although this method differs significantly from the above-mentioned permutation-based variable importance, it provided us with a similar information on the hierarchy of importance of the variables - once again, alcohol seems to be the biggest factor, followed by sulphates. Analysing those plots encouraged us to take a closer look at the role that alcohol content plays on the prediction. 1.4.4.3 PDP Figure 1.33: Partial Dependence Plot for all models. Lines represent the average prediction by the model if all observations had a fixed value of a feature, in this case - alcohol content Next, we used Partial Dependence Plot from DALEX package (Baniecki et al. 2020a) to examine an overall effect the alcohol content has on our predictions. PDP tells us what would the average prediction be, if all observations had a fixed value of certain variable. This time, we used all of our models. As expected, tree-based model (XGBoost, GradientBoosting and Random Forest) behave very similarly, while SVM stands out from the rest. However, the main trend is the same - generally, stronger wines are more likely to be good than the weaker ones. Most models reach their lows at just below 10% alcohol content and plateau around 12%. We have to keep in mind that our data contains only one wine that exceeds 14% alcohol content, which may explain the flat line after that mark for tree-based models. It should be noted that this strongest wine was actually ranked as not good. The fact that the line representing SVM model keeps increasing in value throughout the whole range may indicate that this model is more prone to outliers and doesn’t capture complex interactions as well as other models. It coincides with our prior knowledge of the models and the accuracy scores they achieved. 1.4.4.4 ALE Figure 1.34: Accumulated Local Effects Plot for all models. Lines represent how predictions change in a small range around the value of a feature, in this case - alcohol content. In order to take into account possible interactions concerning our variable, we generated an ALE plot using all of the models (Apley and Zhu 2020). This methods shows us how model predictions change in a small “window” of the feature around certain value for data instances in that window. The results turned out to be quite similar to PDP. It makes sense, considering the fact that our dataset doesn’t contain many strongly corelated variables, and the one that is related the most to alcohol, the density, doesn’t have a very significant impact on the predictions made by the models. The most visible difference is probably the way SVM behaves for the biggest alcohol values - it doesn’t seem to differ as much from the other models as in the previous plot. 1.4.5 Local explanations 1.4.5.1 Break Down and Shapley Additive Explanations (SHAP) The first method that we have used for our local explanations is Shapley Additive Explanations (SHAP). It is based on another explanatory method called Break-Down, which similarly to SHAP is used to compute variables’ attribution to the model’s prediction (Staniak and Biecek 2018). Break-Down method fixes values of variables in sequence and looks at changes in label value. Moreover, as we can see in figure Figure 1.35 this method is sensitive to the order of variables. Figure 1.35: BreakDown plots for one observation with different order of fixed variables. Size and color of bars indicates changes in model’s predictions. All three presented plots describe the same observation with different sequences of examined variables. When critic acid was fixed as the first one, it lowered the prediction. However, the second plot presents a situation where critic acid was second fixed variable and its value showed a positive impact on rising wine’s rating. This dependency on order made it harder for us to conclude about specific wines. Fortunately Shapley Additive Explanations method (SHAP) resolves this issue by averaging results for different permutations of variables (Scott M. Lundberg and Lee 2017). As an example, you can see computed Shapley values for same observation in Figure 1.36 . Figure 1.36: Plot of Shapley Values for good wine. Size and color of bars indicates mean change in model’s prediction over different orders of fixed variable. Here our model was certain about assigning wine to good ones. Nearly all factors contributed to increasing wine’s rating. Not surprisingly, alcohol and sulphates had the biggest influence on the model’s prediction. Only free sulfur dioxide and pH had worsened the quality of wine, all of which, according to global explanations, are practically irrelevant variables to our model. On the contrary Figure 1.37 presents an observation where all attributes except volatile acidity lowered the rating of our wine. From the values of particular variables, we can see that these wines are completely different, which explains model’s distant assessments. Furthermore, exact values that our model understands as beneficial for wine start to emerge. To get a closer look we will have to use new method described in the following section. Figure 1.37: Plot of Shapley Values for bad wine. 1.4.5.2 LIME In order to look at the explainability of individual predictions, the LIME method from DALEX package was used (Baniecki et al. 2020a). When looking at individual visualizations, it is easy to see that extreme alcohol values have the greatest impact on the prediction value. This is clearly visible on the example of incorrectly classified observations - in a significant number of cases it is the non-standard value of alcohol that largely determines the incorrect prediction. The situation is well illustrated by visualizations generated on the basis of two observations which all of our models misclassified. Figure 1.38: Visualization of LIME method for wine incorrectly classified as bad. Left section represents prediction probability. Middle section displays importance of features and right section their exact values. Blue color indicates that variable is supporting class 0 (bad wine) and orange that variable is supporting class 1 (good wine). Figure 1.39: Visualization of LIME method for wine incorrectly classified as good. However, in this case, alcohol does not always dominate, and the prediction result for the XGBoost model is indeed a component of many elements. However, it has to be considered that the LIME method in this case takes into account only certain automatically selected intervals, which, when combined, do not fully represent the essence of the algorithm. 1.4.5.3 Ceteris Paribus Analyzing results from LIME brought to light yet another issue. Is it possible to change wine’s rating by altering the alcohol content? Answers to this question were provided by a method called Ceteris Paribus. “Ceteris paribus” is a Latin phrase meaning “other things held constant” which accurately describes the main concept of method. Ceteris Paribus illustrates the response of a model to changing a single variable while fixing the values of others. As an example, we will use observation from the previous section which our model incorrectly labeled as the bad one. We quietly assumed that it was due to low alcohol content, but now we can verify our thesis. Figure 1.40: Ceteris-paribus profile for all models. Lines represent how the models’ predictions change over different alcohol contents while keeping other variables fixed. Dots represents actual value of alcohol content in examined misclassified wine. Indeed if our wine was less than a half percent stronger, the model would properly give it a higher rating. However, rising alcohol levels in not always an answer. For instance, very strong wine with 14.7% alcohol presented in Figure 1.41 would benefit from a reduction of alcohol content. Figure 1.41: Ceteris-paribus profile for alcohol content in a very strong wine. 1.4.6 Confrontation with science The subject of the quality of wines in relation to their alcohol content has been studied many times before by other scientists and enologists. The topic was covered, among others, on a post on Cult wines blog (England 2019). In 2019, the author pointed that “It is true that alcohol in wine tends to draw out more intense, bold flavours, so the higher the alcohol level, the fuller the body.” Certainly, the phenomenon is also confirmed by the popular fact of preferences of esteemed former wine critic Robert Parker, who was well known for awarding higher scores to higher alcohol wines. On the other hand, lower alcohol wines tend to offer greater balance and pair better with foods. Hence, too much content is also not an advantage in terms of quality. The phenomenon was also examined by the newspaper the Seattle Times, which published an article in 2003 entitled Does a higher alcohol content mean it’s a better drinking wine? (Gregutt 2003). The expert emphasizes that higher alcohol is an indication of better ripeness at harvest and fermentation to complete or near-complete dryness. With time, the wines are also getting stronger, which is the result of better vineyard practices, letting the grapes get more “hang time” and more efficient yeasts, which definitely has a positive effect on the quality. Here, however, the argument for not too high alcohol content is also emphasized, due to the fact that wines with more than 15 percent are almost never ageworthy. The high alcohol throws the balance off and is often accompanied by too much oak and too much tannin. These wines are also hard to drink, as they do not match well with most foods. It is also worth comparing an article from the Decanter website (Jefford 2010) with the results of our study. As a person well acquainted with the subject, the expert considers the alcohol threshold of a good wine, coming to a conclusion consistent with the results of our calculations. Professional research was conducted in 2015. A group of Portugese experts conducted a thorough research, the result of which was the article From Sugar of Grape to Alcohol of Wine: Sensorial Impact of Alcohol in Wine (Jordão et al. 2015). They indicated in it that the quality of grapes, as well as wine quality, flavor, stability, and sensorial characteristics depends on the content and composition of several different groups of compounds from grapes. One of these groups of compounds are sugars and consequently the alcohol content quantified in wines after alcoholic fermentation. During grape berry ripening, sucrose transported from the leaves is accumulated in the berry vacuoles as glucose and fructose resulting in a fuller taste of a final wine. The idea of a threshold of an alcohol content, beyond which wines lose their quality, is a common theme in literature dedicated to the topic. There is evidence suggesting that wines that have more than 14.5% of alcohol start to come off as herbaceous instead of fruity (GOLDNER et al. 2009). The research states that the sensory perception of the aroma changed dramatically with the level of ethanol content in the wine, but the change isn’t consistent among all chemical compounds. Another very interesting topic is the idea of reducing the alcohol content in wine after initial production process (Jordão et al. 2015). Motivation behind such actions is to try to keep all the good aromas and qualities associated with stronger wines, while simultaneously lowering excessive alcohol which causes bitter or sour tastes. Such attempts ended in mixed results, but evidence suggests that it may be possible to lower alcohol content by one percentage point without losing many aromas. 1.4.7 Summary The analysis of the results of modern methods of explainable artificial intelligence showed a relatively unambiguous conclusion - taking into account the proven data set of popular wines assessed by experts, the alcohol content definitely has a large impact on the average rating of wines. Despite taking into account many different chemical factors, as well as the variety of black box models and the differences between individual observations, alcohol is a factor that stands out from other predictors. This result is evidenced not only by the high correlation and simple conclusions from the visualizations, but also by the results of the algorithms explaining the results of the models used. However, the methodology has produced less obvious conclusions - although higher alcohol content is associated with higher alcohol quality (at least in a pseudo-objective sense), there is a threshold at which this property reverses. These considerations are the result of measuring not only one or two approaches, but a total of research based on as many as seven methods. It is a good argument to appreciate both them and modern solutions of explainable artificial intelligence. Indeed, the results of information and mathematical analysis are also consistent with what we can observe in nature - the effects of the calculations made are consistent with previous research related to the topic, and not necessarily based on methods related to the field of data science. 1.4.8 Conclusions The experiment is undoubtedly successful and with a high degree of certainty confirms two facts: firstly, the quality of the wines is dependent on their higher alcohol content, but up to a certain threshold and secondly, modern methods of explainable artificial intelligence provides us valuable results that bring new valuable information about the black box models, which at the same time are consistent with the nature of the data sets on which they were made. Different approaches provide us with various useful information and it is difficult to indicate better or worse solutions - the seven methods mentioned complement each other in their own way, but each allows us to look at the problem of analysis from a different perspective. Leaving aside our succesful implementation of XAI solutions, one must bear in mind, that the results of the experiment need to be put in a certain perspective. It must be remembered that the world of wines is a very diverse one. For example, artificially fortified wines like port wine or sherry should be considered separately. The discussed data set concerns only a certain group of wines and we don’t know enoguh about the experts or conditions of the tastings to call the data entirely representative. Therefore, we should remain cautious when drawing any far-reaching conclusions. Nevertheless, the conducted research approximates the idea of the overall effect of alcohol on the quality of wine and confirms previous research by experts from a biochemical point of view. References Apley, D. W., &amp; Zhu, J. (2020). Visualizing the effects of predictor variables in black box supervised learning models. Journal of the Royal Statistical Society Series B, 82(4), 1059–1086. https://doi.org/10.1111/rssb.12377 Baniecki, H., Kretowicz, W., Piatyszek, P., Wisniewski, J., &amp; Biecek, P. (2020a). dalex: Responsible Machine Learning with Interactive Explainability and Fairness in Python. arXiv:2012.14406. https://arxiv.org/abs/2012.14406 England, R. (2019). Wine’s alcohol levels explained. https://www.wineinvestment.com/wine-blog/2019/05/wines-alcohol-levels-explained?fbclid=IwAR3xpQITEQZrQUPPaEt7-DbFHmvHE559-iVuLsgS6dDinOeWrl04MZiglbM. GOLDNER, M. C., ZAMORA, M. C., DI LEO LIRA, P., GIANNINOTO, H., &amp; BANDONI, A. (2009). EFFECT OF ETHANOL LEVEL IN THE PERCEPTION OF AROMA ATTRIBUTES AND THE DETECTION OF VOLATILE COMPOUNDS IN RED WINE. Journal of sensory studies, 24(2), 243–257. Gregutt, P. (2003). Does a higher alcohol content mean it’s a better drinking wine? The Seattle Times. https://archive.seattletimes.com/archive/?date=20031008&amp;slug=wineqanda08&amp;fbclid=IwAR3lBlpdwUCUWjWKaH4Px21b9fJQwBT0aMTa8bNWCbx4ipo4otWzvR9_mTc Jefford, A. (2010). Alcohol levels: The balancing act. https://www.decanter.com/features/alcohol-levels-the-balancing-act-246426/?fbclid=IwAR0bsIWug6-7l77rxb01Va8P1F_hVkaUTacNtlF-V-wRXb1HA3rJXpl74Pw. Jordão, A. M., Vilela, A., &amp; Cosme, F. (2015). From sugar of grape to alcohol of wine: Sensorial impact of alcohol in wine. Beverages, 1(4), 292–310. https://doi.org/10.3390/beverages1040292 Lundberg, Scott M., &amp; Lee, S.-I. (2017). A unified approach to interpreting model predictions. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, &amp; R. Garnett (Eds.), Advances in neural information processing systems 30 (pp. 4765–4774). Montreal: Curran Associates. http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf Staniak, M., &amp; Biecek, P. (2018). Explanations of model predictions with live and breakDown packages. "],["xai1-explainable-hotels.html", "1.5 eXplaining predictions of booking cancelations", " 1.5 eXplaining predictions of booking cancelations Authors: Mateusz Krzyziński, Anna Urbala, Artur Żółkowski (Warsaw University of Technology) 1.5.1 Introduction One of the biggest problems and challenges facing the hospitality industry is the significant number of canceled reservations. Common reasons for cancellations include sudden deterioration in health, accidents, bad weather conditions, schedule conflicts or unexpected responsibilities (Falk and Vieru 2018). Interestingly, a noticeable group consists of customers who, after making a reservation, are still looking for new, better offers, and even make many reservations at the same time to be able to choose the most advantageous one (Antonio et al. 2017). The hospitality industry’s response to the above problem are hotel cancellation policies. They play a crucial role in determining various aspects of the hotel business, including the ultimate goal of revenues and profits optimisation. In recent years (before the pandemic), there has been a clear tightening of these policies. Hotels do this, for example by shortening the free cancellation windows or increasing cancellation penalties (Riasi et al. 2019; Smith et al. 2015). The use of machine learning to forecast and identify potential cancellations is also playing an increasing role. There are many systems to support hotel management that use booking data. Various machine learning algorithms are used for this purpose, ranging from support vector machines, through artificial neural networks, to the most common tree-based models (Andriawan et al. 2020; Sánchez-Medina and C-Sánchez 2020). Most of the solutions and projects are only theoretical, while some have been tested in practice, enabling cancellations to be reduced by up to 37 percentage points (Antonio et al. 2019). Unfortunately, most papers do not tackle the issue of the importance of the used explanatory variables and do not try to explain the model’s predictions. However, it is the exploration of trained models that should be treated as one of the key factors in the design of hotel management support systems. Business validation and ethical verification of solutions is necessary. Bearing in mind that a strict cancellation policy or overbooking strategy can have negative effects on both reputation and revenue, systems designers should be wary of unfair biased behaviour. At the same time, the use of explanatory artificial intelligence methods is helpful in creating models with better performance scores. In the following chapter, we present an analysis of predictive models for hotel bookings cancellations. We answer questions about the reasons for the model prediction both in general view and in relation to individual reservations. 1.5.2 Dataset and models 1.5.3 Local explanations In order to explain model output for a particular guest and their booking, we used instance-level exploration methods, such as Break-down, SHAP (SHapley Additive exPlanations), LIME (Local Interpretable Model-agnostic Explanations), CP (Ceteris Paribus). We decided to investigate noteworthy predictions, i.e. false positive and false negative (respectively canceled bookings predicted as not canceled and vice versa), the most valid (the predictions the model was most sure of), and the closest to decision boundary. 1.5.3.1 False positive and false negative predictions We might discover that the model is providing incorrect predictions. The key is to find the reasons for this, that is, to answer the question what has driven the wrong prediction. We used local explanations methods for the observations in both groups with the worst predictions, i.e. the lowest probability of proper classification. Figure 1.42: A plot of Shapley values for random forest model and misclassified observation (false positive) with the highest probability of cancellation. The green and red bars correspond to the contribution of the variable to the prediction. The green ones increase the probability of cancellation, while the red ones decrease it (increase the probability of no cancellation). On the x-axis, there is the model prediction value, while on the y-axis there are variables and their values for the observation. Informations with the biggest contribution to the final prediction are guest’s country of origin (Portugal), a total number of special requests equals zero, and the fact that customer type related to the given booking is Transient-Party. This is an indication that the model may be slightly biased due to the country of origin. It is the property of the customer and not of the booking itself. Thus, depending on the application, it is worth considering whether this response is satisfactory and meets ethical standards. With every value contributes to the misprediction, the only feature with the correct contribution is customer type (Transient-Party guests are the most popular type of customers, accounting for as much as 75% of bookers). Figure 1.43: A plot of Shapley values for random forest model and misclassified observation (false negative) with the lowest probability of cancellation. The elements of the plot have the same meaning as in the previous case. It can be seen that the largest contributions are related to the country of origin of the booker and the type of guest assigned to them. It is worth noting that the values of these variables are the same as in the case of the observation analyzed above. Again, they contribute to the same side of the prediction, but the contribution values are different. In this case, the type of client turns out to be the most important. Most of the values also affect the prediction of no cancellation. It is interesting that a slightly later booking in relation to the date of stay (lead time) has the opposite effect than in the previous example. The reason is the dependencies between the variables. Figure 1.44: A plot of LIME model values for the random forest model and the most misslassified observations. The similarity between the observations is also noticeable in the lime method. The first five variables are identical and have almost the same coefficients. Therefore, it is these less significant variables that influence the final prediction. In the case of these two observations with similar characteristics, but completely different predictions, the use of the SHAP method (generalizing the breakdown method) gives a better picture. The Glass-box model selected in LIME method to approximate the black-box model, and not the data themselves, is not able to capture dependencies between variables. 1.5.3.2 The most valid predictions The considered model returns an appropriate prediction in over 89% of cases. However, the level of certainty of the model with respect to the prediction (i.e. the probability that an observation is assigned to a class) may be different. Thus, it is worth considering why the model is almost sure of some outputs and how would the model’s predictions change if the values of some of the explanatory variables changed. We used local explanations methods for the observations in both groups with the best predictions, i.e. the highest probability of proper classification (equal to 1.0). Figure 1.45: A plot of Shapley values for random forest model and observation with sure negative prediction. The elements of the plot were described above. Like in the previous examples - the largest contribution has the country, in that case: France. Again, this is a Transient-Party customer and that also affected the prediction. Also, no special requests affect negatively to prediction (more than eg. no previous cancellations). Only one of the top variables affected positively: it was no required car parking spaces, but this impact was unnoticeable in the final prediction. Figure 1.46: A plot of Shapley values for random forest model and observation with sure positive prediction. The elements of the plot were described above. Again, Portugal as a country of origin affected positively the probability of cancelation (keep in mind that the hotel is in Portugal, so we can assume that compatriots cancel their reservations more often). Also, no special requests affected positively on prediction (although in the previous case it had a negative effect). We can notice that for positive prediction other factors have the biggest impact than for negative. Eg. a longer lead time moved up to third place and now has a positive impact. Figure 1.47: Ceteris-paribus profiles for the selected continuous explanatory variables and label encoded country variable for the random forest model and observations with the sure prediction. Dots indicate the values of the variables and the values of the predictions for observations. Green profiles are for sure positive prediction (a cancellation), while blue profiles are for sure negative prediction. Looking at the ceteris paribus profiles, it is intuitive to see that the prediction for the observation classified as the not canceled stay is more stable, i.e. less sensitive to changes in the values of explanatory variables. In the case of observation of canceled reservations, a change of the arrival date by a few weeks would cause a significant decrease in the certainty of the prediction. It is related to the seasonality of bookings (the decrease occurs at the beginning of July - the holiday period). However, the biggest changes in the prediction for this observation could be due to noting the fact of additional booking requirements (required car parking spaces and a total number of special requests). Changing these values to non-zero would change the prediction completely. Moreover, the huge changes depend on the country of origin of the booker, which in this case is Portugal. When considering the prediction for an observation classified as not canceled, we see that the only explanatory variable whose change would have a significant impact on the certainty of the prediction is the number of previously canceled reservations. A change to any non-zero value would change the prediction, but its certainty would be close to the decision boundary. 1.5.3.3 The closest to decision boundary predictions We analyzed the situations when the model is sure of the returned output. However, the observations for which the prediction was uncertain, close to the decision limit, are also worth considering. We might want to know the answer to the question of whether it is a matter of similar numbers of explanatory variables shifting the prediction in different directions, or maybe there are variables that do not fit the whole picture, and therefore the model is not certain. It is also worth checking how much such predictions fluctuate depending on the changes in the explanatory variables. Figure 1.48: A plot of Shapley values for random forest model and observation classified as negative with probability near 50%. The elements of the plot were described above. This is a very interesting example. One variable fixed prediction. Very short lead time (one day) opposed all other factors like Portugal as country of origin or no special requests and made the model predict correctly. It is amazing, that one factor can change everything. Figure 1.49: A plot of Shapley values for random forest model and observation classified as positive with probability near 50%. The elements of the plot were described above. This observation is not so exciting as the previous one, but it is the next evidence that the special requests decrease the probability that the client will cancel the reservation. Nevertheless, this observation was classified as positive. Agent was the most important positive variable although in the previous examples he did not have such a contribution. But… agent has that huge contribution only in Shapley. Figure 1.50: A plot of LIME model values for the random forest model and the same observation. Here agent has much less impact. This is a reminder for us that each method works differently and takes different variables into account. It is worth remembering this. In this method for that (and a lot of other) observation the most important factor is no previous cancelations, but it is not enough for the model to make a negative decision. 1.5.4 Global explanations The second group of methods of explainable artificial intelligence are those concerning not a single observation, but the entire set of them. We used these model level explanations to provide information about the quality of the model performance and infer how the model behaves in general. The methods we used for this purpose are Permutational Variable Importance, PDP (Partial Dependence Profile), ALE (Accumulated Local Effects). We applied them not only to the main random forest model, but also to other trained models to compare the obtained results. 1.5.4.1 Importance of explanatory variables First, we decided to check which variables are important for our main model and compare the obtained results with the intuitions we had after conducting the exploration at the prediction level. Figure 1.51: A plot of variable importance. The length of each bar represents the difference between the loss function (1-AUC) for the original data and the data with the permuted values of a particular variable. As we can see above, the most important variable for the model is the information about the guest’s country of origin. This confirms the intuitions obtained thanks to local explanations, for many observations this variable was the key aspect. In particular, the origin of Portugal (the country where the hotels are located) is the most significant for the prediction. The method indicated that the next principal variables for predictions are information on lead time and number of special booking requests. Lead time is the number of days that elapsed between the entering date of the booking into the system and the arrival date. It is a factor that may inform about whether the stay was planned long before or it is spontaneous. Meanwhile, the number of special requests is related to additional interest in the booking, which may indicate that it is an important stay for the client. The next variables with a significant average impact on the model are the group of those concerning the booking method (agent and market segment) and the type of booking (customer type and adr associated with the booking cost). It should also be noted that some of the variables are of marginal importance. These are (from the bottom of the plot) the type of hotel (recall that the data relate to two hotels of different specificity), the number of adults, the number of previously not canceled bookings (this is probably also related to a small number of observations with a non-zero value of this feature - 3.1% in the entire dataset), the type of room reserved. These are the variables that should be considered to be excluded in order to simplify the model. On the other hand, it is quite surprising that the number of previously canceled reservation, which was often indicated by LIME as one of the most important factors for the predictions under consideration, is so insignificant (0.007 drop-out loss) according to the algorithm of the permutational variable importance. 1.5.4.2 Comparison with other models Plots similar to that above in Figure 1.51 are useful for comparison of a variables’ importance in different models. It may be helpful in generating new knowledge - identification of recurring key variables may lead to the discovery of new factors involved in a particular mechanism (Biecek and Burzykowski 2021) (in our case, cancellation of reservations). Thus, we decided to compare the importance of the variables in the models we had trained, described in the Dataset and models section. The plots below show the results for our main Random Forest model and 4 other models with legend: green - Random Forest with label encoding (main model), red - Random Forest with one hot encoding, purple - Logistic Regression, blue - Decision Tree, orange - XGBoost. Figure 1.52: The comparison of the importance of explanatory variables for selected models. The elements of a single plot were described above. Note the different starting locations for the bars, due to differences in the AUC score value obtained for the training dataset for different models. It can be seen that the importance of the variables is related to the type of algorithm used in a given model. For example, in a decision tree, each variable has a clearly noticeable effect on the predictions. In all the tree-based models explained, the most important variable overlaps - it is the aforementioned country of origin. In logistic regression model, this feature is the third most important variable, but it is the model with the worst score overall. However, in general, the groups of the most important variables in each model are also similar. So our earlier conclusions regarding the key booking cancellation factors are confirmed. Likewise, in each of the tree-based models (even in a single decision tree model) the same variables were indicated as least important. An interesting fact is that in random forest with one hot encoding the agent variable is even more important than in random forest with label encoding. It is a categorical variable, so we can see the influence of encoding here - trees could extract more information from this variable thanks to not creating unnatural numerical relationships as with label encoding. The second noticeable difference between these models is the importance of the market segment. 1.5.4.3 The global impact of variables Then we decided to check the influence of the most influential variables on the predictions in the context of the whole set. For this, we used the ALE method. The plots below show the results for our Random Forest and 3 other tree-based models with legend: green - Random Forest with label encoding (main model), red - Random Forest with one hot encoding, blue - Decision Tree, orange - XGBoost. Note that we chose not to generate plots for the logistic regression model because it performed too poorly. Figure 1.53: ALE plot for the country. The ALE plots work like Ceteris Paribus - they show how the variable affects a prediction but not only for one observation - for the entire training dataset. We have a greater probability of resignation for one country - this is Portugal. This confirms our hypothesis built on the basis of local explanatory methods. Generally, compatriots more often resign from booking. Figure 1.54: ALE plot for lead time. The elements of a single plot were described above. And again we have confirmation that the longer the lead time, the greater the chance that the customer will resign. The ALE plots are very similar to Ceteris Paribus but that’s a great example that they’re not the same. Look at the Ceteris Paribus profile for lead time in the Local explanations section. The probability of resignation increased to about 170 days, then decreased and remained at a constant level. Here we can see that it was the “local behavior.” Globally, the probability only grows, then it stabilizes (after about 350 days - a year). Therefore, you may suspect that it is not worth allowing reservations so far in advance. Figure 1.55: ALE plot for total of special requests and required car parking spaces. The elements of a single plot were described above. Additional actions taken by the client regarding booking reduce the likelihood of cancellation. The very first special request significantly reduces the probability of resignation. The influence of the next ones is not that clear. This also confirms the thesis we made earlier that the lack of special requests increases the probability of resignation. Reserving a parking space has an even greater impact on the predictions. We may think that in these hotels it is payable in advance, or that car travelers are less dependent on public transport, so their arrival is more certain. Figure 1.56: ALE plot for previous cancellations and previous bookings not canceled. The elements of a single plot were described above. Information about a given customer’s prior bookings is also very valuable for prediction. The fact of earlier cancellation of a reservation strongly influences the prediction of the next one, which seems natural. A non-zero number of prior non-canceled bookings works the opposite way, but the prediction values don’t fluctuate that much. After analyzing these examples, an important conclusion can be drawn about the XAI methods. ALE plots can be a great tool for analyzing the influence of variables on the prediction - they can be used to verify the hypotheses put forward at the stage of local explanations and introduce new ones. When comparing the results for different models, note that the profiles for random forests with both versions of the categorical variable encoding are almost identical (green and red lines in the graphs). Looking more broadly, the profiles are comparable for all models. The most significant differences can be seen in the variables relating to the previous reservations of a given customer - the number of canceled and non-canceled reservations. The XGBoost model favors non-zero values more - the prediction changes are bigger. In general, this model is the most sensitive to all variables, as can be seen from the shape of the profile curves. Moreover, we used the comparison of the PDP and ALE plots for our main model (see Figure 1.57). In the case of some variables, the profiles generated using both methods almost coincide. However, there are also variables where you can see differences in prediction values, but profiles are parallel to each other. This parallelism suggests and allows us to conclude that the used model is additive due for these explanatory variables (Biecek and Burzykowski 2021). Figure 1.57: Partial-dependence and accumulated-local profiles for the main random forest model and selected variables 1.5.5 Summary and conclusions References Andriawan, Z. A., Purnama, S. R., Darmawan, A. S., Ricko, Wibowo, A., Sugiharto, A., &amp; Wijayanto, F. (2020). Prediction of hotel booking cancellation using CRISP-DM. In 2020 4th international conference on informatics and computational sciences (ICICoS) (pp. 1–6). https://doi.org/10.1109/ICICoS51170.2020.9299011 Antonio, N., Almeida, A. de, &amp; Nunes, L. (2017). Predicting hotel booking cancellations to decrease uncertainty and increase revenue. Tourism &amp; Management Studies, 13(2), 25–39. https://doi.org/10.18089/tms.2017.13203 Antonio, N., Almeida, A. de, &amp; Nunes, L. (2019). An automated machine learning based decision support system to predict hotel booking cancellations. Data Science Journal, 18(1), 1–20. https://doi.org/10.5334/dsj-2019-032 Biecek, P., &amp; Burzykowski, T. (2021). Explanatory Model Analysis. Chapman; Hall/CRC, New York. https://pbiecek.github.io/ema/ Falk, M., &amp; Vieru, M. (2018). Modelling the cancellation behaviour of hotel guests. International Journal of Contemporary Hospitality Management, 30(10), 3100–3116. https://doi.org/10.1108/ijchm-08-2017-0509 Riasi, A., Schwartz, Z., &amp; Chen, C.-C. (2019). A paradigm shift in revenue management? The new landscape of hotel cancellation policies. Journal of Revenue and Pricing Management, 18(6), 434–440. https://doi.org/10.1057/s41272-019-00189-3 Sánchez-Medina, A. J., &amp; C-Sánchez, E. (2020). Using machine learning and big data for efficient forecasting of hotel booking cancellations. International Journal of Hospitality Management, 89, 102546. https://doi.org/10.1016/j.ijhm.2020.102546 Smith, S. J., Parsa, H. G., Bujisic, M., &amp; Rest, J.-P. van der. (2015). Hotel cancelation policies, distributive and procedural fairness, and consumer patronage: A study of the lodging industry. Journal of Travel &amp; Tourism Marketing, 32, 886–906. https://doi.org/10.1080/10548408.2015.1063864 "],["explainable-artificial-intelligence-2.html", "Chapter 2 Explainable Artificial Intelligence 2", " Chapter 2 Explainable Artificial Intelligence 2 "],["xai1-explainable-german-credits.html", "2.1 Classifying people as good or bad credit risks", " 2.1 Classifying people as good or bad credit risks Authors: Paweł Fijałkowski, Paulina Jaszczuk, Jakub Szypuła (Warsaw University of Technology) 2.1.1 Introduction In 1980s first ever PC was introduced to the public. Since then, rapid technological progress in computer-science lead to enormous demand for implementing and developing previously theoretical studies to suit capable of remembering and performing extensive calculations machines. By combining academic effort with pragmatical, engineer approach, humanity succeeded in building complex infrastructure that is able to solve pretty much any problem we have ever faced. We found ourselves in the point, where machine-learning algorithms make sensitive and in many cases opinion-based decisions, based on numerical and statistical analysis. It may seem harmless on the first glance, but do we really “trust” these models when it comes to predicting lung cancer? Or granting us a credit? In many cases, we do not and we should not. This is where XAI or Explainable Artificial Intelligence kicks in. It is a branch of AI, which results can be clearly understand and interpreted by experts in a given field (in order to e.g. comply with regulations or improve model user experience). XAI, using multiple complementary tools, turns black-box (meaning, very hard to interpret and understand even by specialist) ML models into white-box predictable solutions. Why is it important? Mainly for two reasons. First one is, we feel safer knowing what is going under the hood. We can’t simply “believe” outputted answer without digging into specifics on what grounds it was constructed. It gives us sense of security and provides a way of detecting possible miscalculations. Second reason is, these models, due to their mathematical construction (often minimizing value of loss function) in many cases proved to tend towards biases. This cannot be accepted while building systems supporting banks or hospitals. In the article below, we are facing the problem of creating explainable classifier for German bank credit score. We will build black-box model and then employ XAI methods in order for us to understand which features and statistics tend to outbalance model’s decision towards good/bad borrower classification. 2.1.2 Dataset and models During entire analysis, we’ll be operating on publicly available data set containing information about German borrowers. Input was processed/collected by Professor Dr. Hans Hofmann from University of Hamburg and is available here. Data set provider, basing on academic research, have chosen certain variables to describe each borrower. That combination of numerical and qualitative variables proved to carry certain importance/influence on probability of paying back borrowed assets. Brief overview of statistics describing an observation (columns in data set): Statistic Explanation checking_status Status of existing checking account (qualitative) duration Credit duration in months (numerical) credit_history Credit history (qualitative) purpose Purpose (qualitative) credit_amount Credit amount (numerical) savings_status Savings account/bonds (qualitative) employment Present employment since (qualitative) installment_commitment Installment rate in percentage of disposable income (numerical) personal_status Personal status and sex (qualitative) other_parties Other debtors / guarantors (qualitative) residence_since Present residence since (numerical) property_magnitude Property ownership (qualitative) age Age in years (numerical) other_payment_plans Other installment plans (qualitative) housing Current housing deal (qualitative) existing_credits Number of existing credits (numerical) job Job type quality (qualitative) num_dependents Number of people being liable to provide maintenance for (numerical) own_telephone Ownership of telephone (qualitative) foreign_worker Worker from abroad (qualitative) Author also suggest using cost function that “punishes” model more for false-positives (classification of bad customer as a good one) than for false-negatives. For a model user (i.e bank-owner), it is much worse to classify bad customer as a good one, than the other way around. After careful deliberation and testing using multiple metrics the team decided to use random forest as a baseline model for further modeling and explanations. Random forest is a supervised machine learning algorithm, basing on a simple idea of decision tree. Each decision tree is making a classification prediction and then their votes are counted/merged. Only difference between standard forest voting and random forest is that in the latter case, only random subset of features is considered during selecting most important feature (with lowest gini index value). Model performed exceptionally good during cross-validation, even without fine parameter hyper-tuning. 2.1.3 Local explanations To better understand the behavior of Random Forest, we employed a set of machine learning explanation methods. In order to comprehend the reasoning behind model predictions and interactions between variables we began at the local level, i.e. single observation. This way, we could compare influence of each variable within a clearly defined context of every potential borrower. Our primary motivation in selecting observations to present in this paper was to show interesting cases and how variables interact with one another in our model in creating predictions. As a result we selected various observations. Methods employed by us were Break-down, SHAP, LIME and Ceteris Paribus. 2.1.3.1 Initial analysis We began by analyzing the output of the Break-down method on two random observations. Once compared side-by-side (see the figure) several key differences emerge. First of all, the importance of checking status variable (which will return in forthcoming sections). One can also notice the importance of the “age” variable, which seems intuitive. A further conclusion can be drawn from this - influence of a variable depends on its value. In fact, depending on interactions between variables in the data set even the same value can have opposite effects. An example (although miniscule) can be found by comparing observations 4 and 7 (both classified correctly). The same value of “job” variable (i.e. “skilled”) gives opposite influences (-1.7% “bad” probability and +1.4% respectively). Thus we should always take into account context in which each variable is evaluated. Regarding specific conjectures on the data set one can draw from this comparison, we can see that both observations share 7 out of the top 10 most influential variables. Among them are, for example age, checking status, which would be expected by a “common sense” intuition. Influences confirm that intuition. Young age contributes negatively towards credit rating and lower duration positively. Same for checking status - the higher the better. Figure 2.1: Comparison of plots of breakdown method for two observations - number 1 (left) and number 94 (right). Positive values indicate an increase in probability of being classified as a “good” borrower while negative values indicate the opposite. Figure 2.2: Comparison of plots of breakdown method for two observations - number 4 (left) and number 7 (right). The same value of job variable has a different effect on the prediction. 2.1.3.2 In-depth analysis Consequently, we used SHAP method to analyze the first observation and compare results with the Break-Down method. The results of this can be seen in the figure below. Duration and checking status remain the two most important variables, while age grows in importance. 8 of the top 10 variables remain the same, with around the same influences. The major difference being absence of information on existing credits and residence since, have been replaced with credit amount and purpose. This method offers new information, primarily about the variation of influence of each variable. One can notice that age is the most varied (since it can affect the prediction both positively and negatively), which again would correspond to the “common sense” intuition (as with duration, the credit amount and checking status). The opposite occurs with credit history, purpose, property magnitude and length of employment, where variation remains lower compared to influence. Changes, in order of importance are rather small, only the installment commitment moves by more than 3 places (from number 5 to number 10). The rest is swapping of checking status and duration, fall of property magnitude by 2 places, employment by 1 place and rise of age by 2 places. It should be noted that all these changes occur similar or very similar absolute influence values in break-down method. One can judge that importance of variables is a trait that can remain consistent between various explanation methods. Figure 2.3: A plot of results of SHAP method for 1st observation 2.1.3.3 The most certain predictions Our model classifies clients in binary terms as good and bad potential borrowers but the certainty of its predictions is not always the same. We decided to analyze the cases in which it has no doubts which group to assign a given client to. For this purpose, we used the Cateris Paribus method which checks how the prediction for a given observation would change for different values of one variable while the others remain constant. Figure 2.4: A plot of results of CP method for 21st observation Looking at the Cateris Paribus plot for an observation that the model classified as a good borrower with almost 97% certainty, we can draw several conclusions. First, the variables were divided into those important and less important for the model prediction. ‘Duration,’ ‘credit amount,’ and ‘age’ would clearly change the result if they had a different value - for example, a few thousand higher credit amount would significantly reduce the prediction, as would the duration value. On the other hand we see plots of ‘installment commitment,’ ‘residence since,’ ‘existing credits’ and ‘num dependents’ which are almost straight lines - changing their values would hardly affect the model’s predictions. This allows us to conclude that in this case it was the values of these three variables (listed first) that mainly determined the classification of the borrower. The explanations also once again confirm our intuition - the high certainty of the model as to the classification of the customer as a good borrower was motivated by logical arguments - a relatively low loan amount, short loan duration and the average age of the borrower. Figure 2.5: A plot of results of CP method for 64st observation Cateris Paribus charts for an observation confidently classified as a bad borrower are almost the opposite of the previous ones. This time the client is very young, he wants to take a high loan with a long repayment period. All these features have a negative effect on prediction. Interestingly, if only a few years older client applied for the same loan, the model’s prediction would be much higher. Once again, ‘installment commitment,’ ‘existing credits’ and ‘num dependents’ features do not seem to be important in the classification. However, the variable ‘residence since’ is different, as if it had assumed the value 1, it would significantly increase the prediction. 2.1.3.4 Incorrect predictions Although the accuracy of our model is at satisfactory level, sometimes it is providing incorrect predictions. We decided to take a closer look and analyze the situations when our model misclassifies. For this purpose we explained the false positives and false negatives predictions with the local methods LIME and SHAP. 2.1.3.4.1 False positive (#fig:2-2-lime_false_pos)A plot of results of LIME method for 56th observation which was missclasified by our model and predicted as false positive The model classified the observation positively mainly on the basis of the features duration (12 months) and savings status (no known savings). This is in line with our earlier conclusions - a short loan duration has a positive effect on prediction. To better understand the operation of the model in this case, we also made an explanation using the SHAP method. (#fig:2-2-shap_false_pos)A plot of results of SHAP method for 56th observation which was missclasified by our model and predicted as false positive The explanations agree on the influence of feature savings status but according to the SHAP method, duration has a marginal positive effect on prediction. Interestingly, both methods give checking status as the most influential feature. Its value (in the range from 0 to 200) negatively affects the prediction which is how the model should classify this observation but this is outweighed by the positive effects of other features. 2.1.3.4.2 False negative (#fig:2-2-lime_false_neg)A plot of results of LIME method for 945th observation which was missclasified by our model and predicted as false negative. The model’s prediction was not certain, but it indicated that the client is a bad borrower with a probability of 65%. The biggest negative contributions are related to checking status (in the range from 0 to 200), credit history (no credits/all paid) and duration (48 months). While the relatively long loan term may indeed have a negative effect on the prediction, the fact that there are no other loans seems to be contradictory. The LIME method did not answer all our questions. To dispel any doubts we also explained the observations using the SHAP method. (#fig:2-2-shap_false_neg)A plot of results of SHAP method for 945th observation which was missclasified by our model and predicted as false negative. SHAP’s explanations are very similar to those LIME’s ones. Again, features duration, checking status and credit hisotry have the greatest influence on negative prediction. Perhaps the strange contribution of the feature credit history is due to its interaction with other variables. 2.1.4 Global explanations A major shortcoming of local explanations is that they are just that - local. It is the ability to see “the bigger picture” that allows us to achieve a better understanding of the model. Thus, we resolved to using Global Explanation methods in order to properly grasp approach behind predictions in our machine learning model. 2.1.4.1 Feature Importance First of all, we began with Feature Importance method which gives information on the importance of features in the model. The results of applying this method on our model can be seen in the figure below. (#fig:2-2-feature_importance)A plot of Feature Importance values for each variable for our model. As one can notice, results are in line with local explanations. That is, the domination of such features as checking status, credit duration and credit amount. Then follows age, saving status and purpose. Yet again, our “common sense” intuition seems to be confirmed. Noteworthy, the plot resembles an exponential function, hinting that predictions are based mostly on the first few most important features, a trait not so obvious to observe in local explanations. It should be noted, however, that the low impact on the prediction of some variables may be dictated by a small number of observations - e.g. the number of non-foreign workers in our data set is just under 4%. Surprisingly, the explanation showed that some variables that had a significant influence on the prediction in local explanations, had a very low global impact on the model, e.g. installment commitment (installment rate in percentage of disposable income), which in both incorrectly classified observations was in the lead in terms of importance according to SHAP explanation but globally its impact was marginal. We can also notice the opposite situation - a globally important variable has almost no impact on the prediction of a specific observation - an example may be the age variable, which, according to feature importance, has the fourth largest impact on the prediction while in the analyzed by us incorrectly classified observations it is quite the opposite. If we assume that the model is correct, then we can also draw conclusions regarding the “real life” importance of features. Those results should still be compared with other Global Explanation methods. 2.1.4.2 Dependence of variables The feature importance method seems to be very helpful in exploring the model, but it also has some drawbacks. One is that it does not perform well on correlated variables - permutating only one dependent variable while the other remains constant is not the best solution. In the case of our data set this problem also occurs - for example, the duration of the loan is strongly correlated with its amount. For this reason, we decided to use the triplot package which enables the analysis of grouped variables. We divided the features of our data into 3 categories - those relating to the loan (e.g. credit amount and duration), the client’s account (e.g. checking and savings status) and the borrower himsefl (e.g. age and employment). Then we explained the contribution of the whole groups of variables. Figure 2.6: triplot for grouped variables. As we can see, although according to Feature Importance the most important variable for our model is checking status (rating group), the total prediction is influenced most by the characteristics of the loan itself while the variables connected with the current account have the most marginal effect on the model 2.1.4.3 Global impact of specific variables In order to better understand the influence of specific variables on the model prediction in relation to the global reference, we decided to perform the PDP explanations. This method works similarly to Cateris Paribus - it shows the preduction of the model for different values of the variable while maintaining other features of the model constants. We explained the four features that turned out to be the most significant for our model according to Feature Importance (checking status, duration, credit amount, age). 2.1.4.3.1 Checking status Figure 2.7: PDP plot for checking status variable. As we can see, a customer with a negative checking status has the lowest probability of being a good borrower according to the model and the best borrowers turn out to be those without a current account. It is somewhat surprising that the slightly higher prediction applies to customers with a lower positive checking status than those with a higher one. This may be due to the small number of clients with high checking status - just over 6% of all while other values are evenly matched. 2.1.4.3.2 Duration Figure 2.8: PDP plot for duration variable. The variable duration plot leads us to similar conclusions as for the local explanations. The longer the loan term, the greater the credit risk. For the initial values the prediction of a good borrower decreases almost linearly (the jagged graph is the result of using the random forest model), to a certain threshold of around 40 months when the loan duration is already so long that this variable does not have a significant impact on the prediction. 2.1.4.3.3 Credit amount Figure 2.9: PDP plot for credit amount variable. The plot of credit amount is somewhat similar to that of duration. As the loan amount increases, the probability that the customer will turn out to be a good borrower decreases until the loan is so high (around 12,000) that increasing it no longer has a significant impact on the prediction. The initial low value of the prediction and the increase for the loan amount from 0 to 4000 may be due to the fact that clients taking loans for small amounts may have problems with financial liquidity. 2.1.4.3.4 Age Figure 2.10: PDP plot for age variable. The age plot is consistent with our earlier assumptions and intuition - young borrowers who do not yet have a stable position on the labor market and their income is uncertain are the clients for whom credit risk is the highest. Subsequently, the prediction increases to peak for an approximately 38-year-old borrower. Statistically, people of this age have a stable job with high earnings. Thereafter, the prediction remains steadily high until around 55 years after which it begins to decline - this corresponds to the period when borrowers begin to retire which causes their incomes to decline frequently. 2.1.5 Summary and conclusions Applying XAI methods to our black-box model helped understand it, turning it, even if by a minimal margin, into more of a glass-box model. We found the most important features (that is checking status, duration, credit amount and age) and the rules governing their influence on predictions. We also confirmed the validity of a “common sense” intuition, e.g. the probability of being a “good” borrower being lower for very young and very old people. This allows us not only to validate the performance of our model, but also to better understand the rules governing this dataset. We can confidently say, that the main aim of our work has been achieved. The model has been explained and various explanations remained stable between methods used. Assuming our methodology was correct, it would mean that it is an adequate explanation that works not only within the model, but also within the data. With real-life data this can approach could prove useful in finding dependencies and interactions with phenomena, one would not connect with machine learning on first instinct. However, this would require greater research, far exceeding scope of this article. "],["how-to-predict-the-probability-of-subsequent-blood-donations.html", "2.2 How to predict the probability of subsequent blood donations?", " 2.2 How to predict the probability of subsequent blood donations? Authors: Maciej Chylak, Mateusz Grzyb, Dawid Janus (Warsaw University of Technology) 2.2.1 Abstract placeholder 2.2.2 Introduction and motivation Interest in explainable artificial intelligence (XAI) has increased significantly in recent years. XAI facilitates humans to understand artificial intelligence (AI) solutions (Barredo Arrieta et al. 2020a). It contrasts with the concept of a black box where even its designers cannot explain why an AI model has arrived at a specific conclusion. The intense development of such methods has led to a wide choice of XAI tools that we have today (Maksymiuk et al. 2021). It includes the R package DALEX (Biecek 2018a), which is the foundation of this work. Our goal is to prepare and explain a model designed to predict the probability of subsequent blood donations based on a history of a patient’s previous offerings. Careful use of XAI tools allows us to verify model correctness and discover phenomenons affecting blood donations that hide in the data. Obtained knowledge may have various implications, including improvement of planning and advertising of blood donation campaigns. 2.2.3 Related work placeholder 2.2.4 Data and model 2.2.4.1 Original dataset The data which all the prepared models are based upon comes from the Blood Transfusion Service Center Data Set, which is available through OpenML website (Vanschoren et al. 2013). The dataset consists of 748 observations (representing individual patients) described by 5 attributes: recency - months since last blood donation, frequency - total number of blood donations, monetary - total amount of donated blood in c.c., time - months since first blood donation, donated - a binary variable representing whether he/she donated blood in March 2007. 2.2.4.2 Data analysis Initial data analysis is a critical process allowing to discover patterns and check assumptions about the data. It is performed with the help of summary statistics and graphical data representations. The short data analysis below is based on two visualizations representing distributions and correlations of variables. Figure 2.11: Distributions of explanatory variables (histogram). Based on the above figure 2.11 an important insight can be made - distributions of Frequency and Monetary variables are identical (excluding support). It probably comes from the fact that during every donation the same amount of blood is drawn. The presence of both of these variables in the final model is pointless. Figure 2.12: Correlations of explanatory variables (correlation matrix). The above figure 2.12 represents correlations of explanatory variables measured using robust Spearman’s rank correlation coefficient. Apart from the already clear perfect correlation of Monetary and Frequency variables, a strong correlation of Time and Monetary/Frequency variables is visible. It probably comes from the fact that the minimal interval between subsequent donations is strictly controlled. Such dependence can negatively affect model performance and explanations. This potential problem is addressed during pre-processing of used data. 2.2.4.3 Pre-processing Simple data pre-processing is conducted, mainly to reduce detected correlations of explanatory variables. Firstly, the variable Monetary is removed from the dataset. The information it carries duplicates information contained in the Frequency variable. Secondly, a derived variable is introduced instead of Time variable. It is called Intensity and is calculated as follows: \\[\\textrm{Intensity} = \\frac{\\textrm{Frequency}}{\\textrm{Time}-\\textrm{Recency}}\\] The above equation results in values from range \\([0.03125, 1.00000]\\). The denominator can be interpreted as a time window bounding all the known donations of a given patient. Spearman’s rank correlation coefficient of the new variable Intensity and the old variable Frequency is \\(-0.46\\), which is lower compared to the previous \\(0.72\\) value for the Time/Frequency combination. 2.2.4.4 Final model According to the OpenML website Ranger implementation of Random Forests [2-3-ranger] is among the best performing classifiers for the considered task. All the tested models utilize this ML algorithm. Performance of the models is assessed through three measures - basic classification accuracy and more complex areas under the ROC (Bradley 1997) (Fawcett 2006) / PR (Raghavan et al. 1989) (Davis and Goadrich 2006) curves. The area under the PR curve is an especially adequate measure for unbalanced classification problems (Saito and Rehmsmeier 2015), which is the case here. Based on the described measures, the best model is chosen from models trained on the following explanatory variables subsets: Recency, Time Recency, Frequency, Recency, Frequency, Time Recency, Frequency, Intensity Models utilizing only two explanatory variables perform significantly worse. Out of the last two models, the model utilizing the Time variable is slightly worse than the model utilizing the Intensity variable. The accuracy of the last model is \\(0.85\\), other performance measures describing it are presented graphically below. Figure 2.13: ROC curve and corresponding AUC for final model. ROC curve visible in the above figure 2.13 represents good model performance and AUC value of almost \\(0.92\\) is definitely satisfactory. Figure 2.14: PR curve and corresponding AUC for final model. The baseline for the ROC AUC is always \\(0.5\\), but it is not the case for the PR AUC. Here, the baseline AUC is equal to the proportion of positive observations in the data. In our case it is \\(\\frac{178}{748}=0.238\\). Due to the above, the PR AUC value of around \\(0.81\\) visible in the figure 2.14 is also proof of high model precision. Summarizing the above model selection, the final model used in all the presented explanations is Ranger implementation of Random Forests utilizing the Recency, Frequency, and Intensity variables. Its performance measures are at least good, so the prepared explanations have a chance to be accurate. 2.2.5 Global explanations placeholder 2.2.5.1 Permutation Feature Importance Figure 2.15: Permutation Feature Importance for final model. 2.2.5.2 Partial Dependence Profile Figure 2.16: Partial Dependence Profile for final model. 2.2.5.3 Accumulated Local Effect Profile Figure 2.17: Accumulated Local Effect Profile for final model. 2.2.6 Local explanations 2.2.6.1 Ceteris Paribus Profiles Figure 2.18: Ceteris Paribus Profile for observation number 342. Figure 2.19: Ceteris Paribus Profile for observation number 16. 2.2.6.2 Break Down Profiles Figure 2.20: Break Down Profile for observation number 4. Figure 2.21: Break Down Profile for observation number 109. 2.2.7 Conclusions and summary placeholder References Barredo Arrieta, A., Díaz-Rodríguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., et al. (2020a). Explainable artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, 58, 82–115. http://www.sciencedirect.com/science/article/pii/S1566253519308103 Biecek, P. (2018a). DALEX: Explainers for Complex Predictive Models in R. Journal of Machine Learning Research, 19(84), 1–5. http://jmlr.org/papers/v19/18-416.html Bradley, A. P. (1997). The use of the area under the ROC curve in the evaluation of machine learning algorithms. Pattern Recogn., 30(7), 1145–1159. https://doi.org/10.1016/S0031-3203(96)00142-2 Davis, J., &amp; Goadrich, M. (2006). The relationship between precision-recall and ROC curves. In Proceedings of the 23rd international conference on machine learning (pp. 233–240). New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/1143844.1143874 Fawcett, T. (2006). An introduction to ROC analysis. Pattern Recognition Letters, 27(8), 861–874. https://doi.org/https://doi.org/10.1016/j.patrec.2005.10.010 Maksymiuk, S., Gosiewska, A., &amp; Biecek, P. (2021). Landscape of r packages for eXplainable artificial intelligence. https://arxiv.org/abs/2009.13248 Raghavan, V., Bollmann, P., &amp; Jung, G. S. (1989). A critical investigation of recall and precision as measures of retrieval system performance. ACM Trans. Inf. Syst., 7(3), 205–229. https://doi.org/10.1145/65943.65945 Saito, T., &amp; Rehmsmeier, M. (2015). The precision-recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets. Plos One, 10(3). https://doi.org/10.1371/journal.pone.0118432 Vanschoren, J., Rijn, J. N. van, Bischl, B., &amp; Torgo, L. (2013). OpenML: Networked science in machine learning. SIGKDD Explorations, 15(2), 49–60. https://doi.org/10.1145/2641190.2641198 "],["how-the-price-of-the-house-is-influenced-by-neighborhood-xai-methods-for-interpretation-the-black-box-model.html", "2.3 How the price of the house is influenced by neighborhood? XAI methods for interpretation the black box model", " 2.3 How the price of the house is influenced by neighborhood? XAI methods for interpretation the black box model Authors: Klaudia Gruszkowska, Bartosz Jamroży, Bartosz Rożek (Warsaw University of Technology) 2.3.1 Abstract The value of a house is extremely important to all of us, no matter if we are buyers, sellers or homeowners. That is why correct price prediction is so important. With the help of machine learning we are able to predict this price but not every time we know what influenced such prediction result. For this reason, the interpretability of models, used in important areas of life, has recently become increasingly valuable. In this article we interpret and explain these unknowns using Explainable Artificial Intelligence (XAI) methods. 2.3.2 Introduction Being able to predict the potential price of a house is crucial for the real estate market. Therefore, over the years, many papers have been focused on improving prediction methods (Ge et al. 2021; Park and Bae 2015). The main question regarding forecasts is what data influences the price. In addition to physical factors of the house, such as size, number of rooms, the condition of the building, the price is also influenced by the area in which the house is located. The neighborhood affects the price of the house and the house affects the value of the surrounding buildings. Thus, house price prediction models based on neighborhood data were created (Can 1990; Heyman and Sommervoll 2019; Law 2017). With the development of artificial intelligence and machine learning, the use of this type of algorithm is gaining popularity in the field of forecasting house prices (Park and Bae 2015). In machine learning, we can distinguish two types of models: glass-box and black-box models. In the glass-box model human can follow the steps from inputs to outputs. Black-box models is also an important part of machine learning, however unlike glass-box models, they do not have a clear, human-readable way of determining predictions (Biecek and Burzykowski 2021). So why do we use them when we have glass-box models at our disposal? The complexity of black-box models is both an advantage and disadvantage at the same time. Due to it, we get better, more tailored predictions, but we also doom ourselves to the lack of precise information on how the given result was obtained. Using algorithms, we would like to know what influenced the prediction result and how. We are not inclined to trust such a difficult and complicated activity to algorithms whose decisions are unclear to human. Therefore, in this paper we use Explainable Artificial Intelligence (XAI) methods to analyze the output of the black-box model. Data The data that was used was the “California Housing Data Set.” This data was initially featured in the following paper:(Pace and Barry 1997).The data pertains to the houses found in a given California district and some summary stats about them based on the 1990 census data. Data is cleaned, contains no empty information. Column title Description longitude A measure of how far west a house is; a higher value is farther west latitude A measure of how far north a house is; a higher value is farther north housingMedianAge Median age of a house within a block; a lower number is a newer building totalRooms Total number of rooms within a block totalBedrooms Total number of bedrooms within a block population Total number of people residing within a block households Total number of households, a group of people residing within a home unit, for a block medianIncome Median income for households within a block of houses (measured in tens of thousands of US Dollars) medianHouseValue Median house value for households within a block (measured in US Dollars) oceanProximity Location relative to the ocean described as however in the category: “NEAR BAY,” “&lt;1H OCEAN,” “INLAND,” “NEAR OCEAN,” “ISLAND” Figure 2.22: Map of property distribution within the state of California. The brightness of the points reflects the price. The map shows a density of bright blue dots in two locations, these are expensive properties located within two urban centers, Los Angeles, San Francisco, and San Diego. Also, more expensive properties are located along the Pacific coast. Figure 2.23: Map of property distribution within the state of California. Colors show the value of the oceanProximity column. Property map, packing the ocean_proximity feature. Properties with the ‘NEAR BAY’ category colored blue, appear in only one grouping next to San Francisco, NEAR BAY means San Francisco Bay. Green category, ISLAND collects 10 properties located on one island near Los Angeles. Machine learning model The machine learning model explained is from https://www.kaggle.com/camnugent/introduction-to-machine-learning-in-r-tutorial. A random forest model(Breiman 1999) from the “randomForest” library with default parameters was used to predict property values. This model does not require scaling of data or decoding of categorical data. This makes it easier to analyze the results. The two columns total_bedrooms and total_rooms were averaged by dividing by the number of household members. The new columns are mean_bedrooms and mead_rooms. 2.3.3 Literature Over the years, many solutions have been developed for the task of house price prediction, and what particularly interests us, solutions using machine learning (Conway 2018; Fan et al. 2018; Park and Bae 2015). However, in our paper we will not focus on the process of creating good models but on the process of explaining models and their results using XAI methods. Why are these explanations so important? As we mentioned in the introduction, black box models do not give us insight into the reason for their decisions. As mentioned in the article (Barredo Arrieta et al. 2020b) that is one of the main barriers AI is facing nowadays. However, not everyone supports the development of XAI (Aivodji et al. 2019; 2-5-against-xai1?; 2-5-against-xai2? ). They point to the insufficient credibility of the explanations and the possibility that the model conceal his unethical behavior. The second issue addressed in our paper is the relationship of house price to location. Several works have been created on this topic as well. For example, Stephen Law in his work (Law 2017) proposed defining a street as a local area unit and measuring its effect on house price. The results of his research of Metropolitan London showed significant local area effects on house prices. In our case the local area unit is block, which is a rectangular area marked by intersecting streets. Another example is the article written by Robin A. Dubin (Dubin 1998). He points out the importance of the correlations existing between the prices of neighboring houses, which is ignored in others method of predicting house values. 2.3.4 Local explanations We used observation 1 and 2000, because they are far from each other in the set so that they have different value. Figure 2.24: Choosen observations presented on a map. Red point - observation 1, green point - observation 2000 2.3.4.1 Break Down Intuition Break down provides a way to show the local importance of variables and the effect of individual values on predictions(Gosiewska and Biecek 2019). The basic idea is to calculate the contribution of variable in prediction of f(x) as changes in the expected model response given other variables. This means that we start with the mean expected model response of the model, successively adding variables to the conditioning. Of course, the order in which the variables are arranged also influences the contribution values. Results Figure 2.25: Break Down decomposition (L) observation 1, (R) observation 2000 Figure 2.25 shows the result of using Break Down method from DALEX (Baniecki et al. 2020a) package. The variables are ranked in terms of the importance of their impact on the predictions. The most important characteristic for both properties is median income. Median income is indicative of neighborhood status. Residents who earn more are likely to purchase more expensive properties. For both observations the influences are strong but opposite, high earnings for the first observation at $83,000 per year have a positive influence. In contrast, for property number 2000, earnings around $15 thousand per year are the reason why the model lowers the predicted price. Longitude is the second significant feature. Average number of rooms and location relative to the ocean was also significant. Interestingly, in observation number 2000, the inland location raises the price. From the map 2.22, it appears that it is the homes farthest from the ocean that are the cheapest. 2.3.4.2 Lime Intuition Local Interpretable Model-agnostic Explanations (LIME) was firstly proposed by Ribeiro, Singh, and Guestrin (Ribeiro et al. 2016). In LIME decomposition, our goal is to create an approximate glass-box model for a given observation. That model would be fully human-readable and can be easier analyzed. To do so, we create an artificial data set and teach the chosen glass-box model on it. The coefficients on the variables for the created model are the coefficients on the validity of the variables for our observation. Results Figure 2.26: Lime decomposition (L) observation 1, (R) observation 2000 Figure above 2.26 shows the result of using LIME method (lime?) from DALEX (Baniecki et al. 2020a) package. Common for both observations is that the median_income variable has the largest effect on prediction. However, the impact in the case 1 is large positive but in the case 2000 it is large negative. It is caused by the value of the variable. It is worth noting that the variables longitude and latitude have opposite signs in both cases. This is because these variables are negatively correlated. The second most valuable variable is the map location variable. It follows that for the observations, the key variables are median_income and some kind of location. This is consistent with our expectation and intuition that the higher the earnings of residents, the richer and more valued the neighborhood. Consequently, the houses themselves are also priced higher. Additionally, plots show us that impact of population,households or housing_median_age is not significant. What is interesting, in the case 2000 above 2.25 BreakDown found this location to be positive. 2.3.4.3 Ceteris Paribus Intuition Ceteris Paribus is a phrase that comes from Latin. Word for word meaning is “other things equal,” but in XAI much more meaningful translation would be “other things held constant.” In fact, these few words describe this method deeply - we choose one variable from observation, change it’s value, while other variables held constant and calculate new model’s prediction. Output of this operation is a function that shows us how prediction changes. Results Figure 2.27: Ceteris Paribus: first observation Plot 2.27 above shows Ceteris Paribus calculated for first observation of data set. As we can see, in most cases it is straight line with change for small values. Households and population profiles are similar, which seems to be reasonable, because not many people look for apartment in a remote area. Latitude and longitude profiles are vertical lines with one decrease. Hence, a region on map can separated where median value would be the highest.Mean bedrooms is hard to explain in the oposition to mean rooms where we can see that the more rooms the higher the value is. It stops at certain value, above which valuation doesn’t change. Median income is almost linear function, which is understandable as rich people can afford expensive apartments. Figure 2.28: Ceteris Paribus: first and 2000th observation Next plot 2.28 shows Ceteris Paribus calculated for two different observations. Specially two observations with massive difference in median value where chosen to observe what would change. Clear differences can be observed. Especially latitude, mean bedrooms, mean rooms and population shows major changes. Rest of variables have similar profiles, but these four are great example why shouldn’t we draw conclusions about whole data set based on one ceteris paribus calculation. 2.3.4.4 Local explanations conclusions As it could be observed, local explanations tells a lot about given observation. Unfortunately, juxtaposition of two different observations showed us, that they can have totally different explanations. Hence, local XAI, in this case, would allow us to increase already built house’s value rather than give an instruction how to build expensive property. To get this instruction global explanations should be used. 2.3.5 Global explanations In global explanations, we want to find out what influenced the model as a whole in the data, rather than looking at individual observations. This gives us an overall view of the predictions for the population. 2.3.5.1 Feature Importance Intuition Idea of permutation-based variable-importance(Maksymiuk et al. 2020): If a variable is important in a model, then after its permutation the model prediction should be less precise. The permutation importance of a variable \\(k\\) is the difference between model prediction for original data and prediction for data with permutation variable \\(k\\). Results Figure 2.29: Feature Importance Plot 2.29 show, Median income turns out to be the most important feature for the model under study. Removing the median income information by permutations, resulted in the largest decrease in the RMSE measure. This means that without this feature the model is much less precise. The geographic variables, longitude latitude and ocean proximity also turn out to be important for prediction. The average number of rooms, age, and the number of householders already have less impact on the prediction. The mean bedrooms category has the smallest and ambiguous impact. Whiskers extending beyond the left border of the bars indicate that there were draws where removing the variable improved the model results. The model may ignore this feature, having partial information about it in the form of number of rooms and household members. 2.3.5.2 PDP Intuition PDP (Friedman 2000) stands for partial dependence plot and it is connected with Ceteris Paribus. We take a random sample from data set, and calculate ceteris paribus for each one. After that, mean of all these functions is calculated, which leaves us with one profile for whole data set. In the plot below thick blue line is PDP and thin grey lines are Ceteris Paribus calculations for each observation. Results Figure 2.30: PPD As we can see above 2.30, households, mean bedrooms, mean rooms and population profiles are almost straight lines with fluctuations for small values. Housing median value is a interesting case, because small curvature can be observed at the end of line. This is probably due to the fact that new apartments are expensive, because they are modern and old ones are expensive because of historical background behind they. Latitude shows that most expensive houses are located on south, especially below \\(34 ^{\\circ} N\\), in California this is area near Los Angeles. Median house value is highest for longitude values lower than \\(118 ^{\\circ} E\\) (majority of a state except of San Diego and surrounding area). As it was in ceteris paribus, median income grows together with median house value to the level where income money simply doesn’t matter. Figure 2.31: PPD in groups PDP makes it possible to create separate profiles for different groups of observation. In our case, we used ocean proximity as it is a categorical variable. In the plot above we can observe, that all profiles are similar to each other, the only significant difference is vertical shift. Conclusion drawn from this plot is the fact that houses 1h &lt;Ocean are most expensive, inland apartments are the cheapest, while near ocean and near bay behave similar to each other and have average prices. If we take a look at the plot from Data paragraph, conclusions look reasonable. 1h &lt;Ocean houses are the ones located in Los Angeles and in the suburbs of San Francisco, which are well known from expensive neighborhoods. 2.3.5.3 Global explanations conclusions In contrast to local explanations, global explanations give a more general picture of the entire data set. With PDP profiles, we can see the relationships between the data. The future importance graph allows understanding what variables are relevant to the model. 2.3.6 Conclusion XAI methods can have multiple applications. They allow to study the behavior of Machine Learning Models. It gives an opportunity to understand the model, detect errors, for example, made during data preprocessing. With a good model matched to the data, XAI can be used to study the dependencies in the data. References Aivodji, U., Arai, H., Fortineau, O., Gambs, S., Hara, S., &amp; Tapp, A. (2019). Fairwashing: The risk of rationalization. In K. Chaudhuri &amp; R. Salakhutdinov (Eds.), Proceedings of the 36th international conference on machine learning (Vol. 97, pp. 161–170). PMLR. http://proceedings.mlr.press/v97/aivodji19a.html Baniecki, H., Kretowicz, W., Piatyszek, P., Wisniewski, J., &amp; Biecek, P. (2020a). dalex: Responsible Machine Learning with Interactive Explainability and Fairness in Python. arXiv:2012.14406. https://arxiv.org/abs/2012.14406 Barredo Arrieta, A., Díaz-Rodríguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., et al. (2020b). Explainable artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, 58, 82–115. http://www.sciencedirect.com/science/article/pii/S1566253519308103 Biecek, P., &amp; Burzykowski, T. (2021). Explanatory Model Analysis. Chapman; Hall/CRC, New York. https://pbiecek.github.io/ema/ Breiman, L. (1999). Random forests. UC Berkeley TR567. Can, A. (1990). The measurement of neighborhood dynamics in urban house prices. Economic Geography, 66(3), 254–272. https://doi.org/10.2307/143400 Conway, J. (2018, January). Artificial Intelligence and Machine Learning : Current Applications in Real Estate (PhD thesis). Retrieved from https://dspace.mit.edu/bitstream/handle/1721.1/120609/1088413444-MIT.pdf Dubin, R. A. (1998). Predicting house prices using multiple listings data. The Journal of Real Estate Finance and Economics. https://doi.org/10.1023/A:1007751112669 Fan, C., Cui, Z., &amp; Zhong, X. (2018). House prices prediction with machine learning algorithms. In Proceedings of the 2018 10th international conference on machine learning and computing (pp. 6–10). New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3195106.3195133 Friedman, J. H. (2000). Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29, 1189–1232. Ge, X., Runeson, G., &amp; Lam, K. C. (2021). Forecasting hong kong housing prices: An artificial neural network approach. Gosiewska, A., &amp; Biecek, P. (2019). Do Not Trust Additive Explanations. arXiv. https://arxiv.org/abs/1903.11420v3 Heyman, A., &amp; Sommervoll, D. (2019). House prices and relative location. Cities, 95, 102373. https://doi.org/10.1016/j.cities.2019.06.004 Law, S. (2017). Defining street-based local area and measuring its effect on house price using a hedonic price approach: The case study of metropolitan london. Cities, 60, 166–179. https://doi.org/10.1016/j.cities.2016.08.008 Maksymiuk, S., Gosiewska, A., &amp; Biecek, P. (2020). Landscape of r packages for eXplainable artificial intelligence. arXiv. https://arxiv.org/abs/2009.13248 Pace, R. K., &amp; Barry, R. (1997). Sparse spatial autoregressions. Statistics &amp; Probability Letters, 33(3), 291–297. Park, B., &amp; Bae, J. (2015). Using machine learning algorithms for housing price prediction: The case of Fairfax County, Virginia housing data. Expert Systems with Applications, 42. https://doi.org/10.1016/j.eswa.2014.11.040 Ribeiro, M. T., Singh, S., &amp; Guestrin, C. (2016). \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, san francisco, CA, USA, august 13-17, 2016 (pp. 1135–1144). https://doi.org/10.18653/v1/n16-3020 "],["deep-learning-1.html", "Chapter 3 Deep Learning 1", " Chapter 3 Deep Learning 1 Author: Weronika Hryniewska Deep learning is one of the most rapidly developing field in artificial intelligence. Problems that previously required a lot of features engineering became easily solvable. New possibilities opened, and deep learning has started to adopt in various domains. One of the most demanding disciplines is medicine. As a result of the outbreak of the COVID-19 pandemic, many scientists became interested in the possibilities of deep learning application in radiology. Many solutions have been created for classification, segmentation and detection based on computed tomography and radiographs of the lungs. During classes, we explored deep learning methods for computer vision. If you would like to read more about them, please take a look at books: “Deep Learning with Python” (Chollet 2017) and “Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems” (Géron 2017). We focused on results reproduction and/or further development of the available code of the following papers:: LungNet (Anthimopoulos et al. 2019) Adam Frej, Piotr Marciniak, Piotr Piątyszek BCDU-Net (Azad et al. 2019) (Asadi-Aghbolaghi et al. 2020) Maria Kałuska, Paweł Koźmiński, Mikołaj Spytek DeepCOVIDExplainer (Karim et al. 2020) Kacper Kurowski, Zuzanna Mróz, Aleksander Podsiad ERSCovid (S. Wang et al. 2020) Bartłomiej Eljasiak, Tomasz Krupiński, Dominik Pawlak COVID-Net (L. Wang et al. 2020a) Jakub Kozieł, Tomasz Nocoń, Kacper Staroń References Anthimopoulos, M., Christodoulidis, S., Ebner, L., Geiser, T., Christe, A., &amp; Mougiakakou, S. (2019). Semantic segmentation of pathological lung tissue with dilated fully convolutional networks. IEEE Journal of Biomedical and Health Informatics, 23(2), 714–722. https://doi.org/10.1109/JBHI.2018.2818620 Asadi-Aghbolaghi, M., Azad, R., Fathy, M., &amp; Escalera, S. (2020). Multi-level context gating of embedded collective knowledge for medical image segmentation. https://arxiv.org/abs/2003.05056 Azad, R., Asadi-Aghbolaghi, M., Fathy, M., &amp; Escalera, S. (2019). Bi-directional ConvLSTM u-net with densley connected convolutions. In 2019 IEEE/CVF international conference on computer vision workshop (ICCVW) (pp. 406–415). https://doi.org/10.1109/ICCVW.2019.00052 Chollet, F. (2017). Deep learning with python. Manning. Géron, A. (2017). Hands-on machine learning with scikit-learn and TensorFlow : Concepts, tools, and techniques to build intelligent systems. O’Reilly Media. Karim, M. R., Döhmen, T., Rebholz-Schuhmann, D., Decker, S., Cochez, M., &amp; Beyan, O. (2020). DeepCOVIDExplainer: Explainable COVID-19 diagnosis from chest x-ray images. IEEE. https://doi.org/10.1109/BIBM49941.2020.9313304 Wang, L., Lin, Z. Q., &amp; Wong, A. (2020a). COVID-net: A tailored deep convolutional neural network design for detection of COVID-19 cases from chest x-ray images. Scientific Reports, 10(1), 19549. https://doi.org/10.1038/s41598-020-76550-z Wang, S., Zha, Y., Li, W., Wu, Q., Li, X., Niu, M., et al. (2020). A fully automatic deep learning system for COVID-19 diagnostic and prognostic analysis. European Respiratory Journal, 56(2). https://doi.org/10.1183/13993003.00775-2020 "],["bcdunet.html", "3.1 BCDUNet", " 3.1 BCDUNet Authors: Maria Kałuska, Paweł Koźmiński, Mikołaj Spytek 3.1.1 Abstract Some text will be here in the future "],["an-exploration-of-deepcovidexplainer-explainable-covid-19-diagnosis-from-chest-x-rays.html", "3.2 An Exploration of DeepCovidExplainer: Explainable COVID-19 Diagnosis from Chest X-rays", " 3.2 An Exploration of DeepCovidExplainer: Explainable COVID-19 Diagnosis from Chest X-rays Authors: Kurowski Kacper, Mróz Zuzanna, Podsiad Aleksander 3.2.1 Introduction and motivation To make no secret of it, the main motivation for our work was to pass the Research Workshop class. Our task was to try to reproduce the results of the DeepCovidExplainer project. It is a Deep Learning model based on deep convolutional neural networks for predicting disease (or lack thereof) from lung x-rays. In our opinion, however, the end goal isn’t the most important part of the journey. It is, as the saying goes, the process (and the friends we made along the way). Therefore, in this article we would like to focus on it. We will outline our adventure in trying to reproduce this model, including the problems and obstacles we encountered along the way. We will also try to describe what could be done to prevent such problems in your own projects. After all, reproducibility in research is necessary for two key reasons: to provide evidence of the correctness of a study’s results, and to provide transparency with your experiment and allow others to understand what was done. 3.2.2 Related work DeepCovidExplainer (Karim et al. 2020) is the work of 6 researchers. This project involved training several neural networks: VGG, ResNet, DenseNet, based on the COVIDx dataset to recognize whether a given image belongs to a healthy person, a COVID-19 patient, or a pneumonia patient. The COVIDx dataset, consists of six smaller publicly available datasets: Covid-chestxray-dataset (Cohen et al. 2020) COVID-chestxray-dataset: a public open dataset of chest X-ray and CT images of patients who are positive or suspected of COVID-19 or other viral and bacterial pneumonias (MERS, SARS, and ARDS.) Built to enhance models for COVID-19 detection (COVID-Net) and COVID-19 risk stratification (COVID-RiskNet): Actualmed COVID-19 Chest X-ray Dataset Initiative: also built to enhance models for COVID-19 detection (COVID-Net) and COVID-19 risk stratification (COVID-RiskNet) COVID-19 Radiography Database described in (Chowdhury et al. 2020) and in (Rahman et al. 2021), rsna-pneumonia-detection-challenge dataset from kaggle competition; Medical Imaging Data Resource Center (MIDRC) - RSNA International COVID-19 Open Radiology Database (RICORD) Release 1c - Chest x-ray Covid+ (MIDRC-RICORD-1c) (Clark et al. 2013). The above dataset is used in many other works on deep neural networks. We will name a couple of them. In the paper (L. Wang et al. 2020b), the authors use COVID-Net. They compare their results (more than 90% Accuracy) with the results they obtained on the VGG-19 and ResNet-50 networks. The paper differs from DeepCovidExplainer mainly in that in the latter the authors practically ensemble three networks, while in COVID-Net they use only the titular one. The authors of the paper (Ucar and Korkmaz 2020) use SqueezeNet to create the model, which is fine-tuned using Bayesian optimization. The fine-tuned model has over 90% Accuracy. The work differs in the network used and the method of selecting optimal parameters. Each of the above mentioned works deals with the application of deep neural networks in classifying the health status of a patient based on CXR images of his lungs. However, as our authors say, the final goal is to create a tool to assist radiologists in diagnosing the condition, not to replace them. 3.2.3 Our work 3.2.3.1 Getting started The first step in our journey was to create a virtual environment to run all the code. Unfortunately, this is where the problems start. The authors did not specify - either in their article or in the project repository - which libraries and their versions are needed, or even which version of Python the code is written for. This would have made our job very difficult if we had not thoroughly searched all the included code files. Fortunately, we were able to find a code snippet that listed the versions of some packages (numpy version 1.18.1, tensforflow version 1.14.0, keras version 2.3.1) and the runtime environment (Python 3.6.9). However, if this piece of code had not appeared in the previously generated .ipynb notebook it would probably have strongly affected the pace of our further progress, perhaps even prevented the completion of the project. Through trial and error (and repeating the process of running the code and installing missing libraries many times) we finally managed to figure out how the virtual environment should look like. Here are our results: Virtual environment Package Version Python 3.6 3.6.9 or newer numpy 1.18.1 tensorflow 1.14.0 tensorflow-gpu 1.14.0 (optional) keras 2.3.1 h5py 2.10.0 weightwatcher 0.2.7 matplotlib newest compatible scipy newest compatible scikit-learn newest compatible pandas newest compatible pydicom newest compatible ipython newest compatible jupyter newest compatible ipykernel newest compatible opencv newest compatible torch newest compatible PIL newest compatible xlrd newest compatible openpyxl newest compatible innvestigate newest compatible Standalone installation Program Version CUDA 10.0 (optional for gpu) cuDNN 7.4.2 (optional for gpu) 3.2.3.2 Finding data The main source of data for our project were the github repositories presented in the original article, which make up the COVIDx dataset. It contains chest X-ray (CXR) images of patients of different ages and health conditions. There are three prediction classes: the lungs of a Covid-19 patient, the lungs of a person with pneumonia, and the lungs of a healthy person. The images are mostly black and white and vary in size. Unfortunately, here we also encountered a major obstacle. Namely, the authors did not include the data on which they trained their model in their repository. We had to find the repositories from which the data were taken knowing the name of the dataset given by the authors (COVIDx). We then generated this dataset from various sources, although this was not without problems. Most of the datasets generated in this way had too few or too many images from the relevant classes relative to the description of the authors of the paper. We were able to obtain a dataset which mostly matched the description of the dataset used by our authors, but to this day we are unsure why the same dataset generation script kept giving different people different results. Hopefully the generated dataset matches the one used in DeepCovidExplainer but unfortunately we have no way of confirming that. 3.2.3.3 First look at the models After acquiring the dataset, it was time to attempt to train and test the preliminary models. In the article, the authors describe three separately trained model types: VGG (16, 19), ResNet (18, 34), and DenseNet (161, 201). All of these networks are convolutional networks; the number behind the name indicates the total number of layers (so, for example, VGG-16 has 16 and VGG-19 has 19). In VGG networks, the last three layers are dense layers, while the previous ones are convolutional layers, with a 3x3 filter, arranged in blocks of 2, 3 or 4. Each such convolutional block ends with max pooling. In all cases (except the last one, where the activation function is softmax,) the activation function used is ReLU. Figure 3.1: VGG19 architecture In the case of VGG, the convolutional blocks end in max pooling with stride=2, which makes the first two dimensions of the data accepted by the next layer twice as small. After a few convolutional blocks, we no longer have the ability to do another stride, making it impossible for the next block to operate on the reduced dimension. The consequence of this problem is that as the number of convolutional layers increases, the prediction error becomes larger. ResNet addresses this problem; it solves it by creating convolutional blocks with an additional connection between the data after the block and that before. In this way, further convolutional blocks learn small changes that can improve the prediction. This idea is somewhat developed in DenseNet networks. In them, instead of convolutional blocks, we have dense blocks that end in convolution with ReLU and max pooling. This time the results of the dense block are transferred as additional information to the result of each subsequent block. In this way, no information is lost and the network learns to improve the results slightly in each subsequent block. Figure 3.2: DenseNet161 architecture Of the six models mentioned in the paper, we decided to focus on the VGG-19 network, which we felt had the clearest architecture, was easily modifiable, and which we were able to run. However, here we also encountered a couple of problems. Firstly, the code from the repository set up by the authors created a model that distinguished 4 classes (covid-viral, non-covid-viral, bacterial, normal), while the article distinguished only 3 classes, which we wrote about earlier. We were therefore forced to modify the model to match the original idea from the article and change some of its parameters. Fortunately, this was not a hard problem to overcome, but its presence indicates that the code provided may be out of date with the article. 3.2.3.4 An attempt at preprocessing Our data for training the model had not been modified in any way at this point other than the resizing to 224x224x3 (RGB conversion) required to run the model. Therefore, we decided to do a full-fledged preprocessing as recommended by the authors of the paper. Unfortunately, the code for preprocessing posted on the repository did not work, so we had to actually write our own version of it, following the description from the article and fragments of unfinished code provided by the authors. Our preprocessing consisted of: resizing the image with anti-aliasing, histogram stretching (contrast enhancement), reducing noise with anisotropic diffusion, converting from RGB to greyscale format for better performance, creation of a mask using the threshold function from the OpenCV library (detection of the brightest spots in the photo), modifying (thickening) the mask using the dilate function from OpenCV (in order to be able to detect thin lines), removing annotations from the image using the inpaint function from OpenCV and using the already created mask. We have also dropped the part of the authors’ code where they introduce the division between the right and left side of the image, as they did not use this later in the code and it does not seem necessary. Figure 3.3: Before and after preprocessing 3.2.3.5 Modifications and other curiosities As we mentioned earlier, we focused on the VGG19 model, mainly because of the straightforward implementation and ease of modification of its architecture. We performed several experimental modifications and improvements that aimed to improve the performance of this model. Figure 3.4: Confusion matrix of the base model 3.2.3.5.1 Superficial changes The first changes we decided to make were small alterations to the VGG network. The reason for choosing VGG was the simplicity of making these changes combined with the ease of evaluating the results. Firstly, the value of filters was changed from 16 to 32 in the last (fifth) convolutional block. The second change was to add a new (sixth) convolutional block at the end of the network with the same values as the other blocks, except for the filters value which was set to 32. The last was to add another dense layer (256 neurons + Dropout 0.25 between the first and second dense layer) in the classifier block. Unfortunately, none of these changes resulted in improved prediction of the network. On the other hand, it’s somewhat interesting to note that these changes didn’t result in a significant worsening of the network prediction: the Accuracy still remains at 70-80%, the same as before the changes were made. Figure 3.5: Confusion matrices of models with small changes: first, second and third We also used 3 types of regularization: L1 regularization, L2 regularization, and mixed L1 and L2 regularization. In the case of mixed regularization, we obtained an improvement in the prediction of pneumonia, but this came at the expense of predicting both the absence of disease and the presence of COVID19. Unfortunately, it cannot be concluded that any of the regularizations improved network performance. Figure 3.6: Confusion matrices of models: with L1, with L2 and with both We also tested the performance of VGG19 without the so far present Dropout 0.5 mechanism between dense layers, and as you might guess we only got worse results. Figure 3.7: Confusion matrix of the model with no dropout 3.2.3.5.2 Multiple outputs and ensemble The idea behind creating the second output in our model was to see which dataset a particular image came from. To remind the reader, the dataset we are using is actually a mixture of several datasets. We tried adding an additional output three times. The first time, we decided to perform branching in the part of the VGG network composed of dense layers. The resulting output unfortunately turned out to be quite poor - first, the COVID-19 detection quality deteriorated, and second, too many images were misclassified as originating from the RSNA dataset. Figure 3.8: Confusion matrix of the model with branching in dense layers The lack of success with the first approach contributed to our second attempt - we separated the last three convolutional blocks and implemented the class weights mechanism. Unfortunately, this did not improve our results - this time all data were classified as coming from the sirm dataset. Figure 3.9: Confusion matrix of the model with earlier branching and class weights mechanism Finally, we decided to separate the network from the first convolutional block and dispensed with class weights. Similarly, this time all images were classified as coming from a single dataset. Figure 3.10: Confusion matrix of the model with the earliest branching and no class weights These results can be seen as a lack of success on the one hand, and somewhat positive news on the other. Despite the lack of improvement in prediction, we learned that images from different datasets are not that different. This is, of course, good news, so that we know that the model does not learn exemplary features based on potential special features of the sets. Let’s move on to our ensemble ideas. The first concept for the ensemble was to combine results from ResNet and VGG. Unfortunately, this idea did not give successful results, because ResNet classified all data as one class, which resulted in incorrect prediction of the whole ensemble. Figure 3.11: Ensemble of ResNet and VGG For this reason, we opted for an ensemble composed of most of the decently performing VGG networks generated during the previous steps. The results obtained were definitely better than those of the previous ensemble idea, but still did not give an improvement over the baseline. Figure 3.12: Ensemble of multiple VGGs 3.2.3.5.3 GAN and transfer learning - unsupervised pretraining and an auxiliary task To augment the training data, we decided to train an image generator using the generative adversarial network (GAN) method. With this model we could perform pretraining on a large number of randomly generated lung images without the risk of overtraining. The results are quite satisfactory however, due to hardware limitations the generated lung images are not of very high quality. Figure 3.13: GAN generator results We then tried to use pretraining on VGG19 using images from our dataset without labels. We chose not to use images generated from the GAN network because we were able to visually determine that the images were not of high enough quality for this task. We trained the convolutional layers using an unsupervised feature detection algorithm (autoencoder). After training the layers in this manner, we added an output layer and tuned the network using supervised learning (without unfreezing the convolutional layers due to the very high encoder accuracy - 99.99%). Unfortunately the results weren’t very satisfactory - the network placed far too much emphasis on the prediction of pneumonia and far too little on the prediction of normal lungs. Figure 3.14: Unsupervised pretraining results Let us focus next on the auxiliary task. We noticed that much of the dataset we use in our project comes from the RSNA dataset. It does not contain the lungs from COVID-19, but it does contain many more other types of ailments. This dataset also contains much more information about each image - such as the gender of the person in the image - and it was because of this information that we decided to create an auxiliary task. Our auxiliary task was to identify whether a photo was of a man or a woman. In order to accomplish this task we trained the VGG network, and then using the weights stored as training starters, we attempted to teach the network our initial task. Unfortunately, while the network’s results on the aux task were quite satisfactory, the network’s results on the output task were much worse - the network was unable to break out of the weights predicting two variables - moreover, it seems to even come close to predicting all values as one class. Figure 3.15: Auxilary task results 3.2.3.6 Final results Interestingly, all our models oscillated between 70-80% accuracy. In the end, it turned out that we got the best (or comparable) results after performing undersampling alone. This result is similar (or even a little better!) to the result obtained in the notebooks on the repository. Despite the fact that we tried to follow as closely as possible the process described by the authors, we could not achieve results similar to those in the article, where VGG19 had results ranging between 85%-95%. This is not surprising, given that we are not experts in Deep Learning practices and had access neither to the actual version of the code used by the authors nor are we even sure if we used exactly the same dataset as they did. Figure 3.16: Original authors’ results 3.2.4 Conclusions and summary As you can see, our road was long and sometimes arduous, but we must admit that we learned many valuable things during it. First of all, we gained a lot of practical knowledge about deep learning and became skilled in using related libraries. After the problems we encountered and the ones we heard about from our colleagues in other groups working on other deep learning projects, we drew some conclusions about what authors of a scientific paper can do to facilitate the reproducibility of their results. First, as authors we should make the code of our project available and also describe as precisely as possible the environment in which it was written, including our programming language’s version and the versions of all necessary libraries and additional software, if we use any. Without this it is very hard to verify our results and you can never be sure if any reproduction is completely accurate. The code we include should also be as up-to-date as possible, and it should be run at least once from start to finish. We should make sure it works as-is and doesn’t need to be corrected. If there are a lot of files, they should have clear and appropriate names, and we might even want to consider writing a short manual If we’re using open-source data, we should also provide access to it if possible, or at least describe its source in detail - it’s very useful to include links if they exist. By following these steps we will ensure reproducibility of our results and make it much easier to verify them, which as we know is crucial in any scientific project. References Chowdhury, M. E. H., Rahman, T., Khandakar, A., Mazhar, R., Kadir, M. A., Mahbub, Z. B., et al. (2020). Can AI help in screening viral and COVID-19 pneumonia? IEEE Access, 8, 132665–132676. https://doi.org/10.1109/ACCESS.2020.3010287 Clark, K., Vendt, B., Smith, K., Freymann, J., Kirby, J., Koppel, P., et al. (2013). The cancer imaging archive (TCIA): Maintaining and operating a public information repository. Journal of Digital Imaging, 26(6), 1045–1057. https://doi.org/10.1007/s10278-013-9622-7 Cohen, J. P., Morrison, P., Dao, L., Roth, K., Duong, T. Q., &amp; Ghassemi, M. (2020). COVID-19 image data collection: Prospective predictions are the future. arXiv 2006.11988. https://github.com/ieee8023/covid-chestxray-dataset Karim, M. R., Döhmen, T., Rebholz-Schuhmann, D., Decker, S., Cochez, M., &amp; Beyan, O. (2020). DeepCOVIDExplainer: Explainable COVID-19 diagnosis from chest x-ray images. IEEE. https://doi.org/10.1109/BIBM49941.2020.9313304 Rahman, T., Khandakar, A., Qiblawey, Y., Tahir, A., Kiranyaz, S., Abul Kashem, S. B., et al. (2021). Exploring the effect of image enhancement techniques on COVID-19 detection using chest x-ray images. Computers in Biology and Medicine, 132, 104319. https://doi.org/https://doi.org/10.1016/j.compbiomed.2021.104319 Ucar, F., &amp; Korkmaz, D. (2020). COVIDiagnosis-net: Deep bayes-SqueezeNet based diagnosis of the coronavirus disease 2019 (COVID-19) from x-ray images. Medical Hypotheses, 140, 109761–109761. Wang, L., Lin, Z. Q., &amp; Wong, A. (2020b). COVID-net: A tailored deep convolutional neural network design for detection of COVID-19 cases from chest x-ray images. Scientific Reports, 10(1), 19549. https://doi.org/10.1038/s41598-020-76550-z "],["erscovid.html", "3.3 ERSCovid", " 3.3 ERSCovid Authors: Bartlomiej Eljasiak, Tomasz Krupinski, Dominik Pawlak 3.3.1 Introduction "],["covid-net.html", "3.4 COVID-Net", " 3.4 COVID-Net Authors: Jakub Kozieł, Tomek Nocoń, Kacper Staroń 3.4.1 Introduction "],["deep-learning-2.html", "Chapter 4 Deep Learning 2", " Chapter 4 Deep Learning 2 Author: Paulina Tomaszewska Artificial Intelligence (AI) especially Deep Learning (DL) is a rapidly emerging field. It is proved by the number of publications – every day some new paper is released. In the spirit of “open science” (Mendez et al. 2020) not only papers are published in journals but also are available previously as preprints. This helps in the fast exchange of knowledge between researchers. In order to fasten progress in the field even more, it is recommended to open source also the code as well as data used for analysis in the paper. In such a scenario, researchers inspired by someone’s papers will not have to implement the described solution independently but rather focus on adding improvements. Such a workflow, however, requires the reproducibility of the results shown in the paper. It means that by running the code given by the authors, the same results as described in the paper should be obtained. People started to verify the reproducibility of the papers also to check whether the results in the paper are reliable. It sometimes happens that the authors do “cherry-picking” of the results. The reproducibility of the papers is getting more and more attention. There is a web page (Yildiz et al. 2021) where the outcomes of the paper reproducibility studies are stored. In this chapters, students focused on the reproducibility of Deep Learning papers. It was motivated by two facts featuring models in Deep Learning: they are complex (often have an immense number of parameters) they have an inherent component of randomness (e.g. weight initialization, data augmentation) These two points show that the task of reproducibility in Deep Learning can be sometimes a challenge (Liu et al. 2020). References Liu, C., Gao, C., Xia, X., Lo, D., Grundy, J., &amp; Yang, X. (2020). On the replicability and reproducibility of deep learning in software engineering. https://arxiv.org/abs/2006.14244 Mendez, D., Graziotin, D., Wagner, S., &amp; Seibold, H. (2020). Open science in software engineering. Contemporary Empirical Methods in Software Engineering, 477–501. https://doi.org/10.1007/978-3-030-32489-6_17 Yildiz, B., Hung, H., Krijthe, J. H., Liem, C. C. S., Loog, M., Migut, G., et al. (2021). ReproducedPapers.org: Openly teaching and structuring machine learning reproducibility. In B. Kerautret, M. Colom, A. Krähenbühl, D. Lopresti, P. Monasse, &amp; H. Talbot (Eds.), Reproducible research in pattern recognition (pp. 3–11). Cham: Springer International Publishing. "],["what-makes-an-article-reproducible-comparison-of-the-fer-paper-and-axondeepseg.html", "4.1 What makes an article reproducible? Comparison of the FER+ paper and AxonDeepSeg", " 4.1 What makes an article reproducible? Comparison of the FER+ paper and AxonDeepSeg Authors: Mikołaj Jakubowski, Patryk Tomaszewski, Mateusz Ziemła (Warsaw University of Technology) 4.1.1 Abstract Reproduction of the code presented in scientific papers tends to be a laborious, yet important process, as it gives the reproducers a better understanding of the methods proposed by the authors, and verifies the credibility of a given paper. While recreating an article, various problems, which sometimes can be hard to overcome, can appear. We decided to go through these problems and compare two papers with their corresponding code. As a result, we identify the most important characteristics of an article that factor into its reproducibility. 4.1.2 Introduction 4.1.2.1 Why is reproducibility important? Reproducibility is the ability to rerun an experiment described in a given article and obtain results similar to those presented. Ideally, we would like for the reproduced results to be identical to the ones in the article, however this can be almost impossible to achieve due to the influence of external factors, such as the internal structure of the GPU used. Unfortunately, in most cases even getting similar results may prove to be very difficult (Baker 2016). Many articles do not contain enough information for the reproduction process to be successful. Crucial steps of the experiment may be omitted or presented incorrectly. This is very damaging for the article itself and for the message it is trying to convey. Without the ability to rerun the experiment, the reader cannot be sure that no mistakes were made in the process or that the results were not purposefully misrepresented to fit the narrative. Because of that, the credibility of the article is significantly reduced, and it may prevent other researchers from using those findings in further works (Tatman et al. 2018). Moreover, being able to see each stage of the experiment on its own, tweak different values and observe the change in results can help the reader to better understand the methodology presented in the article and allow for them to apply it, or even improve it, in the future. All those reasons make reproducibility a crucial part in any scientific article, and yet it is being overlooked more often than it should. 4.1.2.2 Motivation The fundamental purpose of this article is to identify good and bad practices of an article’s reproducibility. We outline the methods used in AxonDeepSeg, so they may be used as a reference in future creation of articles to ensure their reproducibility. 4.1.2.3 Methods We decided to analyze two different papers, FERplus (Barsoum et al. 2016) which will be used for comparison, and AxonDeepSeg (Zaimi Aldo et al. 2018) which was our main focus. We analyzed the articles alongside the source code and other materials provided within them. For both papers, we evaluated their consistencies with the actual code and the ease of replication for anyone trying to verify their results. For the first paper, to evaluate the impact of omitted information and inconsistencies on the final results, we also created a secondary model in the Keras framework, since the original code is presented using the CNTK framework. 4.1.3 Analyzing the FERPlus paper What the authors of this paper want to demonstrate, is how to train a deep convolutional neural network (DCNN) from noisy labels. A great example of obtaining them is getting crowd-sourced labels of facial expressions. With the accuracy oscillating around 65%, there are more precise methods of tagging these photos, but they appear to be significantly more expensive and slower to produce. The authors want to compare 4 different approaches for training DCNNs, showing that including outlier-tagging information in models may have a positive impact on the results. Specifically for this research a new dataset of facial expressions, called FER+, has been created. Experiments have shown that, as the number of taggers increased, their agreement rate increased as well. They ended up with 10 taggers to label each face image, with one of eight emotions. This allowed for experimentation with multiple distribution schemes during training. Authors claim to have used a custom VGG13 model with 64x64 input, 10 convolution layers, interleaved with max pooling and dropout layers, and 8 emotion classes as output. Image 1. DCNN architecture used by the authors As mentioned earlier, various approaches for utilizing labels would be presented. In each example, probability distributions of emotion captured by the facial expression will be generated in a different way. Majority Voting (MV) is the simplest and most obvious method. Each observation gets assigned a label, with 1 for an emotion that was chosen most frequently and 0 for all other emotions. Multi-Label Learning (ML) approach assumes that it is natural for a picture to present more than one emotion. For each observation, whenever a certain emotion’s votes have exceeded a given threshold that emotion is labeled with 1, otherwise with 0. In Probabilistic Label Drawing (PLD), labels are dynamically assigned in each epoch of learning. Every observation gets a label with 1 for a chosen emotion, and 0 for all other emotions. The labels are randomly chosen with probability distribution, based on percentage of taggers’ votes for each of them. The Cross-entropy loss (CEL) method simply feeds the model with labels being equal to probability of an emotion appearing in observation. Exactly as before, probability distribution is based on percentage of taggers’ votes for each of emotions. Results of their work, which lead them to the conclusion that PLD and CEL are more effective in this scenario, were as follows: Scheme Accuracy MV \\(83.85\\%\\) ML \\(83.97\\%\\) PLD \\(\\pmb{84.99}\\%\\) CEL \\(84.72\\%\\) Table 1. Results of the evaluation of the models according to the article 4.1.4 Reproducibility analysis 4.1.4.1 Insufficiently explained data augmentation The original model had a 64x64 input, even though the original FER data was only 48x48. The input size was bigger because data augmentation was used. However, this process was only briefly mentioned. Due to the insufficient explanation of used methods, we chose to skip this process, and just repeat the other described steps, as we believed the data augmentation step not to be crucial to the hypothesis. The transformations applied in the original code were hard to reproduce exactly, as the exact specifics of data augmentation were split over multiple objects, variables, and methods. 4.1.4.2 Inconsistencies in the article As it turns out, the equations for the losses, which were a major part of the hypothesis, were written out incorrectly. The equations were in conflict with the words preceding them. For both Multi-Label Learning and Probabilistic Label Drawing, the losses were given in the form of: \\[\\mathcal{L} = - \\sum^{N}_{i=1}\\underset{k}{\\operatorname{argmax}}g(p)^{i}_{k}\\log q^{i}_{k}\\] where \\(g\\) was some form of an element-wise transformation. For ML, this was described as a new loss function, and for PLD it was described as standard cross entropy loss. The equation given is neither standard cross entropy loss, nor a valid loss function. \\(\\operatorname{argmax}\\) cannot be used as a gradient descent loss function, as it is not continuous, and its gradient is zero everywhere it is defined. After examining the source code, we realized the authors actually used standard cross entropy loss (extended by a transformation function on the target vector) losses except in the ML scenario, where the following loss was used: \\[\\mathcal{L}=- \\log\\sum^{N,\\#\\text{features}}_{i,k = 1}(p \\odot q)^{i}_{k}\\] where \\(\\odot\\) denotes element-wise multiplication. This loss was not present, and is not an equivalent of any loss function given in the article, both in writing or in the form of an equation. With these inconsistencies, a major part of the article is in direct conflict with the code. 4.1.4.3 Source code reproduction Multiple outdated dependencies were required, which forced us to downgrade our environment for it to work properly. Other than that, there were no issues with running the code. Both the setup process and usage was clearly described on the github page and worked as described. It is also worth noting that the original code is compatible with the CUDA architecture, making the training process significantly faster. After training each of the models once, we noticed that the accuracy we received was on average 2% lower than the values presented in the article. As no modifications were done to the source code, we are unsure why such differences appeared. However, the relation between the schemes which were highlighted in the article stayed the same - with majority voting and multi label being noticeably worse than probability or crossentropy. While this time the most accurate scheme turned out to be crossentropy, the gap between it and PLD is small in both the article and our results, and the difference can be attributed to variation. Scheme Accuracy MV \\(85.81\\%\\) ML \\(85.72\\%\\) PLD \\(86.58\\%\\) CEL \\(\\pmb{86.96}\\%\\) Table 2. Reproduced results of the evaluation of the models 4.1.4.4 The Keras model We tried to reproduce the original model in Keras1, trying to limit ourselves only to the information contained in the article, as long as it was possible. As mentioned before, we were forced to change the model to accept 48x48 inputs, and trained it as such, with appropriate losses equivalent to the losses in the original code. Using 48x48 inputs has caused a slight bottleneck in the network after the convolution and pooling layers, as the lower initial inputs reduce the dimensionality of the last convolution layer in the VGG architecture, but we assert that the bottleneck limits the capabilities of the model independently of the loss function. After training the 4 models once, we arrived at similar results as the original paper. The accuracies of the model were significantly lower (~\\(8\\%\\)), but this can be accounted for by a slight bottleneck in the middle of the model caused by the smaller input dimensions, the different gradient descent algorithm we used and untuned hyperparameters. Our training results were: Scheme Accuracy MV \\(77.42\\%\\) ML \\(76.52\\%\\) PLD \\(\\pmb{78.65\\%}\\) CEL \\(78.12\\%\\) Table 3. Results of the evaluation of the reproduced Keras models Our reproduction of the original model gave the same results - that PLD is the best method out of the four given in the article for predicting multilabel data. Sadly, it was not possible to recreate the article’s thesis using the article alone. 4.1.5 Analyzing the AxonDeepSeg paper The previous article has shown some of the issues that may appear when trying to reproduce an article. To further focus on methods of improving reproducibility, we analyze the AxonDeepSeg paper - an article that we have found to be very reproducible. Authors of this paper are introducing a new open-source software AxonDeepSeg, created to ease the process of axon and myelin segmentation from microscopy images of the nervous system. This can be especially useful in research of magnetic resonance imaging, as a validation method for new techniques. Said software has three features: - two ready to use CNN models trained from scanning electron microscopy (SEM) and transmission electron microscopy (TEM). - a CNN architecture, suited for axon and myelin segmentation problem - a training procedure, which can be used to generate new models based on manually-labelled data As the dataset for such a problem is not easily accessible, it had to be created for the purpose of this project. Microscopy images were marked and cross-checked by at least two researchers. Sheets were manually segmented using GIMP. The final ground truth dataset consists of a single png image with the corresponding values: background = 0 (black), myelin = 127 (red), axon = 255 (blue). Some of the pictures required small manual corrections to avoid false positive outcomes. Since this dataset is not large, an augmentation strategy was used in order to reduce overfitting and improve generalization. This strategy includes random shifting, rotation, rescaling, flipping, blurring and elastic deformation. The architecture of the CNN was designed for both SEM and TEM images. The image below explains the architecture design. Dashed lines signify that the segment is viable only for the SEM model, since the TEM model is smaller. Image 2. CNN architectures used in AxonDeepSeg models To assess the quality of the segmentation authors used Dice coefficient, given by \\(Dice = \\frac{2(|A \\cup B|)}{|A| + |B|}\\) where \\(2(|A \\cup B|)\\) is the number of pixels that well predicted by the model in both images, \\(|A|\\) is the number of well predicted pixels in image A and accordingly \\(|B|\\) for image B. To assess the performance of myelinated fiber detection, they used sensitivity (true positive rate) and precision (positive predictive value) measures. Both of them are based on the number of detected axons, calculated using the positions of their centroids. \\[TPR = TP/(TP+FN)\\] \\[PPV = TP/(TP+FP)\\] Authors evaluated their trained model with the metrics given above, which produced the results as follows: Modality Test sample(s) Axon Dice similarity Myelin Dice similarity Pixel-wise accuracy Sensitivity Precision SEM Rat 1 0.9089 0.8193 0.8510 0.9699 0.8468 SEM Rat 2 0.9244 0.8389 0.8822 0.9876 0.7987 SEM Human 0.8089 0.7629 0.8114 0.9300 0.7306 TEM Mice 0.9493 0.8552 0.9451 0.9597 0.9647 TEM Macaque 0.9069 0.7519 0.8438 0.9429 0.8129 Table 4. Results of the evaluation of the models according to the article 4.1.6 Reproducibility analysis 4.1.6.1 Installation instructions The installation was well documented, with all dependencies listed as an anaconda dependency file. Because of that, the process of installation of necessary libraries was streamlined to executing commands listed on the project’s page. The project also provides a video describing the installation process. 4.1.6.2 Usage instructions with examples Since the paper lead to the development of a scientific tool, the code ended up well-documented. A website containing detailed instructions regarding the usage of the tool is provided with the project. For ease of use, the tool also comes with several python notebooks with fully functional code snippets displaying its potential use cases and capabilities, such as data preparation, model training, processing images and usage of various evaluation metrics. 4.1.6.3 Accessible prepared data The source code comes with two example datasets, one for the SEM model and one for the TEM model, which are the same as the ones used in the article. Both of those datasets are available in a format already prepared for work with this tool. Additionally, the documentation contains instructions on how to create a compatible dataset from your own images, as well as a reference to the source of images used in the examples. 4.1.6.4 Documentation of differences between the code and the article The project itself is still being improved after the release of the article. Because of that, there do exist differences between the code and the original paper, but they are documented inside of the repository, and each difference is given a reason for the change. 4.1.6.5 Accessible pretrained models The aforementioned notebooks contained functions capable of downloading pretrained models for the tool from the internet, making the models accessible with a single line of code. As such, for most use cases there is no need of dedicating hours of processing time to train a different model. Evaluating those models yielded similar, if not slightly better results to the ones described in the article. The difference can be attributed to the constant improvement of the source code since the release of the article. Modality Test sample(s) Axon Dice similarity Myelin Dice similarity Pixel-wise accuracy Sensitivity Precision SEM Rat 1 0.9256 0.8366 0.9574 0.9178 0.9336 SEM Rat 2 0.9458 0.8278 0.9666 0.9337 0.9581 TEM Mice 0.9439 0.8661 0.9701 0.9179 0.9738 Table 5. Reproduced results of the evaluation of the models 4.1.6.6 Working training code The notebooks also contained example well commented code allowing easy model training, either with the use of training data used in the article or any other data conforming to the format specified in the documentation. We ensured that the code was working by successfully training a model with a single epoch. 4.1.7 Conclusion While the source code of the first article can be compiled without any difficulties, and the results it generates are similar to the ones described in the article, the code itself is not consistent with the article. The differences in loss methods, which are the focus of the paper, and the lack of a proper description of used augmentation methods makes this article poorly reproducible without access to the code. Even paired with the code, the paper is not a proper description of the code’s actions, and as such, the thesis of the article is not the thesis which the code is evidence for. In contrast, the second article provides a sufficient description of the technical knowledge required to reproduce the source code. No details relevant to the paper’s thesis were omitted. As the code changed over time, all inconsistencies with the article are labeled, and the reason for each one of them is given. The authors went to great lengths to ensure that the usage of the tool is as easy as possible, providing detailed instructions for common use cases, as well as video recordings and premade code snippets. The project also contains example datasets and pretrained models for each of the network types, further simplifying working with the tool. We posit that there are two main components of reproducibility of an article: the consistency of the method used with the method described in the paper, and a proper technical description of all variables relevant to the thesis, so that with enough effort, the method can be repeated again from the paper alone. What’s more, the inclusion of additional material, such as code snippets, is also beneficial for the ease of reproducibility. References Baker, M. (2016). Reproducibility crisis. Nature, 533(26), 353–66. Barsoum, E., Zhang, C., Ferrer, C. C., &amp; Zhang, Z. (2016). Training deep networks for facial expression recognition with crowd-sourced label distribution. In Proceedings of the 18th ACM international conference on multimodal interaction (pp. 279–283). New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/2993148.2993165 Tatman, R., VanderPlas, J., &amp; Dane, S. (2018). A practical taxonomy of reproducibility for machine learning research. Zaimi Aldo, W. M., Herman, V., Antonsanti, P.-L., Perone, C. S., &amp; Cohen-Adad, J. (2018). AxonDeepSeg: Automatic axon and myelin segmentation from microscopy data using convolutional neural networks. Scientific Reports, 8(1), 3816. https://doi.org/10.1038/s41598-018-22181-4 https://gist.github.com/mtizim/364d25f4bdc2ee00cb2a97d270d6aef2↩︎ "],["ara-cnn-a-bayesian-deep-learning-model-intended-for-histopathological-image-classification-.html", "4.2 ARA-CNN - a Bayesian deep learning model intended for histopathological image classification.", " 4.2 ARA-CNN - a Bayesian deep learning model intended for histopathological image classification. *Authors: Wojciech Szczypek, Jakub Lis, Jan Gąska (Warsaw University of Techcnology) "],["rethinking-the-u-net-architecture-for-multimodal-biomedical-image-segmentation.html", "4.3 Rethinking the U-Net architecture for multimodal biomedical image segmentation", " 4.3 Rethinking the U-Net architecture for multimodal biomedical image segmentation Grudzień Adrianna, Łukaszyk Marcin, Piasecki Michał 4.3.1 Abstrac In our work we wanted to explore what reproducibility is, and how it applies to deep learning. The articles we had choesen are An Improvement of Data Classification Using Random Multimodel Deep Learning (RMDL) and MultiResUNet : Rethinking the U-Net architecture for multimodal biomedical image segmentation. In both papers we faced simmilar issues and limitation ,yet we have concluded our work with diffrent results for each article. First paper is unreproducible and second one is mostly reproducible. 4.3.2 What Reproducibility Is? Reproducibility of an experiment plays crucial role in scientific community and serves as a proof for validity of both experiment and its consequences. It constitutes a basis of scientific endeavours and enable us to use science as a method to comprehend reality around us. This essential term means ability to repeat particular experiment and obtain similar results, while having the same conditions, data, tools and means of measurement. Unfortunately, it is not always possible to repeat experiments and conditions required, them might prevent larger community to conduct them. To show these difficulties, we would like to provide two examples, which clearly illustrate, that some experiments might be almost impossible to repeat. Firstly, let’s consider repeating Galileo experiments of dropping objects from fixed height to establish, whether objects fall with the same velocity. Such simple test is available for everyone, who is eager to explore experimentally basic Newtonian principles. On the other hand, if we would like to switch to quantum physics and prove experimentally quantum nonlocality, we are more than likely to be unable to conduct experiments proving this theory. The infrastructure required to conduct such experiments makes reproducibility an exclusive idea limited to only handful of people. It is hard to underestimate the significance of reproducibility in science. We listed below main reasons for its importance: 1 Proof for correctness of experiment Having obtained same results of an experiment conducted many times by the independent researchers, we are ensured that results are not coincidence. As a result, we can start treating new results as a proper description of phenomena around us. 2 Different views on experiment After repeating an experiment and acquiring same results, other researchers may draw different, interesting conclusions. It enables to use conducted experiment to full extent, squeezing out as many conclusions about reality as it is possible. 3 Possibility of improving experiment Other researchers may figure out how to improve experiment to obtain results in more efficient or precise way. 4.3.3 First article (An Improvement of Data Classification Using Random Multimodel Deep Learning (RMDL) ) 4.3.3.1 Summary of Article The main purpose of the article is creating combinations of different neural networks and finding the best model. In order to achieve these goal, authors use various optimalization methods , dropout layers and different models for prediction. Authors describe methods used in article: TF-IDF, Word2Vec for Feature Extraction, NaïveBayesClassifier, SVM and S2GD for classification and types of neural networks : Deep Neural Network, Convolutional Neural Network and Recurrent Neural Network. They explain all algorithms and ideas used in their work. Consequently, they combine different architectures to obtain one model – RMDL. Having constructed the model, authors decide to evaluate their masterpiece on different datasets. They use it both on text data (WOS, Reuters IMDB, 20newsgroup) and image data (MNIST, Cifar-10). Authors evaluate model with metrics like : Precision, Recall F1 and Score. The all model is build with usage of Cuda, tensorflow and keras libraries. At the end, authors discuss how its model might come useful in classification task. 4.3.3.2 Our Work with Reproducibility of First Article We had run all our experiments in google Collaboratory. One of the datasets was not available due to unavailability of server that article was referring to. Rest of datasets were freely accessible. Most of our issues were due to hardware limitations. The predominant ones were RAM lack of free memory errors and memory leaks. Collaboratory doesn’t allow to run program more than 12 hours straight so we couldn’t train the biggest models. Our main goal was to explore reproducibility of given paper, and main issue that we combined were lack of definitions of terms that authors used. One of them was error rate that wasn’t defined. They didn’t specify what depth of RMDL is and a lot of our work was to guess based of their results what is the proper definition. 4.3.3.3 Reproducibility of this Article We cannot acknowledge it as reproducible. The results that were achieved by authors are not properly documented and defined in article making it hard to check if we had similar results. Some of our struggle results from the hardware limitations where we cannot train similar sized models or in extreme situations cannot train at all. 4.3.4 Second article (MultiResUNet : Rethinking the U-Net architecture for multimodal biomedical image segmentation) 4.3.4.1 Introduction Throughout XX century we have developed multiple methods for imaging human bodies. Radiography, functional magnetic resonance and fluoroscopia are just mere examples of methods we invented. It is hard to underestimate its benefits, they enable us to understand processes inside our bodies to much greater extent. Thanks to them, it is possible to detect pathologies and monitor our health at scale not imaginable before. Having been exposed to such vast amount of medical images, computer scientists started thinking about creating software, which could analyse them automatically. Since late 60’s researchers have been attempting to build models segmenting medical images and conducting diagnosis. Were we to create such software, we would relieve doctors from segmenting images manually and support them with independent diagnosis from computer. As a result, it would make our healthcare system less overwhelmed and more people could seek help in predicament. 4.3.4.2 Summary There were multiple ways of creating such models. Firstly, ‘rule-based’ approach was commonplace, with scientists explicitly setting rules for model evaluation. Poor generalization resulted in different approaches based on geometrical analysis or fuzzy logic. Nonetheless, recently we can notice general shift towards Deep Learning as a way of segmenting and diagnosing images. Convolutional neural networks have been obtaining outstanding results, with U-Net architecture standing out of the crowd. We would like to review article “MultiResUNet Rethinking the U-Net Architecture for biomedical image segmentation.” In this article, modification of U-Net – MultiResUNet is proposed. Authors are convinced that small changes in well know U-Net architecture lead to better model performance. Authors list situations, in which U-Net architecture do not get sufficient results. 4.3.4.3 What is U-Net? U-net is a deep learning architecture that was proposed in 2015. It’s a modification of simple neural network with odd number layers of convolution layers and a max-pooling or up-convolutions layers, which is used to for segmentation of images. This modification is a series of connections between each simmilar distanced layer from middle one. This “bends” our network making it U-shaped, thus the name. Each level of network consists of series of convolution layers and then max-pool or up-convolution layer that depends on which side of and U shape are we in. Figure 4.1: Example of U-net In order to improve U-Net , authors decide to introduce following changes in model architecture. They factorize the bigger and more expensive 5 × 5 and 7 × 7 filters as a succession of 3 × 3 filters. This saves computations as we need calculate 3 3x3 convolutions layers to have similar results as calculating 3x3, 5x5 and 7x7 layers. They concatenate all of the estimated layers with one made from 1x1 convolution layers. This allows to better retrieve spatial features from different scales. Figure 4.2: Example of ResBlock The excellence of U-Net architecture stems mainly from the introduction of connection between layers, which enables to save spatial information while going deeper into U-Net architecture. However, authors speculate that combination of a simple copy of an output of previous convolutional layer and information received after all transformations conducted in model results in discrepancy in information carried by both of them. To minimize this discrepancy authors suggest “ResPath” - series of consecutive convolutional layers. They decrease number of convolutional layers while going deeper into the architecture, due to smaller difference in information in “depth” of the model. Figure 4.3: Example of ResPath In the end we get simmilar looking neural network but with improvements in most places. Authors states that their structure has simmilar computational complexity but gives better results. Figure 4.4: MultiResUNet Authors test their improved architecture on 5 different datasets and compare results with “basic” U-Net. They introduce new metric: Jacard Index to measure performance of both models. For mask A and model’s marked area B it calculates the intersection of both sets divided by their union. Authors show that their model obtain better results, learns faster and is less vulnerable to perturbations. 4.3.4.4 Our Work with Reproducibility of Second Article We decided to repeat experiments in Google Colab using Python Unfortunately,we were not able to obtain all datasets used by authors. After long research, we managed to download datasets: Fluorescence microscopy images Murphy Lab, Electron microscopy images, Dermoscopy images IC-2017dataset Having downloaded datasets, we encountered many problems. There were multiple problems with transforming datasets into desired format used by authors. Problems with conversion, partition of datasets and ambiguous files led to frustration. Moreover, Google Colab, environment in which we tried to reproduce results, had not enough RAM to smoothly generate results. At the end, we managed to obtain some results, which were similar to results from the article. [Wykresy dermoskopi i endoskopi] (#fig:Original Rsults)Orginal Plots (#fig:Our Rsults)Our Plots 4.3.4.5 Reproducibility of this article Article is reproducible. In article authors try to improve U-Net architecture. Providing simple explanations and empirical data, they explain motivation behind their improvements. Unfortunately, few of datasets used by authors are no longer available, which may make it harder to conduct same experiments. However, rest of the datasets is easily accessible, so it is still possible to verify results obtained by authors. When it comes to authors argumentation behind new architecture, we are not completely satisfied. As students, who have little expertise in using Machine Learning in medical areas, we are not satisfied with mathematical explanations provided by the authors. No mathematical proofs and small number of datasets make us feel unease, if authors’ architecture outperforms basic U-Net architecture globally. 4.3.5 Conclusion In both articles we coudn’t simply state if article are reproducible. We had to have non-binary measure for each. First article is mostly unreproducible due to lack of proper deffinitions thath dosn’t allow us to be sure thath our experiments are conducted in the same environment. Second article is reproducible. Our tries of recreating simmilar results were mostly positive and most of our issues were with hardwere limitations. The biggest truble is some missing data stes thath are not longer available from refer to sources. Some of our worry is in how authors explained their reasoning as it’s mostly based on assumption and results. "],["dl2-rmdl-unet.html", "4.4 Analyzing Reproducibility churns", " 4.4 Analyzing Reproducibility churns Authors: Marceli Korbin, Szymon Szmajdziński, Paweł Wojciechowski (Warsaw University of Techcnology) "],["title.html", "4.5 Title", " 4.5 Title Authors: Filip Chrzuszcz, Szymon Rećko, Mateusz Sperkowski (Warsaw University of Technology) 4.5.1 Title, Authors, Abstract, Keywords 4.5.2 Introduction 4.5.3 Related Literature 4.5.4 Methods 4.5.5 Result Skrot do pierwszego projektu: Despite lower results than in the first paper, in most datasets we still achieved better results than the baselines paper attempted to beat. The ones that we weren’t able to reproduce where either limits of processing power, or could be assigned to effect off randomness which is basis od this paper. The authors unfortunately didn’t include their randomness results, therefore their exact calculations aren’t reproducible. 4.5.6 Discussion 4.5.7 Conclusion 4.5.8 References 4.5.9 Random Multimodel Deep Learning for Classification Results .tg {border-collapse:collapse;border-spacing:0;} .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; overflow:hidden;padding:10px 5px;word-break:normal;} .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;} .tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} Dataset WOS-5736 WOS-11967 WOS-46985 Reuters-21578 Score Source Paper Repr. Paper Repr. Paper Repr. Paper Repr. RMDL 3 RDLs 90.86 89.37 87.39 84.25 78.39 — 89.10 87.64 9 RDLs 92.60 89.28 90.65 — 81.92 — 90.36 89.83 15 RDLs 92.66 — 91.01 — 81.86 — 89.91 — 30 RDLs 93.57 — 91.59 — 82.42 — 90.69 — Table 1 4.5.9.1 Reuters-21578 Paper’s Plots Our Reproduction Figure 1 4.5.9.2 WOS-5736 Paper’s Plots Our Reproduction Figure 2 .tg {border-collapse:collapse;border-spacing:0;} .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; overflow:hidden;padding:10px 5px;word-break:normal;} .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;} .tg .tg-9wq8{border-color:inherit;text-align:center;vertical-align:middle} .tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} Dataset IMDB 20NewsGroup Score Source Paper Repr. Paper Repr. RMDL 3 RDLs 89.91 88.49 86.73 — 9 RDLs 90.13 — 87.62 — 15 RDLs 90.79 — 87.91 — Table 2 ERROR RATE 1-Accuracy .tg {border-collapse:collapse;border-spacing:0;} .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; overflow:hidden;padding:10px 5px;word-break:normal;} .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;} .tg .tg-9wq8{border-color:inherit;text-align:center;vertical-align:middle} .tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} Dataset MNIST CIFAR-10 Score Source Paper Repr. Paper Repr. RMDL 3 RDLs 0.51 0.55 9.89 38.23 9 RDLs 0.41 0.65 9.1 36.91 15 RDLs 0.21 — 8.74 — 30 RDLs 0.18 — 8.79 — Table 3 4.5.9.3 CIFAR 10 Paper’s Plots Our Reproduction Figure 3 4.5.9.4 MNIST Paper’s Plots Our Reproduction Figure 4 4.5.10 Adversarial Attacks Against Medical Deep Learning Systems "],["machine-learning.html", "Chapter 5 Machine Learning", " Chapter 5 Machine Learning Author: Hubert Baniecki An ever-growing domain of machine learning decision systems in medicine has crossed ways with the COVID-19 pandemic. Precariously, a vast majority of the proposed predictive models focus on achieving high performance; while overlooking comprehensive validation. Nowadays, providing representative data, model explainability, even bias detection become mandatory for responsible prediction making in high-stakes medical applications. The following short papers introduce new views into the already published work on the topic of patients’ COVID-19 mortality prognosis using supervised machine learning: Validation and comparison of COVID-19 mortatility prediction models on multi-source data. Michał Komorowski, Przemysław Olender, Piotr Sieńko, Konrad Welkier One model to fit them all: COVID-19 survival prediction using multinational data. Marcelina Kurek, Mateusz Stączek, Jakub Wiśniewski, Hanna Zdulska Transparent machine learning to support predicting COVID-19 infection risk based on chronic diseases. Dawid Przybyliński, Hubert Ruczyński, Kinga Ulasik Comparison of neural networks and tree-based models in the clinical prediction of the course of COVID-19 illness. Jakub Fołtyn, Kacper Grzymkowski, Konrad Komisarczyk "],["validation-and-comparison-of-covid-19-mortality-prediction-models-on-multi-source-data.html", "5.1 Validation and comparison of COVID-19 mortality prediction models on multi-source data", " 5.1 Validation and comparison of COVID-19 mortality prediction models on multi-source data Authors: Michał Komorowski, Przemysław Olender, Piotr Sieńko, Konrad Welkier 5.1.1 Abstract The work of (yan_et_al_2020?) from the first months of the COVID-19 pandemic laid the foundations for further research in the area of machine learning models that predict patients’ chances of survival. It was done by introducing a simple decision tree that in the opinion of the inventors could potentially support the cause. Since that time a few papers have emerged that touch upon the same case in which other researchers tested this decision tree on their datasets. Their findings that the original model is not suitable for patients from other countries than China appeared interesting to us and hence in the following paper we present results of our work which aim was to build models on each of the considered datasets as well as on all of them combined in order to find an universal approach for classification of patients from various countries. After testing various models such as XGBoost, Logistic Regression, SVM and Tabnet we came up with the conclusion that there is no one model for all of the datasets that includes only at most 4 crucial variables. 5.1.2 Introduction At the end of 2019, the novel coronavirus disease 2019 (COVID-19) pandemic broke out. In the next few months it quickly spread around the world. Daily cases were increasing exponentially. Although most of the cases involved mild symptoms, the healthcare system in many countries still became overloaded. Therefore, the physicians were in an urgent need of a quick system to predict how severe the state of a patient could get, especially if there is a risk of death or a need to be put at the ICU. A certain solution for this problem was using machine learning to create a mortality prediction model based on easily available biomarkers. One of the first models proposed in may 2020 by (yan_et_al_2020?) could predict mortality rates of the patients 10 days in advance with great accuracy. The article is considered the state-of-the-art in the COVID-19 machine learning. Some issues with this model appeared when it was used on data from different hospitals. When used on biomarkers from patients in France, Netherlands and the United States the model’s accuracy significantly dropped. In our article, we describe how the datasets differ, we analyze how (yan_et_al_2020?) model compares to newer model (Zheng et al. 2020a) which evaluates hospitalization priority for COVID-19 patients by validating it on different datasets and propose new model which could yield great results on both datasets. 5.1.3 Data description We have begun our work from analyzing article (yan_et_al_2020?), authors have created a model based on blood samples collected form 375 patients from the Wuhan region in the first quarter of 2020. Data from another 110 patients were treated as an additional test set. Dataset contains 81 variables, including 74 describing the blood tests results, but not all tests were performed on each patient. Scientists finally created a model based on three variables: lactate dehydrogenase, lymphocytes and high-sensitivity C-reactive protein, these are going to be crucial also in our work. The article (yan_et_al_2020?) received several replies, scientists from the USA, France and the Netherlands validated the model on datasets from hospitals from their home countries. The results pointed to the problem of rashly predicting death for patients who eventually survived Covid-19 infection. They also claimed that the (yan_et_al_2020?) model learning from most recently performed measurements was not an appropriate tool to prioritize ICU admissions and should therefore use results from first tests. The datasets from corresponding articles contained 3 previously highlighted features, in datasets from (Dupuis et al. 2021) and (Barish et al. 2021) there are multiple test results and other useful features such as age, but information about other blood components is not included. The data from (Barish et al. 2021) was especially helpful, the data was shared to us after consultation with one of the article’s authors. The data came from Northwell Health, New York State’s largest hospital network serving 11 million patients and contained 1038 records. Table 1: contains all most important information about each of datasets and shows which of them contains features needed to create models. Article Records Variables Lactate Dehyd. C-protein Lymphocytes Neutrophil Age (yan_et_al_2020?) 485 81 Yes Yes Yes Yes Yes (Barish et al. 2021) 1038 14 Yes Yes Yes No No (Quanjel, Holten, Gunst-van der Vliet, et al. 2021) 305 15 Yes Yes Yes No Yes (Dupuis et al. 2021) 178 43 Yes Yes Yes No Yes (Zheng et al. 2020a) 214 33 Yes Yes Yes Yes Yes Careful inspection of distributions of these key variables from various articles helps to understand problem with applying model form [china] on dataset from corresponding articles. Lactate dehydrogenase was the most important feature in model from (yan_et_al_2020?), relying only on its value patients were supposed to be directed to the ICU. The chart shows that indeed distribution of sever cases is significantly shifted towards higer values in comparsion with non severe cases, but such a large difference does not occur in other datasets, patients requiring additional care usually have an increased lactate dehydrogenase, but not as much as patients from China. An interesting dependence ocuurs in the dataset from (Zheng et al. 2020a), a large part of the data also comes from China, there is a great distinction between the severely and slightly ill, but the overall lactate dehydrogenase levels are much lower. Additionally, slightly ill patients from Europe and the US have higher LDH levels than severely ill patients from China. Figure 5.1: Comparison of lactate dehydrogenase amongst datasets A similar situation occurs when it comes to high-sensitivity C-reactive protein (CRP), in the dataset from (yan_et_al_2020?) the distributions are clearly shifted among themselves, but similar dependency occurs only in the data from [new]. Patients suffering from the virus slightly and severely from other countries with have less varied level of CRP. Figure 5.2: Comparison of high reactive C protein amongst datasets The same dependence occurs with lymphocytes, one additional fact about these distributions is that French do not have lower level of these cells as do patients from rest of the world. Figure 5.3: Comparison of lymphocytes amongst datasets Neutrofil was not used in [china] model, but it separates sever and non-severe cases in both datasets very well. Figure 5.4: Comparison of neutrophil amongst datasets Age is also crucial feature in predicting virus severity, the young people get sick more mildly than the old ones. Figure 5.5: Comparison of patients age amongst datasets 5.1.4 Comparison of the models We have decided to validate two models proposed by (yan_et_al_2020?) and (Zheng et al. 2020a). The first one is a decision tree with 3 nodes and the second one is an XGboost model with 4 explanatory variables. To verify if the model proposed by Yan et al. is effective in predicting COVID-19 mortality amongst patients around the world, validation on external datasets was made. The decision tree was tested on combined data from American(Barish et al. 2021), French(Dupuis et al. 2021) and Chinese(yan_et_al_2020?), (Zheng et al. 2020a) hospitals. The merged dataset has 1842 observations and 3 explanatory variables: lymphocyte, lactate dehydrogenase (denoted by LDH), and C-reactive protein (CRP). All these medical indicators have been previously studied as key factors in severity and mortality prediction(Cao et al. 2020), (Zhao et al. 2020). To compare Yan et al. decision tree with exemplary models, we created Logistic Regression, Support Vector Machine (SVM) and Tabnet model. Tabnet is a deep learning model for tabular data (Arik and Pfister 2020). It allows creating high performance and explainable classifier on numeric data. The merged dataset was splitted into a training table (80% of observations) and a test table (20%). Then, newly created models were trained on the training table and validated on the test table. Yan’s algorithm was tested on the same part of a merged dataset as other models. We have selected accuracy, precision, recall, ROC AUC and AUPRC as final score metrics. Table 1: Model scores on test table from merged dataset. Model Accuracy Recall Precision ROC-AUC AUPRC Tree 0.674 0.908 0.474 - - Logistic Regression 0.832 0.606 0.776 0.861 0.746 Tabnet 0.840 0.569 0.838 0.913 0.824 SVM 0.777 0.596 0.631 0.843 0.699 Although the decision tree achieved high recall score (0.908), its precision is unsatisfactory. Models trained on multinational data have distinctly higher accuracy and precision thus they are more suitable for medical triage. SVM Tabnet proved to be the most efficient algorithm, having AUC at 0.91 and AUPRC at 0.824. The SVM was the worst model out of the new ones, with AUC at level 0.843 and AUPRC at 0.699. Figure 5.6: AUPRC results on test table from merged dataset. Tabnet scored significantly better than Logistic Regression and SVM. Six months after the Yan et al’s paper had been published, another group of Chinese scientists released their article about machine learning models for COVID-19 patients (Zheng et al. 2020a). They examined several algorithms from which XGBoost performed most efficiently. Besides variables that had been used in the previous article, XGBoost proposed by Yichao Zheng et al. also needed information about the level of Neutrophil in each patient’s blood sample. Importantly, this model was originally designed to predict severity therefore it was expected to have high recall and low precision in mortality prediction task. Model validation was performed on the dataset from Yan et al’s article. To compare XGBoost performance, another Tabnet model was created. It was fitted to the same data as XGBoost and then validated on Yan et al’s dataset. Additionally, we decided to create two versions of each model. The first one with the Neutrophil variable (the same explanatory variables as proposed in the article) and the second one with Age instead of Neutrophil. Table 2: XGBoost and Tabnet scores on Yan et al’s dataset Model Accuracy Recall Precision ROC-AUC AUPRC XGBoost with N 0.573 0.994 0.515 0.865 0.791 XGBoost with Age 0.746 0.975 0.646 0.942 0.918 Tabnet with N 0.538 0.976 0.432 0.868 0.787 Tabnet with Age 0.747 0.984 0.588 0.941 0.891 As expected, all models achieved very high recall. However, XGBoost from the article (with Neutrophil) had significantly worse performance than XGBoost with Age variable. A similar situation occurred with Tabnet models. Overall, XGBoost with the hyperparameters proposed by Zichao Zheng et al. but trained on data with Age column in the place of Neutrophil had best results with 0.942 AUC and 0.918 AUPRC. Figure 5.7: AUPRC results on Yan et al dataset. Models with Age variable are more effective in mortality prediction. 5.1.5 Conclusions Summing up, the first conclusion is that the original decision tree is not an algorithm that can be universally used to assess chances of survival of any patient around the world. The fact that the blood characteristics vary with ethnic groups is significant and therefore usage of some additional variables can improve the models’ predictive capabilities. This is not what the authors of (yan_et_al_2020?) expected but adding Age or Neutrophil variables to the set indeed boosted the performance. Hence, the intuitive windup would be that the best model is the XGBoost with Age variable since it gets high results according to multiple metrics but it is not that simple. For example its precision is significantly lower in comparison to the Tabnet model prepared on the merged datasets. Therefore, what should be learned from this paper is that none of the proposed models that were validated and compared by us should be used on other datasets than those on which they were trained. References Arik, S. O., &amp; Pfister, T. (2020). TabNet: Attentive Interpretable Tabular Learning. AAAI Conference on Artificial Intelligence (AAAI). https://arxiv.org/abs/1908.07442 Barish, M., Bolourani, S., Lau, L. F., Shah, S., &amp; Zanos, T. P. (2021). External validation demonstrates limited clinical utility of the interpretable mortality prediction model for patients with COVID-19. Nature Machine Intelligence, 3(1), 25–27. https://doi.org/10.1038/s42256-020-00254-2 Cao, Y., Liu, X., Xiong, L., &amp; Cai, K. (2020). Imaging and clinical features of patients with 2019 novel coronavirus SARS-CoV-2: A systematic review and meta-analysis. Journal of Medical Virology, 92(9), 1449–1459. https://doi.org/10.1002/jmv.25822 Dupuis, C., De Montmollin, E., Neuville, M., Mourvillier, B., Ruckly, S., &amp; Timsit, J. F. (2021). Limited applicability of a COVID-19 specific mortality prediction rule to the intensive care setting. Nature Machine Intelligence, 3(1), 20–22. https://doi.org/10.1038/s42256-020-00252-4 Quanjel, M. J. R., Holten, T. C. van, Gunst-van der Vliet, P. C., Wielaard, J., Karakaya, B., Söhne, M., et al. (2021). Replication of a mortality prediction model in dutch patients with COVID-19. Nature Machine Intelligence, 3(1), 23–24. https://doi.org/10.1038/s42256-020-00253-3 Zhao, Q., Meng, M., Kumar, R., Wu, Y., Huang, J., Deng, Y., et al. (2020). Lymphopenia is associated with severe coronavirus disease 2019 (COVID-19) infections: A systemic review and meta-analysis. International Journal of Infectious Diseases, 96, 131–135. https://doi.org/10.1016/j.ijid.2020.04.086 Zheng, Y., Zhu, Y., Ji, M., Wang, R., Liu, X., Zhang, M., et al. (2020a). A Learning-Based Model to Evaluate Hospitalization Priority in COVID-19 Pandemics. Patterns, 1(6), 100092. https://doi.org/10.1016/j.patter.2020.100092 "],["one-model-to-fit-them-all-covid-19-survival-prediction-using-multinational-data.html", "5.2 One model to fit them all: COVID-19 survival prediction using multinational data", " 5.2 One model to fit them all: COVID-19 survival prediction using multinational data Authors: Marcelina Kurek, Mateusz Stączek, Jakub Wiśniewski, Hanna Zdulska 5.2.1 Abstract During the outbreak of SARS-CoV-2 many scientists tried to build a model that was able to predict the survival or death of patients based on available medical data. Yan et al. (2020a) were among the first researchers to introduce their model based on blood data (lactic dehydrogenase (LDH), lymphocyte percentage, and high-sensitivity C-reactive protein (hs-CRP)) with 0.90 accuracy, however, recreations of this model trained on other countries’ data - the US, Netherlands, and France were not so successful. In this article, we explore the possibility of building an international model for predicting COVID-19 survival. We focused on exploring the models, their variable importance, analyzed the bias they introduced, and concluded with guidelines for future researchers working on this topic. 5.2.2 Introduction Machine learning models are becoming popular in medicine because of the various opportunities they create. Such algorithms may be useful in performing early diagnosis, assessing disease severity, or personalizing treatment. During the COVID-19 pandemic, there were numerous possibilities associated with machine learning models. For example, an algorithm could predict which patients should be qualified for the Intensive Care Unit or who should be treated under a respirator. Additionally, due to the worldwide character of COVID-19 pandemic, it was easier to gather data about symptoms and various blood measures from thousands of patients. During this project, we have been working with the article “An interpretable mortality prediction model for COVID-19 patients” by Yan et al. (2020a). The article presents a decision tree, which predicts whether a patient will die or survive the disease based on the level of lactic dehydrogenase, C-reactive protein, and lymphocytes in blood samples. The presented model obtained high accuracy and ROC AUC scores on data from Yan et al., but had poor scores on datasets from the Netherlands and the US. According to the article ‘A study in transfer learning: leveraging data from multiple hospitals to enhance hospital-specific predictions’ Wiens et al. (2014) increasing the number of dataset sources for machine learning models may lead to poor performance. With a large number of datasets, controlling the extent to which data from each hospital contribute to the final model is complicated. As a result, procedures for identifying an optimal setting for hyperparameters can quickly become inefficient. In our research, entitled ‘One model to predict them all,’ we focused on answering the question if creation of the model with satisfactory behavior and performance independent of the data origin is possible. This approach could have led us to create an international model, which could help in recognizing the severity of COVID-19 cases. Contrastingly, creating such an algorithm could be impossible for many reasons, for example conducting different examinations in hospitals or different medical standards. 5.2.3 Data sources To create an international model, we used data from three different sources: China, New York, and the Netherlands. Dataset from China is added to the article ‘An interpretable mortality prediction model for COVID-19 patients’ by Yan et al. (2020a). It contains 375 observations of LDH, CRP, and Lymphocytes percentage, along with the outcome of the COVID-19 disease. Dataset from the Netherlands is attached to the article ‘Replication of a mortality prediction model in Dutch patients with COVID-19’ by Quanjel, Holten, Gunst-van der Vliet, et al. (2021). It contains 306 observations. Apart from information about blood samples, the age and gender of patients are also provided. The New York dataset is not attached to the article ‘External validation demonstrates the limited clinical utility of the interpretable mortality prediction model for patients with COVID-19’ by Barish et al. (2021), as it contains confidential data. However, due to the civility of the authors, we were provided with the dataset. The New York dataset contains 1000 observations of blood samples, which is more than in both other datasets combined. 5.2.4 Model building First, a model performing very well on training data can perform poorly on new observations coming from a different location. In our case, we found that the model presented by Yan et al. (2020a) is not portable and does perform poorly on data from New York and the Netherlands. However, models trained on such a dataset tend to include the source of data as an important feature (as in Figure 5.8). That means the model is biased for some or all sources of data to give better predictions. Simultaneously it fails to achieve scores as good as models dedicated for each country. Unfortunately, such models have low scores in different performance metrics and are worse than a model created specifically for a given source of data. As an example, we used LazyPredict twice to compare scores of various models trained on data from all 3 sources: the first run was on data without the column containing the source of data and a second run was on data containing all columns. The top 3 models from both runs are presented in the tables below (results were sorted by “ROC AUC”). Table 1: Scores from the LazyPredict for the top 3 classifiers sorted by “ROC AUC” trained on data from 3 sources excluding the column containing information about the source of data Model Accuracy Balanced Accuracy ROC AUC F1 Score Time Taken AdaBoostClassifier 0.753 0.714 0.714 0.751 0.191 NearestCentroid 0.700 0.702 0.702 0.708 0.023 KNeighborsClassifier 0.726 0.689 0.689 0.725 0.038 Table 2: Scores from the LazyPredict for the top 3 classifiers sorted by “ROC AUC” trained on data from 3 sources including the column containing information about the source of data. Model Accuracy Balanced Accuracy ROC AUC F1 Score Time Taken RandomForestClassifier 0.784 0.740 0.740 0.779 0.326 XGBClassifier 0.770 0.735 0.735 0.768 0.223 LabelPropagation 0.767 0.729 0.729 0.765 0.114 When the source of data is excluded from the training dataset, the results look less promising. This is expected as the origin of data proved to be a useful feature. Next, we tuned the parameters of the top models from each table with grid search and checked their scores using the dalex (Baniecki et al. 2020b) explainer. RandomForestClassifier and AdaBoostClassifier scored 0.83 and 0.76 ROC AUC respectively which proves the hypothesis about the influence of the origin. Our next step was to explore the ways to measure the effect of training on the data from certain countries. As we have proved, the origin is important in the modeling. Whether it is the effect of the healthcare system, biological differences between people, or the hospitals they were in, it may influence the model in ways that may not be clearly predicted. To determine how important the place of origin is, we trained RandomForestClassifier on data with the said place. We can see in the figure below that the two most important features are blood-related. However, the third most important feature is the information whether the patient was from China or not. Figure 5.8: Variable importance for RandomForestClassifier The variable importance here is measured with perturbations (Fisher et al. 2019a). The idea behind it is that we first measure the performance on the entire model. Secondly, we reorder elements of a column (variable), train model, and then measure performance again. The difference in performances is called drop-out loss and it depicts how important the variables are. This information was not surprising, however, we also made a similar test that conveniently simplifies the model. We created a surrogate decision tree from the earlier classifier. The surrogate model is trained to approximate the predictions of a black box predictor (Molnar 2019). Figure 5.9: Surrogate model for RandomForestClassifier As expected the variable indicating the source of the data was among the three most important splits. We also had concerns over the bias introduced by said sources. The bias (or fairness) of the classifier is discrimination in decisions made by the model. The kind of fairness that we will focus on is called group fairness and it concerns the difference in outcomes between groups of people. There are many ways to measure this bias with so-called fairness metrics. They all can be derived from confusion matrices for different subgroups. We will focus on five of them that are used in Fairness check (Wiśniewski and Biecek 2021). With the help of Equal Opportunity (TPR) (Hardt et al. 2016), Predictive Parity (PPV) (Chouldechova 2016), Predictive Equality (FPR) (Corbett-Davies et al. 2017), Statistical Parity (STP) (Dwork et al. 2012), and Accuracy Equality (ACC) (Berk et al. 2017). The Fairness check detects bias in metrics via the four-fifths rule (Code of Federal Regulations 1978). It simply looks at metrics for the privileged subgroup (in this case whether data comes from China) and for unprivileged subgroups and calculates their ratio. If this ratio is within (0.8, 1.25) then we assume that there is no bias. To investigate this claim we trained two machine learning models. The first one was XGBoost with the same parameters as in Yan et al. (2020a). The results were quite surprising as the model introduced bias in 4 metrics. Figure 5.10: Fairness check on XGBoost. The model has bias present in four metrics. To make sure that the model did not overfit the data and gave steady predictions we also checked the fairness of the Histogram-based Gradient Boosting Classification Tree from the scikit-learn package (Pedregosa et al. 2011). The bias was indeed lower but still significant. Therefore we also decided to use bias mitigation strategies. To do this, we firstly merged some subgroups for the algorithms to work better. We tried to make “fair classifiers” with two Python packages fairtorch and fairlearn (Bird et al. 2020). They are related to each other as the fairtorch implements the solutions from fairlearn. Using those in-processing algorithms (these are the kind of mitigation approaches that reduce the bias during model training) we obtained 2 additional models. One of them was a neural net and the other was the Histogram-based Gradient Boosting Classification Tree that was trained using the reductions approach. The amount of bias reduced by the neural net from fairtorch was not satisfying enough and therefore will not be shown here. However, the results from fairlearn were quite good. Figure 5.11: Fairness check on models before and after the reductions. As we can see despite the fact that the reduced model does not fit within the green field we decided that the bias was in fact reduced. The last thing to check was the performance of the said model. Such reductions in the amount of bias may result in a significant drop in performance. In this case, it was the same. The ROC AUC metric dropped from 0.71 to 0.59 which for the medical applications is not enough. Therefore we concluded that in the case of this data the models were biased towards different origins. 5.2.5 Summary and discussion Machine learning can be very useful when applied to medical data. Over the last few years, the increased amount of health-related information created many possibilities to aid medical specialists, for example in Decision Support Systems. Such data can be successfully applied to predicting COVID-19 as proven by Yan et al. (2020a). However, creating a multinational model isn’t trivial for a few reasons. Even in a single country, forms of data collection vary from hospital to hospital. It’s impossible to enforce a unified system and data format across continents. Moreover, some measurements may depend on the time of the day taken - for example, blood pressure will be different in the morning and in the evening. Another problem was observed by Kaushal et al. (2020): Whether by race, gender or geography, medical AI has a data diversity problem: researchers can’t easily obtain large, diverse medical data sets—and that can lead to biased algorithms. The case of having a model that doesn’t take into account the origin of the sample will lead to simplification of such a model since cut-off values were different for data from China, NY, and the Netherlands. This leads to lower metric scores and this excludes it from using in a medical environment, where precision is crucial. We were working on a dataset merged from data coming from different countries and in this setting, machine learning models tend to be biased towards different nationalities. Our attempts at reducing this discrimination were not successful. We believe that with a bigger or more balanced dataset we would have a slightly better chance at meeting our goal. Making models on country levels or geographical regions to achieve maximum fairness is more reasonable. It will also allow scientists to achieve the best results in predicting COVID-19 survival and ultimately we defeat COVID-19 and live happily ever after, till the end of our days, as said by short man(HOBBIT) and acclaimed author Bilbo Baggins. References Baniecki, H., Kretowicz, W., Piatyszek, P., Wisniewski, J., &amp; Biecek, P. (2020b). dalex: Responsible Machine Learning with Interactive Explainability and Fairness in Python. arXiv:2012.14406. https://arxiv.org/abs/2012.14406 Barish, M., Bolourani, S., Lau, L. F., Shah, S., &amp; Zanos, T. P. (2021). External validation demonstrates limited clinical utility of the interpretable mortality prediction model for patients with COVID-19. Nature Machine Intelligence, 3(1), 25–27. https://doi.org/10.1038/s42256-020-00254-2 Berk, R., Heidari, H., Jabbari, S., Kearns, M., &amp; Roth, A. (2017). Fairness in Criminal Justice Risk Assessments: The State of the Art. Sociological Methods &amp; Research. https://doi.org/10.1177/0049124118782533 Bird, S., Dudík, M., Edgar, R., Horn, B., Lutz, R., Milan, V., et al. (2020). Fairlearn: A toolkit for assessing and improving fairness in AI (No. MSR-TR-2020-32). Microsoft. https://www.microsoft.com/en-us/research/publication/fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai/ Chouldechova, A. (2016). Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments. Big Data, 5. https://doi.org/10.1089/big.2016.0047 Code of Federal Regulations. (1978). SECTION 4D, UNIFORM GUIDELINES ON EMPLOYEE SELECTION PROCEDURES (1978). https://www.govinfo.gov/content/pkg/CFR-2014-title29-vol4/xml/CFR-2014-title29-vol4-part1607.xml Corbett-Davies, S., Pierson, E., Feller, A., Goel, S., &amp; Huq, A. (2017). Algorithmic Decision Making and the Cost of Fairness. https://doi.org/10.1145/3097983.3098095 Dwork, C., Hardt, M., Pitassi, T., Reingold, O., &amp; Zemel, R. (2012). Fairness through awareness. ITCS. https://doi.org/10.1145/2090236.2090255 Fisher, A., Rudin, C., &amp; Dominici, F. (2019a). All Models are Wrong, but Many are Useful: Learning a Variable’s Importance by Studying an Entire Class of Prediction Models Simultaneously. Journal of Machine Learning Research, 20(177), 1–81. http://jmlr.org/papers/v20/18-760.html Hardt, M., Price, E., Price, E., &amp; Srebro, N. (2016). Equality of Opportunity in Supervised Learning. NeurIPS. https://papers.nips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html Kaushal, A., Altman, R., &amp; Langlotz, C. (2020). Health Care AI Systems Are Biased. Scientific American. https://www.scientificamerican.com/article/health-care-ai-systems-are-biased Molnar, C. (2019). Interpretable Machine Learning: A Guide for Making Black Box Models Explainable. https://christophm.github.io/interpretable-ml-book Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., et al. (2011). Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12, 2825–2830. Quanjel, M. J. R., Holten, T. C. van, Gunst-van der Vliet, P. C., Wielaard, J., Karakaya, B., Söhne, M., et al. (2021). Replication of a mortality prediction model in dutch patients with COVID-19. Nature Machine Intelligence, 3(1), 23–24. https://doi.org/10.1038/s42256-020-00253-3 Wiens, J., Guttag, J., &amp; Horvitz, E. (2014). A study in transfer learning: leveraging data from multiple hospitals to enhance hospital-specific predictions. Journal of the American Medical Informatics Association, 21(4), 699–706. https://doi.org/10.1136/amiajnl-2013-002162 Wiśniewski, J., &amp; Biecek, P. (2021). fairmodels: A Flexible Tool For Bias Detection, Visualization, And Mitigation. arXiv:2104.00507. https://arxiv.org/abs/2104.00507 Yan, L., Zhang, H.-T., Goncalves, J., Xiao, Y., Wang, M., Guo, Y., et al. (2020a). An interpretable mortality prediction model for COVID-19 patients. Nature Machine Intelligence, 2(5), 283–288. https://doi.org/10.1038/s42256-020-0180-7 "],["transparent-machine-learning-to-support-predicting-covid-19-infection-risk-based-on-chronic-diseases.html", "5.3 Transparent machine learning to support predicting COVID-19 infection risk based on chronic diseases", " 5.3 Transparent machine learning to support predicting COVID-19 infection risk based on chronic diseases Authors: Dawid Przybyliński, Hubert Ruczyński, Kinga Ulasik 5.3.1 Abstract The moment the COVID-19 outbreak in 2019 occurred, people started using machine learning in order to help in managing the situation. Our work is a build up to research described in (Yan et al. 2020b) and in (Bello-Chavolla et al. 2020) articles. We improve and create new models predicting COVID-19 mortality based on blood parameters and analyze original models presented in the articles. We train a prediction model calculating risk of getting infected with the virus depending only on person’s sex, age and chronic diseases and created an application which calculates the COVID-19 mortality risk based on input data and explains the model with visualizations. Considering accuracy of the model, calculations based only on chronic diseases may not achieve best results but we propose a transparent machine learning approach which doesn’t need any additional medical information, is explainable and easy to understand and requires only data that everyone has access to. 5.3.2 Introduction The COVID-19 pandemic is currently one of the world’s biggest problem, affecting not only everyone’s private lives, but also other areas of human activity, including research. In the scientific field, this is a specific issue that scientists are still able to approach this problem in completely different ways, because no distinctive and right path has yet been established. It is in the combination of these three aspects that we see an opportunity to deepen the current understanding of the pandemic and create a useful tool for predicting mortality from the disease. All available models for predicting coronavirus mortality have numerous disadvantages, which include predictions based on small data sets, the use of medical information unavailable to the average person, unexplained models or their low effectiveness. In order to provide additional knowledge for each of us, we present a solution that eliminates all these drawbacks, to a greater or lesser extent. The proposed application bases its mortality prediction on the information about chronic diseases that the patient suffers from. In addition to the predicted mortality, it also provides a break down plot as an explanation of the mortality, SHAP visualization, to show impact of the most important parameters and also analysis of the mortality risk depending on age. By using the random forest model, we also ensure the explainability of the entire process. Thanks to this solution, our application is useful for both ordinary people and doctors. 5.3.3 Flaws As a first step, we started analyzing one of the first articles (Yan et al. 2020b) on the development of predictive models to predict COVID-19 mortality in depth. The main disadvantages of the model presented in it were the very poor prediction of mortality in the case of testing on external data sets (Quanjel, Holten, Vliet, et al. 2021), (Barish et al. 2020) and the related allegation of not testing one’s solution on external data (Barish et al. 2020). In addition, our team noticed a very large bias present in the original data, thanks to which even the simplest models achieved extremely high efficiency of over 90%. 5.3.4 Improvements Our first approach to the problem was to try to improve the work of the articles authors in question through very different methods. Firstly, we started from creating a correlation between variables heatmap to see which features were the most important. In the next step we created two models using Gradient Boosting and Ada Boosting which both achieved better results than the model presented in (Yan et al. 2020b). Moreover, thanks to (Barish et al. 2020) we were given access to additional data set with more records and were able to test the models. Next, we used Principal Component Analysis to try to split the data using reducing dimensions of the data receiving a surprisingly good result - we were able to separate the two classes with one straight line. Finally, took a look at the data distribution - it is visibly skewed left. We fixed it by applying proper transformation which resulted in improving the model. While evaluating the models we mainly focused on precision score which defines what percentage of patients predicted to be in chosen class (survival and death in this case) really represents that class. If the precision score is low it is either dangerous for people’s life or leads to overcrowding the hospitals. Prediction using original parameters on external data First, we decided to use the broader pandemic knowledge that the authors of the original article did not have to improve the model on its default data. Inspired by a newer article (Barda et al. 2020), we decided to test how the selection of variables improved according to the latest knowledge will affect the predictions of the predictive models. For the suggested parameters: age, C-reactive protein, chloride, albumin, lymphocyte count and LDH, we created a correlation map to select the most important of them. Figure 5.12: Correlation heatmap Figure 5.12 shows that among the most important features there are age, albumin, LDH and C protein, which we used in our models. After training and testing the GradientBoostingClassifier and AdaBoostClassifier models on slightly reduced data (the original test set had to be replaced by the test and validation set), we obtain cross-validation precision at the level of 0.979 and 0.958 and the following reports 5.13, 5.14. Moreover we present confusion matrix from original model 5.15. Figure 5.13: Confusion matrix for GradientBoosting Figure 5.14: Confusion matrix for AdaBoosting Figure 5.15: Confusion matrix for original model The above results confirm the improvement in the quality of the original model 5.15, whose precision score for death is only 0.81 and cross-validation stands around 0.97. Thanks to the authors of the paper (5-3-american?), we were given access to additional data with features coincident with those we already had. Data set contained over 1000 observations and only fourteen features that were selected by owners to fit as good model as possible. In order to test previous model (AdaBoost), we used it to predict the outcome for those thousand patients, receiving following results 5.16. Figure 5.16: Results received for external data Model performance has dropped significantly, the outcome is far from desired. PCA In order to analyze data further, we performed Principal Component Analysis. For visualization’s simplification we considered only first two, most substantial components. Obtained two-dimensional plot is shown on Figure 8. Explained variance ratios were: 0.226 (for the first component) and 0.063 for the second component), which results in total explained variance ratio of 0.289 for both components together. Taking absolute values of the scores of each feature from the first component might be also used for feature selection. Those with the highest magnitudes were: Prothrombin activity, Lactate dehydrogenase, albumin, Urea, neutrophils(%), (%)lymphocyte. Some of the features are those, that we have already known are important, such as albumin, (\\%)lymphocyte or Lactate dehydrogenase, but age was around the middle, not among the top ones. Figure 5.17: Principal Component Analysis The most noticeable fact is that our two classes are almost separable with just a single line. Even without any sort of complex machine learning or other algorithms, it’s possible and not complicated to fit a line that divides cases ended in death from cases followed by patient’s recovery. As an example, same visualization with additional function \\(y=2.5x+2.5\\), created without any sort of optimization techniques, is presented below. Figure 5.18: Principal Component Analysis with line separator Such division achieves (train) accuracy of 0.94, which is almost as good as results received by machine learning algorithms described in the paper, what might encourage to consider given data not authoritative. Data distribution analysis We analyzed the distribution of percentage of Lymphocytes, Lactate dehydrogenase and High sensitivity C-reactive protein by creating histograms for the original and the new data. Figure 5.19: Original Data distribution Figure 5.20: New Data distribution We noticed that all variables are strongly left skewed which is unfavorable for the model because more reliable predictions are made if the predictors and the target variable are normally distributed. Trying to make our model better, we applied square root transformation. Then we trained a new model on them (also using Ada Boost Classifier) and we tested it on the new data: Figure 5.21: Skewed data results From figure 5.21 we can see that the model generally improved (the accuracy is higher) and it perform slightly better. General drawbacks In addition to the aforementioned bias, our models can still be accused of learning on very small data sets not exceeding even 1000 observations, which still affects the uncertainty related to the effectiveness of the presented proposals. Moreover, the most promising model, developed on an external data set, unfortunately did not show sufficiently high efficiency to be useful in medical applications. In addition, the models that have been proposed by us, are, unfortunately, not explainable models, which makes them less desirable by doctors. The last, and perhaps the least obvious disadvantage is that the data used for prediction alone is unattainable for single entities, as blood tests can only be performed by highly qualified medical personnel. This aspect makes solutions based on these models incomprehensible to the average person, which significantly limits their usefulness. Summary To sum up, the most desirable effect of our work turns out to be an explainable model with high-quality predictions, based on a large database. In addition, it should be based on easy-to-obtain information about a person’s health and be understandable to both ordinary people and physicians. In search of research that would help us explore this branch of machine learning, we manage to find an article (Bello-Chavolla et al. 2020), which provided us with both a comprehensive set of data, understandable to everyone, and a very rich information background. 5.3.5 Transparent Machine Learning While working in machine learning one can often encounter an issue called the black-box problem (Rai 2020). It occurs when a model is complex, unexplainable and not transparent. Explainability solves this problem by “unpacking the black-box” which is essential in building trust in the model. That is why we want to create an explainable model which could be useful to every person which doesn’t need any additional medical information and easy to understand and requires only data that everyone has access to. We used explainers from (Biecek 2018b), package for explainable Machine Learning, to create visualizations, that allow user to understand where the results come from and because of that, they are more transparent and clear. Model To create the model we use random forest from (Wright and Ziegler 2017) package constructing a multitude of decision trees which are one of the most transparent and easy to explain models, even for people not familiar with machine learning concepts. Moreover, we tuned and tested XGBoost ((Wright and Ziegler 2016) and Support Vector Machines from (Meyer et al. 2021) package to maximize AUC measure. Ranger performs best with AUC of 0.92, XGboost turns out to be slightly worse with AUC of 0.87. Support Vector Machines achieve the worst results - AUC around 0.78 (figure 5.22). Figure 5.22: AUC for considered models Data The data set used in the application is the same one that was used in the article (Bello-Chavolla et al. 2020) and it is an open source data published by General Directorate of Epidemiology in Mexico. It consists of 150,000 records from Mexican hospitals, of which over 50,000 are patients with confirmed coronavirus. The most important data for the project are information of chronic diseases, age and date of death. 5.3.6 Application In order to achieve our goal we create an easy application, in which one can choose his or hers chronic diseases, sex, age and it calculates the COVID-19 mortality risk for particular infected person. Additionally, in the bookmarks there are presented plots about the model. The first one 5.23 is a Break Down plot which shows how the contributions attributed to individual explanatory variables change the mean models prediction. Despite printing out the risk it also enables its user an easy option to understand the outcome. Another one 5.24 is Ceteris Paribus profile which examines the influence of an explanatory variable (which is age in this case) by assuming that the values of all other variables do not change. This visualization is very useful for doctors to properly distinguish a higher risk groups. The last one 5.25 is a SHAP plot, it calculates the importance of a feature by comparing what a model predicts with and without the feature. Our application is avaiable here: https://hubertr21.shinyapps.io/Explainable_COVID19_Mortality_Predictor/ Figure 5.23: Mortality Breakdown profile Figure 5.24: Mortality Ceteris Paribus profile Figure 5.25: Mortality SHAP profile 5.3.7 Conclusions Figure 5.26: XAI stakeholders from (Barredo Arrieta et al. 2020c) Thanks to the developed application, various XAI stakeholders, as presented in (Barredo Arrieta et al. 2020c) - 5.26, may have easier access to transparent and explainable model that estimates mortality risk in case of being infected by COVID-19. Our application allows people to see which diseases are contributing the most to the outcome, without the need of doctor’s examination, such as any blood properties and other information, that can only be gathered by a specialist are not necessary. Only by selecting diseases that one suffers from, reliable prediction can be obtained quickly and without leaving home. References Barda, N., Riesel, D., Akriv, A., Levy, J., Finkel, U., Yona, G., et al. (2020). Developing a COVID-19 mortality risk prediction model when individual-level data are not available. Nature Communications, 11. https://doi.org/10.1038/s41467-020-18297-9 Barish, M., Bolourani, S., Lau, L. F., Shah, S., &amp; Zanos, T. P. (2020). External validation demonstrates limited clinical utility of the interpretable mortality prediction model for patients with COVID-19. Nature Machine Intelligence, 3, 25–27. https://doi.org/10.1038/s42256-020-00254-2 Barredo Arrieta, A., Díaz-Rodríguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., et al. (2020c). Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, 58, 82–115. https://doi.org/10.1016/j.inffus.2019.12.012 Bello-Chavolla, O. Y., Bahena-López, J. P., Antonio-Villa, N. E., Vargas-Vázquez, A., González-Díaz, A., Márquez-Salinas, A., et al. (2020). Predicting Mortality Due to SARS-CoV-2: A Mechanistic Score Relating Obesity and Diabetes to COVID-19 Outcomes in Mexico. The Journal of Clinical Endocrinology &amp; Metabolism, 105, 2752--2761. https://doi.org/10.1210/clinem/dgaa346 Biecek, P. (2018b). DALEX: Explainers for Complex Predictive Models in R. Journal of Machine Learning Research, 19(84), 1–5. https://jmlr.org/papers/v19/18-416.html Meyer, D., Dimitriadou, E., Hornik, K., Weingessel, A., Leisch, F., Chang, C.-C., &amp; Lin, C.-C. (2021). e1071: Misc Functions of the Department of Statistics, Probability Theory Group. R package. https://CRAN.R-project.org/package=e1071 Quanjel, M. J. R., Holten, T. C. van, Vliet, P. C. G. der, Wielaard, J., Karakaya, B., Söhne, M., et al. (2021). Replication of a mortality prediction model in Dutch patients with COVID-19. Nature Machine Intelligence, 3, 23–24. https://doi.org/10.1038/s42256-020-00253-3 Rai, A. (2020). Explainable AI: from black box to glass box. Journal of the Academy of Marketing Science, 48, 137–141. https://link.springer.com/article/10.1007/s11747-019-00710-5 Wright, M. N., &amp; Ziegler, A. (2016). XGBoost: A Scalable Tree Boosting System. SIGKDD International Conference on Knowledge Discovery and Data Mining. https://doi.org/10.1145/2939672.2939785 Wright, M. N., &amp; Ziegler, A. (2017). ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R. Journal of Statistical Software, 77(1), 1–17. https://doi.org/10.18637/jss.v077.i01 Yan, L., Zhang, H.-T., Goncalves, J., Xiao, Y., Wang, M., Guo, Y., et al. (2020b). An interpretable mortality prediction model for COVID-19 patients. Nature Machine Intelligence, 2(5), 283--288. https://www.nature.com/articles/s42256-020-0180-7 "],["comparison-of-neural-networks-and-tree-based-models-in-the-clinical-prediction-of-the-course-of-covid-19-illness.html", "5.4 Comparison of neural networks and tree-based models in the clinical prediction of the course of COVID-19 illness", " 5.4 Comparison of neural networks and tree-based models in the clinical prediction of the course of COVID-19 illness Authors: Jakub Fołtyn, Kacper Grzymkowski, Konrad Komisarczyk 5.4.1 Abstract The COVID-19 pandemic overwhelmed medical staff around the world, showing that effective and explainable models are needed to help allocate limited resources to those in need. Many published models for predicting COVID-19 related ICU admission and mortality were tree-based models or neural network models. We compared the two architectures in effectiveness, explainability and reproducibility. The two architectures appear to be similar with regards to their effectiveness, but the neural network model had significant reproducibility issues and worse explainability. 5.4.2 Introduction In 2020 many papers presenting models aimed to predict the course of COVID-19 in patients emerged (Ma et al. 2020; Yan et al. 2020c; Zheng et al. 2020b). Article we are referring to (Li et al. 2020) differed from the majority with the use of neural networks. In the past there were many attempts to compare effectiveness of artificial neural networks and tree-based models at various tasks using different types of data including tabular data (Ahmad et al. 2017; Arsad et al. 2013) and tasks connected to hospital patients outcomes (J. Wang et al. 2009). There were analyses showing superiority of forests over simple neural networks on small (up to 1000 records) tabular datasets (Klambauer et al. 2017). Data used by the referred article’s authors consisted of 1020 or 1106 observations depending on the researched problem. Authors note the importance of explainability of clinical models (Tonekaboni et al. 2019) due to the need of establishing clinician’s trust to successfully deploy a model. There are tools for explaining tree-based models (Biecek and Burzykowski 2021; Chen and Guestrin 2016) and some of methods are not available for neural networks. Tools to calculate SHAP values for forest models were developed (Scott M. Lundberg et al. 2019), while general algorithm to compute them exactly in a reasonable complexity for deep learning models, only approximations can be made. Explaining neural networks with SHAP values is an important issue in the field (R. Wang et al. 2021). The referred article does not provide a source code for the replication of the results. Taking care about the reproducibility is considered a major problem in the academic society (WUoT 2020). Motivated by the preceding we aim to contribute to the work started by the authors of the referred article. At first, we try to replicate models. Then we propose different network architectures and XGBoost models. Finally, we compare the effectiveness of all the models at the prediction tasks and their explanations based on the SHAP values. 5.4.3 Methods Three neural network models and XGBoost model were trained for both ICU admission prediction and death prediction. The tested neural network architectures were: replication of the architecture proposed by (Li et al. 2020) (referred to as the Baseline model), modified version of that architecture using binary cross entropy as the loss function (Baseline (crossentropy) or Modified)), basic neural network model using two hidden layers of 32 neurons each with binary cross entropy as the loss function (Basic). Neural network models were created using Keras Python library (Abadi et al. et al. 2015). All models were trained and tested on data provided in the article (Li et al. 2020). Feature selection was performed as described in the article. Data was split in a 75:25 (train:test) ration. To reduce overfitting, an internal 0.2 validation set size was used. Effectiveness of those models were compared using receiver operating characteristic area under curve (ROC AUC) metric (Hanley 2014). ROC AUC values were calculated using the test data held out from training. Neural network models were trained 25 times each to compare stability. SHAP values were calculated for the xgboost models using the R treeshap library (Komisarczyk, Konrad and Maksymiuk, Szymon and Koźmiński, Paweł and Biecek, Przemysław 2020). Approximate SHAP values were approximated for neural network models using DeepExplainer from the Python SHAP library. Feature importance was calculated using mean SHAP values and compared. 5.4.4 Results ROC curve comparison between all ICU admission models is shown in the figure 5.27. XGBoost model was the best-performing model, based on the ROC AUC score. The best-performing neural network model was the Basic model (the neural network model with only 2 layers), with a ROC AUC score of 0.696. In the mortality prediction task, however, the Basic neural network model outperformed all other models, as shown in figure 5.28. It is worth noting that in both cases the Baseline model, which was the replication of the (Li et al. 2020) model had the lowest ROC AUC scored, and therefore is indicated to have performed the worst. Figure 5.27: ROC curves comparison for ICU admission prediction models. The dashed line indicates a random classifier (a model that classifies values in a random way). Each row in the legend contains model’s line color, name and ROC AUC score. Neural network model names explained: Baseline - a replication of the model from (Li et al. 2020) article, Basic - basic model using two hidden layers of 32 neurons each with binary cross entropy as the loss function, Modified - modified version of the article model using binary cross entropy as the loss function. Figure 5.28: ROC curves comparison for mortality prediction models. The dashed line indicates a random classifier (a model that classifies values in a random way). Each row in the legend contains model’s line color, model’s and ROC AUC score. Neural network model names explained: Baseline - a replication of the model from (Li et al. 2020) article, Basic - basic model using two hidden layers of 32 neurons each with binary cross entropy as the loss function, Modified - modified version of the article model using binary cross entropy as the loss function. To ensure the reliability of neural network models, each model was trained and tested 25 times. The resulting boxplots can be seen in the figure 5.29 for the ICU admission prediction data and in the figure 5.30 for the mortality data. The comparison score for both cases was ROC AUC score. As we can see in figure 5.29, the Baseline model trained on ICU admission data has proven to be especially unreliable, with AUC score ranging from 0.35 up to 0.7, while other 2 models outperformed it, with both better scores and lower variances. Figure 5.30 shows that models trained and tested on the mortality data had overall better ROC AUC scores than their ICU admission counterparts, but some outliers can be noticed for both Baseline and Modified, reaching as low as 0.5 ROC AUC score. Figure 5.29: each boxplot represents 25 independent model tests. Neural network model names explained: Baseline - a replication of the model from (Li et al. 2020) article, Basic - basic model using two hidden layers of 32 neurons each with binary cross entropy as the loss function, Modified - modified version of the article model using binary cross entropy as the loss function. Figure 5.30: each boxplot represents 25 independent model tests. Neural network model names explained: Baseline - a replication of the model from (Li et al. 2020) article, Basic - basic model using two hidden layers of 32 neurons each with binary cross entropy as the loss function, Modified - modified version of the article model using binary cross entropy as the loss function. Feature importance comparison for ICU admission task is shown in figure 5.31. In all of these models Procalcitionin was the most important feature (it is worth reminding, that our models were trained only on the top 5 features for ICU admission task, and top 6 features for mortality prediction task), while other features’ significance differ between models. What is more, Procalcitionin was much more impactful for the neural network model, with mean SHAP value nearly doubling the value of the 2nd most import feature. Figure 5.31: a) Boruta algorithm results from the article. Only the top 5 features are taken into consideration. b) feature importance barplot for xgboost model c) feature importance barplot for Basic neural network (model using two hidden layers of 32 neurons each with binary cross entropy as the loss function). The bars in b) and c) represent the mean SHAP values for each feature. Feature importance comparison for mortality prediction task is shown in figure 5.32. Age was the most important feature for all of these models. As in ICU admission prediction task, other features’ significance differ between models. Figure 5.32: a) Boruta algorithm results from the article. Only the top 6 features are taken into consideration. b) feature importance barplot for xgboost model c) feature importance barplot for Basic neural network (model using two hidden layers of 32 neurons each with binary cross entropy as the loss function). The bars in b) and c) represent the mean SHAP values for each feature. 5.4.5 Discussion The COVID-19 pandemic has demonstrated a need for quick development, testing, validation and deployment of machine learning models. However, this can’t come at the expense of reproducibility, as the replication crisis still poses a serious issue. Parts of the description provided in the reference article can be imprecise enough to be misunderstood or have more than one meaning. In the field of machine learning both data and source code are necessary components to reproduce results. This is even more important when proposing unusual model architectures, as those can lead to new discoveries in the field. However, when proposing such an architecture, a baseline model should be used to demonstrate that the proposed architecture can preform better. Finally, different external validation techniques should be used, such as a leave-one-hospital-out cross-validation or working on combined data from multiple sources. While our team focused primarily on comparing model architectures, other chapters in the WB Book examine those aspects of fair artificial intelligence. In conclusion, we have shown the importance of providing reproducible code, as well as having a baseline model to compare the results. 5.4.6 Source code Source code for all models and figures mentioned in the article, including used data is available at the Github repository https://github.com/konrad-komisarczyk/wb2021 in the proj2 subdirectory. References Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., et al., et al. (2015). TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. USENIX conference on Operating Systems Design and Implementation. https://dl.acm.org/doi/10.5555/3026877.3026899 Ahmad, M. W., Mourshed, M., &amp; Rezgui, Y. (2017). Trees vs Neurons: Comparison between random forest and ANN for high-resolution prediction of building energy consumption. Energy and Buildings, 147, 77--89. https://doi.org/10.1016/j.enbuild.2017.04.038 Arsad, P. M., Buniyamin, N., &amp; Manan, J. A. (2013). Prediction of engineering students’ academic performance using Artificial Neural Network and Linear Regression: A comparison. ICEED. https://doi.org/10.1109/iceed.2013.6908300 Biecek, P., &amp; Burzykowski, T. (2021). Explanatory Model Analysis. Chapman; Hall/CRC, New York. https://pbiecek.github.io/ema/ Chen, T., &amp; Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. KDD. https://doi.org/https://doi.org/10.1145/2939672.2939785 Hanley, J. A. (2014). Receiver Operating Characteristic (ROC) Curves. Wiley StatsRef: Statistics Reference Online. https://doi.org/10.1002/9781118445112.stat05255 Klambauer, G., Unterthiner, T., Mayr, A., &amp; Hochreiter, S. (2017). Self-Normalizing Neural Networks. arXiv:1706.02515. https://arxiv.org/abs/1706.02515 Komisarczyk, Konrad and Maksymiuk, Szymon and Koźmiński, Paweł and Biecek, Przemysław. (2020). treeshap: Fast SHAP values computation for ensemble models. R package. https://github.com/ModelOriented/treeshap Li, X., Ge, P., Zhu, J., Li, H., Graham, J., Singer, A., et al. (2020). Deep learning prediction of likelihood of ICU admission and mortality in COVID-19 patients using clinical variables. PeerJ, 8. https://peerj.com/articles/10337/ Lundberg, Scott M., Erion, G. G., &amp; Lee, S.-I. (2019). Consistent Individualized Feature Attribution for Tree Ensembles. ICML Workshop. https://arxiv.org/abs/1802.03888 Ma, X., Ng, M., Xu, S., Xu, Z., Qiu, H., Liu, Y., et al. (2020). Development and validation of prognosis model of mortality risk in patients with COVID-19. Epidemiology and Infection, 148. http://doi.org/10.1017/S0950268820001727 Tonekaboni, S., Joshi, S., McCradden, M. D., &amp; Goldenberg, A. (2019). What Clinicians Want: Contextualizing Explainable Machine Learning for Clinical End Use. Machine Learning for Healthcare. http://proceedings.mlr.press/v106/tonekaboni19a.html Wang, J., Li, M., Hu, Y., &amp; Zhu, Y. (2009). Comparison of hospital charge prediction models for gastric cancer patients: neural network vs. decision tree models. BMC Health Services Research, 9(1). https://doi.org/10.1186/1472-6963-9-161 Wang, R., Wang, X., &amp; Inouye, D. I. (2021). Shapley Explanation Networks. ICLR. https://openreview.net/forum?id=vsU0efpivw WUoT. (2020). ML case studies: Reproducibility of scientific papers. https://mini-pw.github.io/2020L-WB-Book/reproducibility.html Yan, L., Zhang, H.-T., Goncalves, J., Xiao, Y., Wang, M., Guo, Y., et al. (2020c). An interpretable mortality prediction model for COVID-19 patients. Nature Machine Intelligence, 2(5), 283--288. https://www.nature.com/articles/s42256-020-0180-7 Zheng, Y., Zhu, Y., Ji, M., Wang, R., Liu, X., Zhang, M., et al. (2020b). A Learning-Based Model to Evaluate Hospitalization Priority in COVID-19 Pandemics. Patterns, 1(9), 100173. https://doi.org/10.1016/j.patter.2020.100173 "],["rashomonml.html", "Chapter 6 RashomonML", " Chapter 6 RashomonML "],["topic.html", "6.1 Topic", " 6.1 Topic Authors: Adrian Stańdo, Maciej Pawlikowski, Mariusz Słapek (Warsaw University of Technology) 6.1.1 Abstract Technological advances were early adopted by healthcare with great benefits and developments. In many health-related realms machine learning is crucial such as: development of new medical procedures, the treatment of chronic diseases, the management of patient records and data. Explainable AI (XAI) gives invaluable tools in healthcare for understanding the models by humans. The aim of this article is to compare PDP profiles for a Rashomon set with a given metric. Following we use five different metrics of distance function and based on that we compare PDP curves. Additionally, the library in the Python language has been created, which automates this research. We have observed that the created models, which have similar scores, have different variable importances. In this paper these differences were measured and assessed to understand better the problem of predicting hospital mortality using data from MIMIC-III. 6.1.2 Literature review Rashomon is a intriguing Japanese movie in which four people witness an incident from different vantage points. When they come to testify in court, they all report the same facts, but their stories of what happened are very different. In machine learning Rashomon set is used to characterise problem in which many different models offer accurate results describing the same data. However, not every accurate model gives a right conclusion as described in (Breiman et al. 2001): “If the model is a poor emulation of nature, the conclusion might be worng”. Herein authors also explain basics of Rashomon sets on example. Much more in depth and mathematical description is provided in (Semenova et al. 2019). Another important topic related to Rashomon sets is analysing the feature importance of the model. It was described in this article (Fisher et al. 2019b), where authors suggested to study the maximum and minimum of variable importance across all models included in the Rashomon set. This technique was called MCR (Model Class Reliance). Furthermore, (Dong and Rudin 2020) presented technique to visualise the “cloud” of variable importance for models in the set, which could help us understand the Rashomon set and choose the one which give the best interpretation. The last question stated in the article (Rudin et al. 2021) was about choosing model from the Rashomon set. It might be a difficult task, especially when we lack good exploration tools. (Das et al. 2019) created a system called BEAMS that allows to choose the most important features. Next, the program searches the hypothesis space in order to find model which fits best to given constraints. Since this system works only with linear regression classifiers, (Rudin et al. 2021) stated a question if it is possible to design a simmilar system which will search only models within the Rashomon set. 6.1.3 Results 6.1.3.1 Results of models search 6.1.4 Best models 6.1.4.1 Boxplots of abs_sum metric for the best models 6.1.4.2 Boxplots of abs_sum metric for each feature 6.1.4.3 PDP curve for albumin_std 6.1.4.4 PDP curve for atempc_min References Breiman, L. et al. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the author). Statistical science, 16(3), 199–231. Das, S., Cashman, D., Chang, R., &amp; Endert, A. (2019). BEAMES: Interactive multimodel steering, selection, and inspection for regression tasks. IEEE Computer Graphics and Applications, 39(5), 20–32. https://doi.org/10.1109/MCG.2019.2922592 Dong, J., &amp; Rudin, C. (2020). Exploring the cloud of variable importance for the set of all good models. Nature Machine Intelligence, 2(12), 810–824. Fisher, A., Rudin, C., &amp; Dominici, F. (2019b). All models are wrong, but many are useful: Learning a variable’s importance by studying an entire class of prediction models simultaneously. Journal of Machine Learning Research, 20(177), 1–81. Rudin, C., Chen, C., Chen, Z., Huang, H., Semenova, L., &amp; Zhong, C. (2021). Interpretable machine learning: Fundamental principles and 10 grand challenges. arXiv preprint arXiv:2103.11251. Semenova, L., Rudin, C., &amp; Parr, R. (2019). A study in rashomon curves and volumes: A new perspective on generalization and model simplicity in machine learning. arXiv preprint arXiv:1908.01755. "],["title-1.html", "6.2 Title", " 6.2 Title Authors: Jan Borowski, Konstanty Kraszewski, Krzysztof Wolny 6.2.1 Literature review In 1950 Japanese director, Akira Kurosawa, presented film Rashomon. Movie resolves around four witnesses, that describes the same crime in four different ways. This situation was called Rashomon effect after the name of the movie. In other words Rashomon effect is a situation when we have multiple different descriptions to the same event. This term is commonly used in multiple sciences like sociology, psychology or history. At the begging of the 21st century Rashomon effect was introduced to predictive modelling by Leo Breiman and his work ‘Statistical modeling: The Two Cultures’(Breiman et al. 2001). In this article he named Rashomon effect situation, where there are many approximately-equally accurate models. Although these models have similar results, they can differ, when it comes to the way they managed to achieve it. Breiman called for closer examination of the Rashomon effect and conclusions that can be drawn from it. Recently, we can observe growing interest in Rashomon effect, although there is still a lot to be discovered. One of the articles, that bring closer the problem is ‘A study in Rashomon curves and volumes: A new perspective on generalization and model simplicity in machine learning’[6-0-rashomon-intro]. It provides several approaches for estimating the size of the Rashomon effect as well as the usefulness of the Rashomon curve in model selection. References Breiman, L. et al. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the author). Statistical science, 16(3), 199–231. "],["rashomon-on-mimic-draft.html", "6.3 Rashomon on MIMIC - draft", " 6.3 Rashomon on MIMIC - draft Authors: Degórski Karol, Fic Piotr, Kamiński Adrian (Warsaw University of Technology) 6.3.1 Abstract Rashomon effect occurs when there are many different machine learning models with similar predictions. Therefore choosing only one model out of them may have impact on the final results, so it should be done consciously, carefully and with the help of XAI methods. In our study, we performed an analysis of different XGBoost models using PCA and KMeans algorithms, so that we explained the factors that influenced on their final behavior. The task that we reproduced was an in-hospital mortality prediction conducted by (Tang et al. 2018). For building rashomon sets we used publicly available MIMIC-III dataset, which contains medical information. Our results suggest that XGB models from rashomon set may be grouped into clusters in the reduced parameter space. 6.3.2 Review of the literature The term Rashomon effect was created to describe a situation when there are many different models with quite similar predictions. Very often there are many different descriptions giving about the same minimum error rate, so that we cannot point one model as the best (Breiman et al. 2001). As an example of this effect in reality they gave Linear Regression model and finding 5 from 30 best describing variables of a given problem. In this case there are approximately 140,000 such subsets. The authors explained that usually we choose the model which has best results on a test set, although there may be also different subsets of 5 variables that give very similar results. They also noticed that this effect occurs in different models, such as decision trees or neural networks. Furthermore, (Semenova et al. 2019) contributed to expand the study about Rashomon effect. They defined Rashomon set as a subset of models that have similar performance to the best model in terms of loss function. Moreover they introduced Rashomon ratio, that represents the fraction of models that fit our data equally well. Also they explained that Rashomon curve is a function of empirical risk versus the Rashomon ratio. They saw that there is a good generalization of the Rashomon curve’s elbow model when choosing between performance and simplicity of the model. They found out that interpretability of model is connected with Rashomon sets. Accordingly, when the Rashomon set is large there may exist simpler and higher performing model. Analysis of the Rashomon effect is still a new and open for developement field of the interpretable machine learning. Because of that, there are remaining challanges and problems, which are missing a state-of-the-art approach. Some of them are a proper measure of the Rashomon set, the best techniques of its visualization and optimal choice of the model from the Rashomon set (Rudin et al. 2021). One of useful tools for the mentioned tasks is a framework called Variable Importance Clouds (Dong and Rudin 2020), which can be used for studying the variable importance among Rashomon set. 6.3.3 Raw results 6.3.3.1 PCA dimension reduction in hyper-parameters space 6.3.3.2 Mean hyper-parameters values among clusters 6.3.3.3 Mean score values among clusters 6.3.3.4 The best models from clusters: variable importance 6.3.3.5 The best models from clusters: PD plots 6.3.3.6 The best models from clusters: ALE plots 6.3.3.7 Metrics change versus hyper-parameters over rashomon set References Breiman, L. et al. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the author). Statistical science, 16(3), 199–231. Dong, J., &amp; Rudin, C. (2020). Exploring the cloud of variable importance for the set of all good models. Nature Machine Intelligence, 2(12), 810–824. Rudin, C., Chen, C., Chen, Z., Huang, H., Semenova, L., &amp; Zhong, C. (2021). Interpretable machine learning: Fundamental principles and 10 grand challenges. arXiv preprint arXiv:2103.11251. Semenova, L., Rudin, C., &amp; Parr, R. (2019). A study in rashomon curves and volumes: A new perspective on generalization and model simplicity in machine learning. arXiv preprint arXiv:1908.01755. Tang, F., Xiao, C., Wang, F., &amp; Zhou, J. (2018). Predictive modeling in urgent care: A comparative study of machine learning approaches. Jamia Open, 1(1), 87–98. "],["roshomon-sets-on-death-prediction-xgb-models-using-mimic-iii-database.html", "6.4 Roshomon sets on death prediction XGB models using MIMIC-III database", " 6.4 Roshomon sets on death prediction XGB models using MIMIC-III database Authors: Ada Gąssowska, Elżbieta Jowik (Warsaw University of Techcnology) 6.4.1 An initial literature review As the Rashomon Effect is not a common concept, any references to the term in the literature are somewhat limited. The phenomenon is considered to occur when the same matter can be explained equally aptly in multitudinous ways. Hence the core of the name concept is the title of the Kurosawa’s movie from 1950 in which each character has different perspective on the same crime. In relation to Machine Learning the Rashomon Effect term was first used in (Breiman et al. 2001) to introduce a class of problems where many differing, accurate models exist to describe the same data i.e. to describe the case where there exist many models that are non-identical but almost-equally-accurate for a given issue. Breiman emphasized that the observation of many different accurate models on specific datasets is a common phenomenon. However, from 2001 on the topic has rarely been discussed. While doing research on different machine learning models, data was quite often not taken into consideration at all. As stated in the recent article (Semenova et al. 2019) Rashomon Effect is directly linked to the topic of Explainable Machine Learning. According to the paper, large volume of data in the Rashomon set might imply the existence of multiple explainable model performing on the dataset equally accurately. The article aims to analyze the Rashomon effect on various datasets and attempt to formulate a statement regarding the information about the machine learning problem carried by the size of the Rashomon set. Another matter closely related to the Rashomon effect that needs to be addressed, is the variables importance analysis. This area of research is described in an (Fisher et al. 2019b) article. The publication emphasizes the existence the fields where Explainable Machine Learning (including Rashomon effect) is particularly important, as the non-explainable models may rely on undesirable variables. In an (Dong and Rudin 2020) paper, it is pointed out that only by comparing many models of similar performance the importance of a variable compared to other variables can be profoundly understood. The authors presented the concept of variable importance cloud and conducted the research showing that the variable importance may dramatically differ in approximately equally good models. References Breiman, L. et al. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the author). Statistical science, 16(3), 199–231. Dong, J., &amp; Rudin, C. (2020). Exploring the cloud of variable importance for the set of all good models. Nature Machine Intelligence, 2(12), 810–824. Fisher, A., Rudin, C., &amp; Dominici, F. (2019b). All models are wrong, but many are useful: Learning a variable’s importance by studying an entire class of prediction models simultaneously. Journal of Machine Learning Research, 20(177), 1–81. Semenova, L., Rudin, C., &amp; Parr, R. (2019). A study in rashomon curves and volumes: A new perspective on generalization and model simplicity in machine learning. arXiv preprint arXiv:1908.01755. "],["roshomon-sets-of-in-hospital-mortality-prediction-random-forest-models.html", "6.5 Roshomon sets of in-hospital mortality prediction random forest models", " 6.5 Roshomon sets of in-hospital mortality prediction random forest models Authors: Jeugeniusz Winiczenko, Mikolaj Malec, Patryk Wrona (Warsaw University of Techcnology) 6.5.1 Abstract The concept of Rashomon set is gaining more and more popularity in machine learning world. However, most efficient ways of building and analysing such sets are yet to be discovered. The main aim of this study was to develop several approaches to creating Rashomon sets, examining their characteristics and using them for further predictions. Performance of models was estimated using the area under the receiver operating characteristic (AUC) curve. For models from Rashomon sets analysis of features’ importance and PDP curves was also conducted. In this study, physiological time-series and medical histories from the Medical Information Mart for Intensive Care (MIMIC-III) database were used. Random forest models were trained for mortality prediction task on 2 datasets; the first containing only physiological time-series and the second containing both physiological time-series and medical histories. For 2 sets of trained models, corresponding to 2 datasets, several Rashomon sets were created using different thresholds. 6.5.2 Related work Rashomon sets are sets of models performing extraordinarily well on a given task. In machine learning, this term was used for the very first time by Leo Breiman in his paper issued in 2001 (Breiman et al. 2001). Just as the task could be any, like in our case predicting patient’s mortality, the use of given features in order to explain vary among many highly accurate models. Moreover, Leo Breiman also described this situation as the Rashomon Effect and explained details using exemplary models. Until recently, the Rashomon sets have been rarely a subject of scientific research. In 2019 (Semenova et al. 2019) approached the issue creating mathematical and statistical definitions and notations regarding such sets of models. They described Rashomon sets as subspaces of the hypothesis space, that is subsets of models having comparable performance as the best model with respect to a loss function. In order to define well the problem, they introduced Rashomon ratio (fraction of models in rashomon set and all models from hypothesis space) and shattering coefficient - the maximum number of ways any n data points can be classified using functions from the hypothesis space. Another outstanding remark concerning Rashomon set was made when in 2019 (Fisher et al. 2019b) emphasized the analysis of features’ importance within Rashomon sets. The authors suggested Model Class Reliance - a new variable importance (VI) tool to study the the range of VI values across all highly accurate models - models included in rashomon sets. Later, (Rudin et al. 2021) provided basic rules for interpretable machine learning and identified 10 technical challenge areas in interpretable machine learning. They emphasized the troubleshooting and easiness of using glass-box models today as well as their advantage over black-box models due to their inscrutable nature. In this article, Challenge number 9 involves understanding, exploring, and measuring the Rashomon set. The authors address questions about how to characterize and visualize rashomon sets, and finally, how to pick the best model out of rashomon set. The Variable Importance Clouds, introduced in (Dong and Rudin 2020), are an excellent tool that one can use to address the above problems. Sush cloud maps every variable to its importance for every well-performing model. In our work, we choose and visualize the Rashomon sets built on a set of features as well as their subset. We address the problem of searching the most crucial predictive variables among those Rashomon sets and investigate the impact of choosing subsets of input features on the whole process of determining Rashomon sets and their characteristics. 6.5.3 Results 6.5.3.1 Number of models in Rashomon set - influence on AUC The best number of models that we can deduce from this figure is about 10 models in a Rashomon set. 6.5.3.2 Variable Importance Plots 6.5.3.2.1 Best AUC Rashomon Sets - X48 6.5.3.2.2 Experts Rashomon Sets - X48 The is no any ramarkable difference between most importanr variables across these different Rashomon sets made on X48. 6.5.3.2.3 Best AUC Rashomon Sets - W48 Half of 6 best AUC models do not treasure that much the variables from X48 (having numbers up to 73) - only the most important one (46) is noticeable in 3 cases. 6.5.3.2.4 Experts Rashomon Sets - W48 All expert models emphasize the influence of X48 variables. Moreover, in 3 cases (out of 6 models) variables being less important than the 46(mean_inr) variable are now more important. These varables are 27(model 2) and 56(model 4 &amp; model 5). 6.5.3.3 Voting in mortality prediction 6.5.3.3.1 MSE - Best AUC &amp; Experts Rashomon Set made on X48 Voting experts had better results than the best model/models when given smaller set of variables - X48. 6.5.3.3.2 MSE - Best AUC &amp; Experts Rashomon Set made on W48 This time, voting experts had worse results than the best model/models when given larger set of variables - W48. Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., et al., et al. (2015). TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. USENIX conference on Operating Systems Design and Implementation. https://dl.acm.org/doi/10.5555/3026877.3026899 Ahmad, M. W., Mourshed, M., &amp; Rezgui, Y. (2017). Trees vs Neurons: Comparison between random forest and ANN for high-resolution prediction of building energy consumption. Energy and Buildings, 147, 77--89. https://doi.org/10.1016/j.enbuild.2017.04.038 Aivodji, U., Arai, H., Fortineau, O., Gambs, S., Hara, S., &amp; Tapp, A. (2019). Fairwashing: The risk of rationalization. In K. Chaudhuri &amp; R. Salakhutdinov (Eds.), Proceedings of the 36th international conference on machine learning (Vol. 97, pp. 161–170). PMLR. http://proceedings.mlr.press/v97/aivodji19a.html Andriawan, Z. A., Purnama, S. R., Darmawan, A. S., Ricko, Wibowo, A., Sugiharto, A., &amp; Wijayanto, F. (2020). Prediction of hotel booking cancellation using CRISP-DM. In 2020 4th international conference on informatics and computational sciences (ICICoS) (pp. 1–6). https://doi.org/10.1109/ICICoS51170.2020.9299011 Anthimopoulos, M., Christodoulidis, S., Ebner, L., Geiser, T., Christe, A., &amp; Mougiakakou, S. (2019). Semantic segmentation of pathological lung tissue with dilated fully convolutional networks. IEEE Journal of Biomedical and Health Informatics, 23(2), 714–722. https://doi.org/10.1109/JBHI.2018.2818620 Antonio, N., Almeida, A. de, &amp; Nunes, L. (2017). Predicting hotel booking cancellations to decrease uncertainty and increase revenue. Tourism &amp; Management Studies, 13(2), 25–39. https://doi.org/10.18089/tms.2017.13203 Antonio, N., Almeida, A. de, &amp; Nunes, L. (2019). An automated machine learning based decision support system to predict hotel booking cancellations. Data Science Journal, 18(1), 1–20. https://doi.org/10.5334/dsj-2019-032 Apley, D. W., &amp; Zhu, J. (2020). Visualizing the effects of predictor variables in black box supervised learning models. Journal of the Royal Statistical Society Series B, 82(4), 1059–1086. https://doi.org/10.1111/rssb.12377 Arik, S. O., &amp; Pfister, T. (2020). TabNet: Attentive Interpretable Tabular Learning. AAAI Conference on Artificial Intelligence (AAAI). https://arxiv.org/abs/1908.07442 Arsad, P. M., Buniyamin, N., &amp; Manan, J. A. (2013). Prediction of engineering students’ academic performance using Artificial Neural Network and Linear Regression: A comparison. ICEED. https://doi.org/10.1109/iceed.2013.6908300 Asadi-Aghbolaghi, M., Azad, R., Fathy, M., &amp; Escalera, S. (2020). Multi-level context gating of embedded collective knowledge for medical image segmentation. https://arxiv.org/abs/2003.05056 Azad, R., Asadi-Aghbolaghi, M., Fathy, M., &amp; Escalera, S. (2019). Bi-directional ConvLSTM u-net with densley connected convolutions. In 2019 IEEE/CVF international conference on computer vision workshop (ICCVW) (pp. 406–415). https://doi.org/10.1109/ICCVW.2019.00052 Baker, M. (2016). Reproducibility crisis. Nature, 533(26), 353–66. Baniecki, H., Kretowicz, W., Piatyszek, P., Wisniewski, J., &amp; Biecek, P. (2020a). dalex: Responsible Machine Learning with Interactive Explainability and Fairness in Python. arXiv:2012.14406. https://arxiv.org/abs/2012.14406 Baniecki, H., Kretowicz, W., Piatyszek, P., Wisniewski, J., &amp; Biecek, P. (2020b). dalex: Responsible Machine Learning with Interactive Explainability and Fairness in Python. arXiv:2012.14406. https://arxiv.org/abs/2012.14406 Barda, N., Riesel, D., Akriv, A., Levy, J., Finkel, U., Yona, G., et al. (2020). Developing a COVID-19 mortality risk prediction model when individual-level data are not available. Nature Communications, 11. https://doi.org/10.1038/s41467-020-18297-9 Barish, M., Bolourani, S., Lau, L. F., Shah, S., &amp; Zanos, T. P. (2020). External validation demonstrates limited clinical utility of the interpretable mortality prediction model for patients with COVID-19. Nature Machine Intelligence, 3, 25–27. https://doi.org/10.1038/s42256-020-00254-2 Barish, M., Bolourani, S., Lau, L. F., Shah, S., &amp; Zanos, T. P. (2021). External validation demonstrates limited clinical utility of the interpretable mortality prediction model for patients with COVID-19. Nature Machine Intelligence, 3(1), 25–27. https://doi.org/10.1038/s42256-020-00254-2 Barredo Arrieta, A., Díaz-Rodríguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., et al. (2020c). Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, 58, 82–115. https://doi.org/10.1016/j.inffus.2019.12.012 Barredo Arrieta, A., Díaz-Rodríguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., et al. (2020b). Explainable artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, 58, 82–115. http://www.sciencedirect.com/science/article/pii/S1566253519308103 Barredo Arrieta, A., Díaz-Rodríguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., et al. (2020a). Explainable artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, 58, 82–115. http://www.sciencedirect.com/science/article/pii/S1566253519308103 Barsoum, E., Zhang, C., Ferrer, C. C., &amp; Zhang, Z. (2016). Training deep networks for facial expression recognition with crowd-sourced label distribution. In Proceedings of the 18th ACM international conference on multimodal interaction (pp. 279–283). New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/2993148.2993165 Belke, A., &amp; Keil, J. (2017). Fundamental determinants of real estate prices: A panel study of german regions, (731). Ruhr Economic Papers. https://doi.org/10.4419/86788851 Bello-Chavolla, O. Y., Bahena-López, J. P., Antonio-Villa, N. E., Vargas-Vázquez, A., González-Díaz, A., Márquez-Salinas, A., et al. (2020). Predicting Mortality Due to SARS-CoV-2: A Mechanistic Score Relating Obesity and Diabetes to COVID-19 Outcomes in Mexico. The Journal of Clinical Endocrinology &amp; Metabolism, 105, 2752--2761. https://doi.org/10.1210/clinem/dgaa346 Berk, R., Heidari, H., Jabbari, S., Kearns, M., &amp; Roth, A. (2017). Fairness in Criminal Justice Risk Assessments: The State of the Art. Sociological Methods &amp; Research. https://doi.org/10.1177/0049124118782533 Biecek, P. (2018b). DALEX: Explainers for Complex Predictive Models in R. Journal of Machine Learning Research, 19(84), 1–5. https://jmlr.org/papers/v19/18-416.html Biecek, P. (2018a). DALEX: Explainers for Complex Predictive Models in R. Journal of Machine Learning Research, 19(84), 1–5. http://jmlr.org/papers/v19/18-416.html Biecek, P., &amp; Burzykowski, T. (2021). Explanatory Model Analysis. Chapman; Hall/CRC, New York. https://pbiecek.github.io/ema/ Bird, S., Dudík, M., Edgar, R., Horn, B., Lutz, R., Milan, V., et al. (2020). Fairlearn: A toolkit for assessing and improving fairness in AI (No. MSR-TR-2020-32). Microsoft. https://www.microsoft.com/en-us/research/publication/fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai/ Bradley, A. P. (1997). The use of the area under the ROC curve in the evaluation of machine learning algorithms. Pattern Recogn., 30(7), 1145–1159. https://doi.org/10.1016/S0031-3203(96)00142-2 Breiman, L. (1999). Random forests. UC Berkeley TR567. Breiman, L. et al. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the author). Statistical science, 16(3), 199–231. Can, A. (1990). The measurement of neighborhood dynamics in urban house prices. Economic Geography, 66(3), 254–272. https://doi.org/10.2307/143400 Cao, Y., Liu, X., Xiong, L., &amp; Cai, K. (2020). Imaging and clinical features of patients with 2019 novel coronavirus SARS-CoV-2: A systematic review and meta-analysis. Journal of Medical Virology, 92(9), 1449–1459. https://doi.org/10.1002/jmv.25822 Chen, T., &amp; Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. KDD. https://doi.org/https://doi.org/10.1145/2939672.2939785 Chollet, F. (2017). Deep learning with python. Manning. Chouldechova, A. (2016). Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments. Big Data, 5. https://doi.org/10.1089/big.2016.0047 Chowdhury, M. E. H., Rahman, T., Khandakar, A., Mazhar, R., Kadir, M. A., Mahbub, Z. B., et al. (2020). Can AI help in screening viral and COVID-19 pneumonia? IEEE Access, 8, 132665–132676. https://doi.org/10.1109/ACCESS.2020.3010287 Clark, K., Vendt, B., Smith, K., Freymann, J., Kirby, J., Koppel, P., et al. (2013). The cancer imaging archive (TCIA): Maintaining and operating a public information repository. Journal of Digital Imaging, 26(6), 1045–1057. https://doi.org/10.1007/s10278-013-9622-7 Code of Federal Regulations. (1978). SECTION 4D, UNIFORM GUIDELINES ON EMPLOYEE SELECTION PROCEDURES (1978). https://www.govinfo.gov/content/pkg/CFR-2014-title29-vol4/xml/CFR-2014-title29-vol4-part1607.xml Cohen, J. P., Morrison, P., Dao, L., Roth, K., Duong, T. Q., &amp; Ghassemi, M. (2020). COVID-19 image data collection: Prospective predictions are the future. arXiv 2006.11988. https://github.com/ieee8023/covid-chestxray-dataset Conway, J. (2018, January). Artificial Intelligence and Machine Learning : Current Applications in Real Estate (PhD thesis). Retrieved from https://dspace.mit.edu/bitstream/handle/1721.1/120609/1088413444-MIT.pdf Corbett-Davies, S., Pierson, E., Feller, A., Goel, S., &amp; Huq, A. (2017). Algorithmic Decision Making and the Cost of Fairness. https://doi.org/10.1145/3097983.3098095 Das, S., Cashman, D., Chang, R., &amp; Endert, A. (2019). BEAMES: Interactive multimodel steering, selection, and inspection for regression tasks. IEEE Computer Graphics and Applications, 39(5), 20–32. https://doi.org/10.1109/MCG.2019.2922592 Davis, J., &amp; Goadrich, M. (2006). The relationship between precision-recall and ROC curves. In Proceedings of the 23rd international conference on machine learning (pp. 233–240). New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/1143844.1143874 Dong, J., &amp; Rudin, C. (2020). Exploring the cloud of variable importance for the set of all good models. Nature Machine Intelligence, 2(12), 810–824. Dubin, R. A. (1998). Predicting house prices using multiple listings data. The Journal of Real Estate Finance and Economics. https://doi.org/10.1023/A:1007751112669 Dupuis, C., De Montmollin, E., Neuville, M., Mourvillier, B., Ruckly, S., &amp; Timsit, J. F. (2021). Limited applicability of a COVID-19 specific mortality prediction rule to the intensive care setting. Nature Machine Intelligence, 3(1), 20–22. https://doi.org/10.1038/s42256-020-00252-4 Dwork, C., Hardt, M., Pitassi, T., Reingold, O., &amp; Zemel, R. (2012). Fairness through awareness. ITCS. https://doi.org/10.1145/2090236.2090255 England, R. (2019). Wine’s alcohol levels explained. https://www.wineinvestment.com/wine-blog/2019/05/wines-alcohol-levels-explained?fbclid=IwAR3xpQITEQZrQUPPaEt7-DbFHmvHE559-iVuLsgS6dDinOeWrl04MZiglbM. Falk, M., &amp; Vieru, M. (2018). Modelling the cancellation behaviour of hotel guests. International Journal of Contemporary Hospitality Management, 30(10), 3100–3116. https://doi.org/10.1108/ijchm-08-2017-0509 Fan, C., Cui, Z., &amp; Zhong, X. (2018). House prices prediction with machine learning algorithms. In Proceedings of the 2018 10th international conference on machine learning and computing (pp. 6–10). New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/3195106.3195133 Fawcett, T. (2006). An introduction to ROC analysis. Pattern Recognition Letters, 27(8), 861–874. https://doi.org/https://doi.org/10.1016/j.patrec.2005.10.010 Fisher, A., Rudin, C., &amp; Dominici, F. (2019a). All Models are Wrong, but Many are Useful: Learning a Variable’s Importance by Studying an Entire Class of Prediction Models Simultaneously. Journal of Machine Learning Research, 20(177), 1–81. http://jmlr.org/papers/v20/18-760.html Fisher, A., Rudin, C., &amp; Dominici, F. (2019b). All models are wrong, but many are useful: Learning a variable’s importance by studying an entire class of prediction models simultaneously. Journal of Machine Learning Research, 20(177), 1–81. Friedman, J. H. (2000). Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29, 1189–1232. Ge, X., Runeson, G., &amp; Lam, K. C. (2021). Forecasting hong kong housing prices: An artificial neural network approach. Géron, A. (2017). Hands-on machine learning with scikit-learn and TensorFlow : Concepts, tools, and techniques to build intelligent systems. O’Reilly Media. Ghysels, E., Plazzi, A., Valkanov, R., &amp; Torous, W. (2013). Chapter 9 - forecasting real estate prices, 2, 509–580. https://doi.org/https://doi.org/10.1016/B978-0-444-53683-9.00009-8 GOLDNER, M. C., ZAMORA, M. C., DI LEO LIRA, P., GIANNINOTO, H., &amp; BANDONI, A. (2009). EFFECT OF ETHANOL LEVEL IN THE PERCEPTION OF AROMA ATTRIBUTES AND THE DETECTION OF VOLATILE COMPOUNDS IN RED WINE. Journal of sensory studies, 24(2), 243–257. Gosiewska, A., &amp; Biecek, P. (2019). Do Not Trust Additive Explanations. arXiv. https://arxiv.org/abs/1903.11420v3 Gregutt, P. (2003). Does a higher alcohol content mean it’s a better drinking wine? The Seattle Times. https://archive.seattletimes.com/archive/?date=20031008&amp;slug=wineqanda08&amp;fbclid=IwAR3lBlpdwUCUWjWKaH4Px21b9fJQwBT0aMTa8bNWCbx4ipo4otWzvR9_mTc Hanley, J. A. (2014). Receiver Operating Characteristic (ROC) Curves. Wiley StatsRef: Statistics Reference Online. https://doi.org/10.1002/9781118445112.stat05255 Hardt, M., Price, E., Price, E., &amp; Srebro, N. (2016). Equality of Opportunity in Supervised Learning. NeurIPS. https://papers.nips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html Heyman, A., &amp; Sommervoll, D. (2019). House prices and relative location. Cities, 95, 102373. https://doi.org/10.1016/j.cities.2019.06.004 Jefford, A. (2010). Alcohol levels: The balancing act. https://www.decanter.com/features/alcohol-levels-the-balancing-act-246426/?fbclid=IwAR0bsIWug6-7l77rxb01Va8P1F_hVkaUTacNtlF-V-wRXb1HA3rJXpl74Pw. Jordão, A. M., Vilela, A., &amp; Cosme, F. (2015). From sugar of grape to alcohol of wine: Sensorial impact of alcohol in wine. Beverages, 1(4), 292–310. https://doi.org/10.3390/beverages1040292 Karim, M. R., Döhmen, T., Rebholz-Schuhmann, D., Decker, S., Cochez, M., &amp; Beyan, O. (2020). DeepCOVIDExplainer: Explainable COVID-19 diagnosis from chest x-ray images. IEEE. https://doi.org/10.1109/BIBM49941.2020.9313304 Kaushal, A., Altman, R., &amp; Langlotz, C. (2020). Health Care AI Systems Are Biased. Scientific American. https://www.scientificamerican.com/article/health-care-ai-systems-are-biased Klambauer, G., Unterthiner, T., Mayr, A., &amp; Hochreiter, S. (2017). Self-Normalizing Neural Networks. arXiv:1706.02515. https://arxiv.org/abs/1706.02515 Komisarczyk, Konrad and Maksymiuk, Szymon and Koźmiński, Paweł and Biecek, Przemysław. (2020). treeshap: Fast SHAP values computation for ensemble models. R package. https://github.com/ModelOriented/treeshap Law, S. (2017). Defining street-based local area and measuring its effect on house price using a hedonic price approach: The case study of metropolitan london. Cities, 60, 166–179. https://doi.org/10.1016/j.cities.2016.08.008 Li, X., Ge, P., Zhu, J., Li, H., Graham, J., Singer, A., et al. (2020). Deep learning prediction of likelihood of ICU admission and mortality in COVID-19 patients using clinical variables. PeerJ, 8. https://peerj.com/articles/10337/ Liu, C., Gao, C., Xia, X., Lo, D., Grundy, J., &amp; Yang, X. (2020). On the replicability and reproducibility of deep learning in software engineering. https://arxiv.org/abs/2006.14244 Lundberg, Scott M., Erion, G. G., &amp; Lee, S.-I. (2019). Consistent Individualized Feature Attribution for Tree Ensembles. ICML Workshop. https://arxiv.org/abs/1802.03888 Lundberg, Scott M., &amp; Lee, S.-I. (2017). A unified approach to interpreting model predictions. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, &amp; R. Garnett (Eds.), Advances in neural information processing systems 30 (pp. 4765–4774). Montreal: Curran Associates. http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf Ma, X., Ng, M., Xu, S., Xu, Z., Qiu, H., Liu, Y., et al. (2020). Development and validation of prognosis model of mortality risk in patients with COVID-19. Epidemiology and Infection, 148. http://doi.org/10.1017/S0950268820001727 Maksymiuk, S., Gosiewska, A., &amp; Biecek, P. (2020). Landscape of r packages for eXplainable artificial intelligence. arXiv. https://arxiv.org/abs/2009.13248 Maksymiuk, S., Gosiewska, A., &amp; Biecek, P. (2021). Landscape of r packages for eXplainable artificial intelligence. https://arxiv.org/abs/2009.13248 Mendez, D., Graziotin, D., Wagner, S., &amp; Seibold, H. (2020). Open science in software engineering. Contemporary Empirical Methods in Software Engineering, 477–501. https://doi.org/10.1007/978-3-030-32489-6_17 Meyer, D., Dimitriadou, E., Hornik, K., Weingessel, A., Leisch, F., Chang, C.-C., &amp; Lin, C.-C. (2021). e1071: Misc Functions of the Department of Statistics, Probability Theory Group. R package. https://CRAN.R-project.org/package=e1071 Molnar, C. (2019). Interpretable Machine Learning: A Guide for Making Black Box Models Explainable. https://christophm.github.io/interpretable-ml-book Pace, R. K., &amp; Barry, R. (1997). Sparse spatial autoregressions. Statistics &amp; Probability Letters, 33(3), 291–297. Park, B., &amp; Bae, J. (2015). Using machine learning algorithms for housing price prediction: The case of Fairfax County, Virginia housing data. Expert Systems with Applications, 42. https://doi.org/10.1016/j.eswa.2014.11.040 Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., et al. (2011). Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12, 2825–2830. Quanjel, M. J. R., Holten, T. C. van, Gunst-van der Vliet, P. C., Wielaard, J., Karakaya, B., Söhne, M., et al. (2021). Replication of a mortality prediction model in dutch patients with COVID-19. Nature Machine Intelligence, 3(1), 23–24. https://doi.org/10.1038/s42256-020-00253-3 Quanjel, M. J. R., Holten, T. C. van, Vliet, P. C. G. der, Wielaard, J., Karakaya, B., Söhne, M., et al. (2021). Replication of a mortality prediction model in Dutch patients with COVID-19. Nature Machine Intelligence, 3, 23–24. https://doi.org/10.1038/s42256-020-00253-3 R Core Team. (2018). R: A language and environment for statistical computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/ Raghavan, V., Bollmann, P., &amp; Jung, G. S. (1989). A critical investigation of recall and precision as measures of retrieval system performance. ACM Trans. Inf. Syst., 7(3), 205–229. https://doi.org/10.1145/65943.65945 Rahman, T., Khandakar, A., Qiblawey, Y., Tahir, A., Kiranyaz, S., Abul Kashem, S. B., et al. (2021). Exploring the effect of image enhancement techniques on COVID-19 detection using chest x-ray images. Computers in Biology and Medicine, 132, 104319. https://doi.org/https://doi.org/10.1016/j.compbiomed.2021.104319 Rai, A. (2020). Explainable AI: from black box to glass box. Journal of the Academy of Marketing Science, 48, 137–141. https://link.springer.com/article/10.1007/s11747-019-00710-5 Riasi, A., Schwartz, Z., &amp; Chen, C.-C. (2019). A paradigm shift in revenue management? The new landscape of hotel cancellation policies. Journal of Revenue and Pricing Management, 18(6), 434–440. https://doi.org/10.1057/s41272-019-00189-3 Ribeiro, M. T., Singh, S., &amp; Guestrin, C. (2016). \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, san francisco, CA, USA, august 13-17, 2016 (pp. 1135–1144). https://doi.org/10.18653/v1/n16-3020 Rudin, C., Chen, C., Chen, Z., Huang, H., Semenova, L., &amp; Zhong, C. (2021). Interpretable machine learning: Fundamental principles and 10 grand challenges. arXiv preprint arXiv:2103.11251. Saito, T., &amp; Rehmsmeier, M. (2015). The precision-recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets. Plos One, 10(3). https://doi.org/10.1371/journal.pone.0118432 Sánchez-Medina, A. J., &amp; C-Sánchez, E. (2020). Using machine learning and big data for efficient forecasting of hotel booking cancellations. International Journal of Hospitality Management, 89, 102546. https://doi.org/10.1016/j.ijhm.2020.102546 Semenova, L., Rudin, C., &amp; Parr, R. (2019). A study in rashomon curves and volumes: A new perspective on generalization and model simplicity in machine learning. arXiv preprint arXiv:1908.01755. Smith, S. J., Parsa, H. G., Bujisic, M., &amp; Rest, J.-P. van der. (2015). Hotel cancelation policies, distributive and procedural fairness, and consumer patronage: A study of the lodging industry. Journal of Travel &amp; Tourism Marketing, 32, 886–906. https://doi.org/10.1080/10548408.2015.1063864 Staniak, M., &amp; Biecek, P. (2018). Explanations of model predictions with live and breakDown packages. Tang, F., Xiao, C., Wang, F., &amp; Zhou, J. (2018). Predictive modeling in urgent care: A comparative study of machine learning approaches. Jamia Open, 1(1), 87–98. Tatman, R., VanderPlas, J., &amp; Dane, S. (2018). A practical taxonomy of reproducibility for machine learning research. Tonekaboni, S., Joshi, S., McCradden, M. D., &amp; Goldenberg, A. (2019). What Clinicians Want: Contextualizing Explainable Machine Learning for Clinical End Use. Machine Learning for Healthcare. http://proceedings.mlr.press/v106/tonekaboni19a.html Ucar, F., &amp; Korkmaz, D. (2020). COVIDiagnosis-net: Deep bayes-SqueezeNet based diagnosis of the coronavirus disease 2019 (COVID-19) from x-ray images. Medical Hypotheses, 140, 109761–109761. Vanschoren, J., Rijn, J. N. van, Bischl, B., &amp; Torgo, L. (2013). OpenML: Networked science in machine learning. SIGKDD Explorations, 15(2), 49–60. https://doi.org/10.1145/2641190.2641198 Wang, J., Li, M., Hu, Y., &amp; Zhu, Y. (2009). Comparison of hospital charge prediction models for gastric cancer patients: neural network vs. decision tree models. BMC Health Services Research, 9(1). https://doi.org/10.1186/1472-6963-9-161 Wang, L., Lin, Z. Q., &amp; Wong, A. (2020a). COVID-net: A tailored deep convolutional neural network design for detection of COVID-19 cases from chest x-ray images. Scientific Reports, 10(1), 19549. https://doi.org/10.1038/s41598-020-76550-z Wang, L., Lin, Z. Q., &amp; Wong, A. (2020b). COVID-net: A tailored deep convolutional neural network design for detection of COVID-19 cases from chest x-ray images. Scientific Reports, 10(1), 19549. https://doi.org/10.1038/s41598-020-76550-z Wang, R., Wang, X., &amp; Inouye, D. I. (2021). Shapley Explanation Networks. ICLR. https://openreview.net/forum?id=vsU0efpivw Wang, S., Zha, Y., Li, W., Wu, Q., Li, X., Niu, M., et al. (2020). A fully automatic deep learning system for COVID-19 diagnostic and prognostic analysis. European Respiratory Journal, 56(2). https://doi.org/10.1183/13993003.00775-2020 Wiens, J., Guttag, J., &amp; Horvitz, E. (2014). A study in transfer learning: leveraging data from multiple hospitals to enhance hospital-specific predictions. Journal of the American Medical Informatics Association, 21(4), 699–706. https://doi.org/10.1136/amiajnl-2013-002162 Wiśniewski, J., &amp; Biecek, P. (2021). fairmodels: A Flexible Tool For Bias Detection, Visualization, And Mitigation. arXiv:2104.00507. https://arxiv.org/abs/2104.00507 Wright, M. N., &amp; Ziegler, A. (2016). XGBoost: A Scalable Tree Boosting System. SIGKDD International Conference on Knowledge Discovery and Data Mining. https://doi.org/10.1145/2939672.2939785 Wright, M. N., &amp; Ziegler, A. (2017). ranger: A Fast Implementation of Random Forests for High Dimensional Data in C++ and R. Journal of Statistical Software, 77(1), 1–17. https://doi.org/10.18637/jss.v077.i01 WUoT. (2020). ML case studies: Reproducibility of scientific papers. https://mini-pw.github.io/2020L-WB-Book/reproducibility.html Yan, L., Zhang, H.-T., Goncalves, J., Xiao, Y., Wang, M., Guo, Y., et al. (2020a). An interpretable mortality prediction model for COVID-19 patients. Nature Machine Intelligence, 2(5), 283–288. https://doi.org/10.1038/s42256-020-0180-7 Yan, L., Zhang, H.-T., Goncalves, J., Xiao, Y., Wang, M., Guo, Y., et al. (2020b). An interpretable mortality prediction model for COVID-19 patients. Nature Machine Intelligence, 2(5), 283--288. https://www.nature.com/articles/s42256-020-0180-7 Yan, L., Zhang, H.-T., Goncalves, J., Xiao, Y., Wang, M., Guo, Y., et al. (2020c). An interpretable mortality prediction model for COVID-19 patients. Nature Machine Intelligence, 2(5), 283--288. https://www.nature.com/articles/s42256-020-0180-7 Yildiz, B., Hung, H., Krijthe, J. H., Liem, C. C. S., Loog, M., Migut, G., et al. (2021). ReproducedPapers.org: Openly teaching and structuring machine learning reproducibility. In B. Kerautret, M. Colom, A. Krähenbühl, D. Lopresti, P. Monasse, &amp; H. Talbot (Eds.), Reproducible research in pattern recognition (pp. 3–11). Cham: Springer International Publishing. Zaimi Aldo, W. M., Herman, V., Antonsanti, P.-L., Perone, C. S., &amp; Cohen-Adad, J. (2018). AxonDeepSeg: Automatic axon and myelin segmentation from microscopy data using convolutional neural networks. Scientific Reports, 8(1), 3816. https://doi.org/10.1038/s41598-018-22181-4 Zhao, Q., Meng, M., Kumar, R., Wu, Y., Huang, J., Deng, Y., et al. (2020). Lymphopenia is associated with severe coronavirus disease 2019 (COVID-19) infections: A systemic review and meta-analysis. International Journal of Infectious Diseases, 96, 131–135. https://doi.org/10.1016/j.ijid.2020.04.086 Zheng, Y., Zhu, Y., Ji, M., Wang, R., Liu, X., Zhang, M., et al. (2020a). A Learning-Based Model to Evaluate Hospitalization Priority in COVID-19 Pandemics. Patterns, 1(6), 100092. https://doi.org/10.1016/j.patter.2020.100092 Zheng, Y., Zhu, Y., Ji, M., Wang, R., Liu, X., Zhang, M., et al. (2020b). A Learning-Based Model to Evaluate Hospitalization Priority in COVID-19 Pandemics. Patterns, 1(9), 100173. https://doi.org/10.1016/j.patter.2020.100173 References Breiman, L. et al. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the author). Statistical science, 16(3), 199–231. Dong, J., &amp; Rudin, C. (2020). Exploring the cloud of variable importance for the set of all good models. Nature Machine Intelligence, 2(12), 810–824. Fisher, A., Rudin, C., &amp; Dominici, F. (2019b). All models are wrong, but many are useful: Learning a variable’s importance by studying an entire class of prediction models simultaneously. Journal of Machine Learning Research, 20(177), 1–81. Rudin, C., Chen, C., Chen, Z., Huang, H., Semenova, L., &amp; Zhong, C. (2021). Interpretable machine learning: Fundamental principles and 10 grand challenges. arXiv preprint arXiv:2103.11251. Semenova, L., Rudin, C., &amp; Parr, R. (2019). A study in rashomon curves and volumes: A new perspective on generalization and model simplicity in machine learning. arXiv preprint arXiv:1908.01755. "]]
